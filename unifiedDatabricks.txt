




~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    upload jar and use these steps 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


get the jar file

Upload the built build/libs/hail-all-spark.jar to Databricks Workspace > *Shared* > Libraries > Create Library 
and click Drop library JAR here to upload

Next click the Clusters icon on the left sidebar and then +Create Cluster. 

For Apache Spark Version, select Spark 2.0 (Auto-updating, Scala 2.11). 

In the Databricks cluster creation dialog, click on the Spark tab, and paste the text below into the Spark config box.

THEN go back to Workspace > Shared > Libraries click on the hail-all-spark.jar library and attach it to the created cluster.


BOTTOM LINE:
	
	shared, create, library
	then go to your cluster and install new, select workspace, and will be there











~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	files and directories  
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

%fs ls

%fs ls /dbfs/FileStore/

%fs ls /dbfs/FileStore/jars

# Really good sample hail data to work with:
display(dbutils.fs.ls("/databricks-datasets/hail/data-001"))

dbutils.fs.ls("/")

dbutils.fs.ls("dbfs:/FileStore/")

dbutils.fs.ls(".")
[FileInfo(path='dbfs:/FileStore/', name='FileStore/', size=0),
 FileInfo(path='dbfs:/cbp.vds/', name='cbp.vds/', size=0),
 FileInfo(path='dbfs:/databricks-datasets/', name='databricks-datasets/', size=0),
 FileInfo(path='dbfs:/databricks-results/', name='databricks-results/', size=0),
 FileInfo(path='dbfs:/delta/', name='delta/', size=0),
 FileInfo(path='dbfs:/kdd/', name='kdd/', size=0),
 FileInfo(path='dbfs:/local_disk0/', name='local_disk0/', size=0),
 FileInfo(path='dbfs:/ml/', name='ml/', size=0),
 FileInfo(path='dbfs:/mnt/', name='mnt/', size=0),
 FileInfo(path='dbfs:/tmp/', name='tmp/', size=0),
 FileInfo(path='dbfs:/tom/', name='tom/', size=0),
 FileInfo(path='dbfs:/user/', name='user/', size=0)]












~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	dbutils 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

shortcut
	
	db and then tab










~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	libraries installPyPI
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

import pkg_resources

dbutils.library.list()

dbutils.library.installPyPI('numpy')

https://databricks.com/blog/2019/01/08/introducing-databricks-library-utilities-for-notebooks.html

	dbutils.library.installPyPI('hail')














~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
     open a file and read it 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

with open("/dbfs/databricks-datasets/README.md") as f:
    x = ''.join(f.readlines())
print(x)













~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
    cluster configuration json view 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

{
    "num_workers": 0,
    "cluster_name": "HailvF",
    "spark_version": "5.4.x-scala2.11",
    "spark_conf": {
        "spark.hadoop.mapreduce.input.fileinputformat.split.minsize": "1099511627776",
        "spark.hadoop.io.compression.codecs": "org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,org.apache.hadoop.io.compress.GzipCodec",
        "spark.sql.files.openCostInBytes": "1099511627776",
        "spark.databricks.delta.preview.enabled": "true",
        "spark.sql.files.maxPartitionBytes": "1099511627776",
        "spark.hadoop.parquet.block.size": "1099511627776"
    },
    "aws_attributes": {
        "first_on_demand": 0,
        "availability": "ON_DEMAND",
        "zone_id": "us-west-2c",
        "spot_bid_price_percent": 100,
        "ebs_volume_count": 0
    },
    "node_type_id": "dev-tier-node",
    "driver_node_type_id": "dev-tier-node",
    "ssh_public_keys": [],
    "custom_tags": {},
    "spark_env_vars": {
        "ENABLE_HAIL": "true",
        "PYSPARK_PYTHON": "/databricks/python3/bin/python3"
    },
    "autotermination_minutes": 120,
    "enable_elastic_disk": false,
    "cluster_source": "API",
    "init_scripts": [],
    "cluster_id": "0812-164141-nifty797"
}






















































































--- example spark config ---

spark.hadoop.mapreduce.input.fileinputformat.split.minsize 1099511627776
spark.hadoop.io.compression.codecs org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,org.apache.hadoop.io.compress.GzipCodec
spark.sql.files.openCostInBytes 1099511627776
spark.databricks.delta.preview.enabled true
spark.sql.files.maxPartitionBytes 1099511627776
spark.hadoop.parquet.block.size 1099511627776







--- starting hail ---


I have this set now as a library to start on all new clusters:
dbfs:/FileStore/jars/35bb0c74_d723_4381_bc5c_5bb9d4ea6326-hail_tutorial_databricks-e84f3.jar




ENABLE_HAIL=true
PYSPARK_PYTHON=/databricks/python3/bin/python3



spark.hadoop.mapreduce.input.fileinputformat.split.minsize 1099511627776
spark.hadoop.io.compression.codecs org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,org.apache.hadoop.io.compress.GzipCodec
spark.sql.files.openCostInBytes 1099511627776
spark.databricks.delta.preview.enabled true
spark.sql.files.maxPartitionBytes 1099511627776
spark.hadoop.parquet.block.size 1099511627776


dbfs:/FileStore/jars/35bb0c74_d723_4381_bc5c_5bb9d4ea6326-hail_tutorial_databricks-e84f3.jar	35bb0c74_d723_4381_bc5c_5bb9d4ea6326-hail_tutorial_databricks-e84f3.jar	27876587
dbfs:/FileStore/jars/a3245b97_8a2b_4bce_902a_cf2cec710769-hail_tutorial_databricks-e84f3.jar	a3245b97_8a2b_4bce_902a_cf2cec710769-hail_tutorial_databricks-e84f3.jar	27876587
dbfs:/FileStore/jars/b9008399_4234_4453_837b_0c918d34f13a-hail_tutorial_databricks-e84f3.jar	b9008399_4234_4453_837b_0c918d34f13a-hail_tutorial_databricks-e84f3.jar	27876587
dbfs:/FileStore/jars/c7eaedcd_8828_4b98_8cc6_90caba462aa4-hail_devel_py2_7_databricks-20ed0.egg	c7eaedcd_8828_4b98_8cc6_90caba462aa4-hail_devel_py2_7_databricks-20ed0.egg	153491
dbfs:/FileStore/jars/ce0ce418_f2d9_4cc8_89ee_206a3245fff5-hail_tutorial_databricks-e84f3.jar	ce0ce418_f2d9_4cc8_89ee_206a3245fff5-hail_tutorial_databricks-e84f3.jar	27876587


