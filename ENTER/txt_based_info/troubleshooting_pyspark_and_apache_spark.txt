



1.  Existing env variables











2.  jupyter kernel locations






x.  example of pyspark via windows cmd working


C:\SPARK\bin>pyspark
Picked up _JAVA_OPTIONS: -Xmx512M -Xms512M
[I 18:26:30.557 NotebookApp] The port 8888 is already in use, trying another port.
[W 18:26:30.625 NotebookApp] Terminals not available (error was No module named 'winpty.cywinpty')
[I 18:26:30.793 NotebookApp] Loading IPython parallel extension
[I 18:26:30.928 NotebookApp] [jupyter_nbextensions_configurator] enabled 0.4.1
[I 18:26:31.583 NotebookApp] Jupyter-Spark enabled!
[I 18:26:31.775 NotebookApp] JupyterLab extension loaded from C:\Users\tbresee\AppData\Roaming\Python\Python37\site-packages\jupyterlab
[I 18:26:31.776 NotebookApp] JupyterLab application directory is C:\Users\tbresee\AppData\Roaming\Python\share\jupyter\lab
SPARKMONITOR_SERVER: Loading Server Extension
[I 18:26:31.815 NotebookApp] Serving notebooks from local directory: C:\SPARK\bin
[I 18:26:31.817 NotebookApp] The Jupyter Notebook is running at:
[I 18:26:31.821 NotebookApp] http://localhost:8889/?token=152b6ac95807db02750d0a2e12bb2d2dcfcfd926b0e3f76c
[I 18:26:31.824 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[C 18:26:32.559 NotebookApp]

    Copy/paste this URL into your browser when you connect for the first time,
    to login with a token:
        http://localhost:8889/?token=152b6ac95807db02750d0a2e12bb2d2dcfcfd926b0e3f76c
[I 18:26:33.756 NotebookApp] Accepting one-time-token-authenticated connection from ::1
[I 18:26:42.861 NotebookApp] Creating new notebook in
[I 18:26:44.673 NotebookApp] Kernel started: f4fdaa35-8cd0-4345-a32c-7c093dd4750f
[W 18:26:45.057 NotebookApp] 404 GET /nbextensions/sparkmonitor/module.js?v=20190610182628 (::1) 9.02ms referer=http://localhost:8889/notebooks/Untitled.ipynb?kernel_name=python3
[W 18:26:45.064 NotebookApp] 404 GET /nbextensions/plotlywidget/extension.js?v=20190610182628 (::1) 4.01ms referer=http://localhost:8889/notebooks/Untitled.ipynb?kernel_name=python3
[W 18:26:45.125 NotebookApp] 404 GET /notebooks/nbextensions/c:/Users/tbresee?v=20190610182628 (::1): nbextensions/c:/Users/tbresee is outside root contents directory
[W 18:26:45.127 NotebookApp] 404 GET /notebooks/nbextensions/c:/Users/tbresee?v=20190610182628 (::1) 3.01ms referer=http://localhost:8889/notebooks/Untitled.ipynb?kernel_name=python3
[W 18:26:45.261 NotebookApp] 404 GET /nbextensions/widgets/notebook/js/extension.js?v=20190610182628 (::1) 5.01ms referer=http://localhost:8889/notebooks/Untitled.ipynb?kernel_name=python3
Picked up _JAVA_OPTIONS: -Xmx512M -Xms512M
Picked up _JAVA_OPTIONS: -Xmx512M -Xms512M
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[I 18:27:00.522 NotebookApp] Adapting to protocol v5.1 for kernel f4fdaa35-8cd0-4345-a32c-7c093dd4750f














"""
An interactive shell.

This file is designed to be launched as a PYTHONSTARTUP script.
"""

import atexit
import os
import platform
import warnings

import py4j

from pyspark import SparkConf
from pyspark.context import SparkContext
from pyspark.sql import SparkSession, SQLContext

if os.environ.get("SPARK_EXECUTOR_URI"):
    SparkContext.setSystemProperty("spark.executor.uri", os.environ["SPARK_EXECUTOR_URI"])

SparkContext._ensure_initialized()

try:
    spark = SparkSession._create_shell_session()
except Exception:
    import sys
    import traceback
    warnings.warn("Failed to initialize Spark session.")
    traceback.print_exc(file=sys.stderr)
    sys.exit(1)

sc = spark.sparkContext
sql = spark.sql
atexit.register(lambda: sc.stop())

# for compatibility
sqlContext = spark._wrapped
sqlCtx = sqlContext

print(r"""Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version %s
      /_/
""" % sc.version)
print("Using Python version %s (%s, %s)" % (
    platform.python_version(),
    platform.python_build()[0],
    platform.python_build()[1]))
print("SparkSession available as 'spark'.")

# The ./bin/pyspark script stores the old PYTHONSTARTUP value in OLD_PYTHONSTARTUP,
# which allows us to execute the user's PYTHONSTARTUP file:
_pythonstartup = os.environ.get('OLD_PYTHONSTARTUP')
if _pythonstartup and os.path.isfile(_pythonstartup):
    with open(_pythonstartup) as f:
        code = compile(f.read(), _pythonstartup, 'exec')
        exec(code)



