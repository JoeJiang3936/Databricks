
19/08/12 22:48:18 WARN SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.


































19/08/12 22:48:16 INFO StaticConf$: DB_HOME: /databricks



19/08/12 22:48:17 INFO DriverDaemon$: ========== driver starting up ==========
19/08/12 22:48:17 INFO DriverDaemon$: Java: Oracle Corporation 1.8.0_212			SEE HOW IT IS 1.8 ?  *******************************************************
19/08/12 22:48:17 INFO DriverDaemon$: OS: Linux/amd64 4.4.0-1087-aws				
19/08/12 22:48:17 INFO DriverDaemon$: CWD: /databricks/driver					IT IS AWAKE ON /databricks/driver ******************************************
19/08/12 22:48:17 INFO DriverDaemon$: Mem: Max: 43.8G loaded GCs: PS Scavenge, PS MarkSweep
19/08/12 22:48:17 INFO DriverDaemon$: Logging multibyte characters: ✓
19/08/12 22:48:17 INFO DriverDaemon$: 'publicFile' appender in root logger: class com.databricks.logging.RedactionRollingFileAppender
19/08/12 22:48:17 INFO DriverDaemon$: 'org.apache.log4j.Appender' appender in root logger: class com.codahale.metrics.log4j.InstrumentedAppender
19/08/12 22:48:17 INFO DriverDaemon$: 'null' appender in root logger: class com.databricks.logging.RequestTracker
19/08/12 22:48:17 INFO DriverDaemon$: == Modules:
19/08/12 22:48:17 INFO DriverDaemon$: Starting prometheus metrics log export timer
19/08/12 22:48:17 INFO DriverDaemon$: Universe Git Hash: 9fe869e25b36fd138ce6237260383d9acb7597f5
19/08/12 22:48:17 INFO DriverDaemon$: Spark Git Hash: da21e1ae0fc26ce7b8930f2fa37323adf8ff376c
19/08/12 22:48:17 WARN RunHelpers$: Missing tag isolation client: java.util.NoSuchElementException: key not found: TagDefinition(clientType,The client type for a request, used for isolating resources for the request.)
19/08/12 22:48:17 INFO DatabricksILoop$: Creating throwaway interpreter
19/08/12 22:48:17 INFO SparkConfUtils$: Customize spark config according to file /tmp/custom-spark.conf
19/08/12 22:48:17 INFO SparkConfUtils$: new spark config: spark.sql.files.maxPartitionBytes -> 1099511627776			**************** THESE WRONG ? *********************
19/08/12 22:48:17 INFO SparkConfUtils$: new spark config: spark.sql.files.openCostInBytes -> 1099511627776
19/08/12 22:48:17 INFO SparkConfUtils$: new spark config: spark.executor.tempDirectory -> /local_disk0/tmp
19/08/12 22:48:17 INFO SparkConfUtils$: new spark config: spark.hadoop.mapreduce.input.fileinputformat.split.minsize -> 1099511627776
19/08/12 22:48:17 INFO SparkConfUtils$: new spark config: spark.hadoop.parquet.block.size -> 1099511627776
19/08/12 22:48:17 INFO SparkConfUtils$: new spark config: spark.driver.tempDirectory -> /local_disk0/tmp
19/08/12 22:48:17 INFO SparkConfUtils$: new spark config: spark.hadoop.io.compression.codecs -> org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,org.apache.hadoop.io.compress.GzipCodec



19/08/12 22:48:17 INFO MetastoreMonitor$: Internal internal metastore configured (config=DbMetastoreConfig{host=md13pes28u6ilc4.chkweekm4xjq.us-east-1.rds.amazonaws.com, port=3306, dbName=organization978277182616062, user=wAYERvGGkTCcCyiR})
19/08/12 22:48:17 INFO JettyClient$: Creating new HttpClient with SSLContextFactory=None,maxRequestHeaderSize=65536, namePrefix=Some(DriverDaemon), idleTimeout=2 hours, useBlockingConnect: true
19/08/12 22:48:18 INFO HikariDataSource: metastore-monitor - Starting...
19/08/12 22:48:18 INFO HikariDataSource: metastore-monitor - Start completed.



19/08/12 22:48:18 INFO DriverCorral: Creating the driver context
19/08/12 22:48:18 INFO DatabricksILoop$: Class Server Dir: /local_disk0/tmp/repl/spark-4399150706229039107-01be5a39-c211-4a4c-a096-05665e2ccbe7
19/08/12 22:48:18 INFO SparkConfUtils$: Customize spark config according to file /tmp/custom-spark.conf
19/08/12 22:48:18 INFO SparkConfUtils$: new spark config: spark.sql.files.maxPartitionBytes -> 1099511627776
19/08/12 22:48:18 INFO SparkConfUtils$: new spark config: spark.sql.files.openCostInBytes -> 1099511627776
19/08/12 22:48:18 INFO SparkConfUtils$: new spark config: spark.executor.tempDirectory -> /local_disk0/tmp
19/08/12 22:48:18 INFO SparkConfUtils$: new spark config: spark.hadoop.mapreduce.input.fileinputformat.split.minsize -> 1099511627776
19/08/12 22:48:18 INFO SparkConfUtils$: new spark config: spark.hadoop.parquet.block.size -> 1099511627776
19/08/12 22:48:18 INFO SparkConfUtils$: new spark config: spark.driver.tempDirectory -> /local_disk0/tmp
19/08/12 22:48:18 INFO SparkConfUtils$: new spark config: spark.hadoop.io.compression.codecs -> org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,org.apache.hadoop.io.compress.GzipCodec
19/08/12 22:48:18 WARN SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
19/08/12 22:48:18 INFO HikariDataSource: metastore-monitor - Shutdown initiated...
19/08/12 22:48:18 INFO SparkContext: Running Spark version 2.4.3
19/08/12 22:48:18 INFO HikariDataSource: metastore-monitor - Shutdown completed.
19/08/12 22:48:18 INFO MetastoreMonitor: Metastore healthcheck successful (connection duration = 734 milliseconds)
19/08/12 22:48:18 WARN SparkConf: Detected deprecated memory fraction settings: [spark.storage.memoryFraction, spark.shuffle.memoryFraction]. As of Spark 1.6, execution and storage memory management are unified. All memory fractions used in the old model are now deprecated and no longer read. If you wish to use the old memory management, you may explicitly enable `spark.memory.useLegacyMode` (not recommended).
19/08/12 22:48:18 INFO SparkContext: Submitted application: Databricks Shell
19/08/12 22:48:18 INFO SparkContext: Spark configuration:
eventLog.rolloverIntervalSeconds=3600



spark.akka.frameSize=256






spark.app.name=Databricks Shell	                <------------------------------------------------------

spark.cleaner.referenceTracking.blocking=false

spark.databricks.acl.client=com.databricks.spark.sql.acl.client.SparkSqlAclClient
spark.databricks.acl.provider=com.databricks.sql.acl.ReflectionBackedAclProvider
spark.databricks.cloudProvider=AWS		<------------------------------------------------------
spark.databricks.clusterSource=UI		<------------------------------------------------------
spark.databricks.clusterUsageTags.autoTerminationMinutes=45
spark.databricks.clusterUsageTags.clusterAllTags=[{"key":"Vendor","value":"Databricks"},{"key":"Creator","value":"tbresee@mail.smu.edu"},{"key":"ClusterName","value":"Temp"},{"key":"ClusterId","value":"0812-224616-grad803"},{"key":"Name","value":"978277182616062-1eb54f5d-0990-42ac-b078-0e5707e67a1d-worker"}]		<------------------------------------------------------
spark.databricks.clusterUsageTags.clusterAvailability=SPOT_WITH_FALLBACK	
spark.databricks.clusterUsageTags.clusterCreator=Webapp
spark.databricks.clusterUsageTags.clusterEbsVolumeCount=3
spark.databricks.clusterUsageTags.clusterEbsVolumeSize=100
spark.databricks.clusterUsageTags.clusterEbsVolumeType=GENERAL_PURPOSE_SSD
spark.databricks.clusterUsageTags.clusterFirstOnDemand=1
spark.databricks.clusterUsageTags.clusterGeneration=0
spark.databricks.clusterUsageTags.clusterId=0812-224616-grad803
spark.databricks.clusterUsageTags.clusterLogDeliveryEnabled=false
spark.databricks.clusterUsageTags.clusterLogDestination=
spark.databricks.clusterUsageTags.clusterMaxWorkers=8
spark.databricks.clusterUsageTags.clusterMetastoreAccessType=RDS_DIRECT
spark.databricks.clusterUsageTags.clusterMinWorkers=2
spark.databricks.clusterUsageTags.clusterName=Temp
spark.databricks.clusterUsageTags.clusterNoDriverDaemon=false
spark.databricks.clusterUsageTags.clusterNodeType=r4.2xlarge
spark.databricks.clusterUsageTags.clusterNumSshKeys=0
spark.databricks.clusterUsageTags.clusterOwnerOrgId=978277182616062
spark.databricks.clusterUsageTags.clusterOwnerUserId=*********(redacted)
spark.databricks.clusterUsageTags.clusterPinned=false
spark.databricks.clusterUsageTags.clusterPythonVersion=3
spark.databricks.clusterUsageTags.clusterResourceClass=default
spark.databricks.clusterUsageTags.clusterScalingType=autoscaling
spark.databricks.clusterUsageTags.clusterSku=STANDARD_SKU
spark.databricks.clusterUsageTags.clusterSpotBidPricePercent=100
spark.databricks.clusterUsageTags.clusterState=Pending
spark.databricks.clusterUsageTags.clusterStateMessage=Starting Spark
spark.databricks.clusterUsageTags.clusterTargetWorkers=2
spark.databricks.clusterUsageTags.clusterWorkers=2
spark.databricks.clusterUsageTags.containerType=LXC
spark.databricks.clusterUsageTags.containerZoneId=us-east-1c
spark.databricks.clusterUsageTags.driverContainerId=c748523573e64c9a93a4002c44d9bbf2
spark.databricks.clusterUsageTags.driverContainerPrivateIp=10.67.237.124
spark.databricks.clusterUsageTags.driverInstanceId=i-07369000e410090b3
spark.databricks.clusterUsageTags.driverInstancePrivateIp=10.67.231.19
spark.databricks.clusterUsageTags.driverNodeType=r4.2xlarge
spark.databricks.clusterUsageTags.driverPublicDns=ec2-3-223-191-117.compute-1.amazonaws.com
spark.databricks.clusterUsageTags.enableCredentialPassthrough=*********(redacted)
spark.databricks.clusterUsageTags.enableDfAcls=false
spark.databricks.clusterUsageTags.enableElasticDisk=false
spark.databricks.clusterUsageTags.enableJdbcAutoStart=true
spark.databricks.clusterUsageTags.enableJobsAutostart=true
spark.databricks.clusterUsageTags.enableSqlAclsOnly=false
spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2=0
spark.databricks.clusterUsageTags.sparkVersion=5.5.x-scala2.11
spark.databricks.clusterUsageTags.userProvidedRemoteVolumeCount=*********(redacted)
spark.databricks.clusterUsageTags.userProvidedRemoteVolumeSizeGb=*********(redacted)
spark.databricks.clusterUsageTags.userProvidedRemoteVolumeType=*********(redacted)
spark.databricks.clusterUsageTags.workerEnvironmentId=978277182616062-1eb54f5d-0990-42ac-b078-0e5707e67a1d






spark.databricks.credential.redactor=*********(redacted)
spark.databricks.delta.logStore.crossCloud.fatal=true
spark.databricks.delta.multiClusterWrites.enabled=true
spark.databricks.driverNodeTypeId=r4.2xlarge		<------------------------------------------------------



spark.databricks.eventLog.dir=eventlogs
spark.databricks.io.directoryCommit.enableLogicalDelete=false
spark.databricks.overrideDefaultCommitProtocol=org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol
spark.databricks.passthrough.adls.gen2.tokenProviderClassName=*********(redacted)
spark.databricks.passthrough.adls.tokenProviderClassName=*********(redacted)
spark.databricks.passthrough.glue.credentialsProviderFactoryClassName=*********(redacted)
spark.databricks.passthrough.glue.executorServiceFactoryClassName=com.databricks.backend.daemon.driver.GlueClientExecutorServiceFactory
spark.databricks.passthrough.s3a.tokenProviderClassName=*********(redacted)
spark.databricks.preemption.enabled=true




spark.databricks.redactor=com.databricks.spark.util.DatabricksSparkLogRedactorProxy
spark.databricks.session.share=false
spark.databricks.sparkContextId=4399150706229039107
spark.databricks.tahoe.logStore.aws.class=com.databricks.tahoe.store.MultiClusterLogStore
spark.databricks.tahoe.logStore.azure.class=com.databricks.tahoe.store.AzureLogStore
spark.databricks.tahoe.logStore.class=com.databricks.tahoe.store.DelegatingLogStore
spark.databricks.workerNodeTypeId=r4.2xlarge
spark.driver.allowMultipleContexts=false
spark.driver.maxResultSize=4g
spark.driver.tempDirectory=/local_disk0/tmp






spark.eventLog.enabled=false
spark.executor.extraClassPath=*********(redacted)
spark.executor.extraJavaOptions=-Djava.io.tmpdir=/local_disk0/tmp -XX:ReservedCodeCacheSize=256m -XX:+UseCodeCacheFlushing -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:+PrintFlagsFinal -XX:+PrintGCDateStamps -verbose:gc -XX:+PrintGCDetails -Xss4m -Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.DatatypeFactoryImpl -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -Djavax.xml.validation.SchemaFactory:http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory -Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplementationSourceImpl -Djavax.net.ssl.sessionCacheSize=10000 -Ddatabricks.serviceName=spark-executor-1
spark.executor.memory=44632m
spark.executor.tempDirectory=/local_disk0/tmp
spark.extraListeners=com.databricks.backend.daemon.driver.DBCEventLoggingListener
spark.files.fetchFailure.unRegisterOutputOnHost=true
spark.files.overwrite=true
spark.files.useFetchCache=false
spark.hadoop.databricks.dbfs.client.version=v2
spark.hadoop.databricks.s3commit.client.sslTrustAll=false
spark.hadoop.fs.abfs.impl=shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem
spark.hadoop.fs.abfs.impl.disable.cache=true
spark.hadoop.fs.abfss.impl=shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem
spark.hadoop.fs.abfss.impl.disable.cache=true
spark.hadoop.fs.adl.impl=com.databricks.adl.AdlFileSystem
spark.hadoop.fs.adl.impl.disable.cache=true
spark.hadoop.fs.azure.skip.metrics=true
spark.hadoop.fs.s3.impl=com.databricks.s3a.S3AFileSystem
spark.hadoop.fs.s3a.connection.maximum=200
spark.hadoop.fs.s3a.fast.upload=true
spark.hadoop.fs.s3a.fast.upload.default=true
spark.hadoop.fs.s3a.impl=com.databricks.s3a.S3AFileSystem
spark.hadoop.fs.s3a.multipart.size=10485760
spark.hadoop.fs.s3a.multipart.threshold=104857600
spark.hadoop.fs.s3a.threads.max=136
spark.hadoop.fs.s3n.impl=com.databricks.s3a.S3AFileSystem
spark.hadoop.fs.wasb.impl=shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem
spark.hadoop.fs.wasb.impl.disable.cache=true
spark.hadoop.fs.wasbs.impl=shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem
spark.hadoop.fs.wasbs.impl.disable.cache=true
spark.hadoop.hive.server2.enable.doAs=false
spark.hadoop.hive.server2.idle.operation.timeout=7200000
spark.hadoop.hive.server2.idle.session.timeout=900000
spark.hadoop.hive.server2.keystore.password=*********(redacted)
spark.hadoop.hive.server2.keystore.path=/databricks/keys/jetty-ssl-driver-keystore.jks
spark.hadoop.hive.server2.session.check.interval=60000
spark.hadoop.hive.server2.thrift.http.cookie.auth.enabled=false
spark.hadoop.hive.server2.thrift.http.port=10000
spark.hadoop.hive.server2.transport.mode=http
spark.hadoop.hive.server2.use.SSL=true
spark.hadoop.hive.warehouse.subdir.inherit.perms=false
spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,org.apache.hadoop.io.compress.GzipCodec
spark.hadoop.mapred.output.committer.class=com.databricks.backend.daemon.data.client.DirectOutputCommitter
spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1099511627776
spark.hadoop.parquet.block.size=1099511627776
spark.hadoop.parquet.memory.pool.ratio=0.5
spark.hadoop.spark.sql.parquet.output.committer.class=org.apache.spark.sql.parquet.DirectParquetOutputCommitter
spark.hadoop.spark.sql.sources.outputCommitterClass=com.databricks.backend.daemon.data.client.MapReduceDirectOutputCommitter
spark.hadoop.spark.thriftserver.customHeadersToProperties=*********(redacted)
spark.home=/databricks/spark
spark.logConf=true
spark.master=spark://10.67.237.124:7077
spark.metrics.conf=/databricks/spark/conf/metrics.properties
spark.r.backendConnectionTimeout=604800
spark.r.numRBackendThreads=1
spark.rdd.compress=true
spark.repl.class.outputDir=/local_disk0/tmp/repl/spark-4399150706229039107-01be5a39-c211-4a4c-a096-05665e2ccbe7
spark.rpc.message.maxSize=256
spark.scheduler.listenerbus.eventqueue.capacity=20000
spark.scheduler.mode=FAIR
spark.serializer.objectStreamReset=100
spark.shuffle.manager=SORT
spark.shuffle.memoryFraction=0.2
spark.shuffle.reduceLocality.enabled=false
spark.shuffle.service.enabled=true
spark.shuffle.service.port=4048
spark.sparkr.use.daemon=false
spark.speculation=false
spark.speculation.multiplier=3
spark.speculation.quantile=0.9
spark.sql.allowMultipleContexts=false
spark.sql.files.maxPartitionBytes=1099511627776
spark.sql.files.openCostInBytes=1099511627776
spark.sql.hive.convertCTAS=true
spark.sql.hive.convertMetastoreParquet=true
spark.sql.hive.metastore.jars=/databricks/hive/*
spark.sql.hive.metastore.sharedPrefixes=org.mariadb.jdbc,com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,microsoft.sql.DateTimeOffset,microsoft.sql.Types,com.databricks,com.codahale,com.fasterxml.jackson,shaded.databricks
spark.sql.hive.metastore.version=0.13.0
spark.sql.parquet.cacheMetadata=true
spark.sql.parquet.compression.codec=snappy
spark.sql.sources.commitProtocolClass=com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol
spark.sql.streaming.checkpointFileManagerClass=com.databricks.spark.sql.streaming.DatabricksCheckpointFileManager
spark.sql.ui.retainedExecutions=100
spark.sql.warehouse.dir=*********(redacted)
spark.storage.blockManagerTimeoutIntervalMs=300000
spark.storage.memoryFraction=0.5
spark.streaming.driver.writeAheadLog.allowBatching=true
spark.streaming.driver.writeAheadLog.closeFileAfterWrite=true
spark.task.reaper.enabled=true
spark.task.reaper.killTimeout=60s
spark.ui.port=48182
spark.worker.cleanup.enabled=false
19/08/12 22:48:19 INFO SecurityManager: Changing view acls to: root
19/08/12 22:48:19 INFO SecurityManager: Changing modify acls to: root
19/08/12 22:48:19 INFO SecurityManager: Changing view acls groups to: 
19/08/12 22:48:19 INFO SecurityManager: Changing modify acls groups to: 
19/08/12 22:48:19 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
19/08/12 22:48:19 INFO Utils: Successfully started service 'sparkDriver' on port 44982.
19/08/12 22:48:19 INFO SparkEnv: Registering MapOutputTracker
19/08/12 22:48:19 INFO SparkEnv: Registering BlockManagerMaster
19/08/12 22:48:19 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
19/08/12 22:48:19 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
19/08/12 22:48:19 INFO DiskBlockManager: Created local directory at /local_disk0/blockmgr-abb5532e-a253-4af7-82e3-4d56cff0c804
19/08/12 22:48:19 INFO MemoryStore: MemoryStore started with capacity 24.3 GB
19/08/12 22:48:19 INFO SparkEnv: Registering OutputCommitCoordinator
19/08/12 22:48:19 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
19/08/12 22:48:19 INFO log: Logging initialized @4444ms
19/08/12 22:48:19 INFO Server: jetty-9.3.20.v20170531
19/08/12 22:48:19 INFO Server: Started @4540ms


















19/08/12 22:48:19 INFO AbstractConnector: Started ServerConnector@6ba060f3{HTTP/1.1,[http/1.1]}{10.67.237.124:48182}
19/08/12 22:48:19 INFO Utils: Successfully started service 'SparkUI' on port 48182.
19/08/12 22:48:19 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@19e0dffe{/jobs,null,AVAILABLE,@Spark}
19/08/12 22:48:19 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@31142d58{/jobs/json,null,AVAILABLE,@Spark}
19/08/12 22:48:19 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@e38f0b7{/jobs/job,null,AVAILABLE,@Spark}
19/08/12 22:48:19 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4f8d86e4{/jobs/job/json,null,AVAILABLE,@Spark}
19/08/12 22:48:19 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5f631ca0{/stages,null,AVAILABLE,@Spark}
19/08/12 22:48:19 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1697f2b3{/stages/json,null,AVAILABLE,@Spark}
19/08/12 22:48:19 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@12ad1b2a{/stages/stage,null,AVAILABLE,@Spark}
19/08/12 22:48:19 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5f5effb0{/stages/stage/json,null,AVAILABLE,@Spark}
19/08/12 22:48:19 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@25d0cb3a{/stages/pool,null,AVAILABLE,@Spark}
19/08/12 22:48:19 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@345cbf40{/stages/pool/json,null,AVAILABLE,@Spark}
19/08/12 22:48:19 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6e3eb0cd{/storage,null,AVAILABLE,@Spark}
19/08/12 22:48:19 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@463561c5{/storage/json,null,AVAILABLE,@Spark}
19/08/12 22:48:19 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@659feb22{/storage/rdd,null,AVAILABLE,@Spark}
19/08/12 22:48:19 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3468ee6e{/storage/rdd/json,null,AVAILABLE,@Spark}
19/08/12 22:48:19 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2f4b98f6{/environment,null,AVAILABLE,@Spark}
19/08/12 22:48:19 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@421def93{/environment/json,null,AVAILABLE,@Spark}
19/08/12 22:48:19 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@58c1da09{/executors,null,AVAILABLE,@Spark}
19/08/12 22:48:19 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2b2954e1{/executors/json,null,AVAILABLE,@Spark}
19/08/12 22:48:19 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@58d6e55a{/executors/threadDump,null,AVAILABLE,@Spark}
19/08/12 22:48:19 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@751ae8a4{/executors/threadDump/json,null,AVAILABLE,@Spark}
19/08/12 22:48:19 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@235d659c{/executors/heapHistogram,null,AVAILABLE,@Spark}
19/08/12 22:48:19 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4232b34a{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
19/08/12 22:48:19 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2da16263{/static,null,AVAILABLE,@Spark}
19/08/12 22:48:19 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@49fa1d74{/,null,AVAILABLE,@Spark}
19/08/12 22:48:19 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3f362135{/api,null,AVAILABLE,@Spark}
19/08/12 22:48:19 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7cfb4736{/jobs/job/kill,null,AVAILABLE,@Spark}
19/08/12 22:48:19 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2a097d77{/stages/stage/kill,null,AVAILABLE,@Spark}
19/08/12 22:48:19 INFO SparkUI: Bound SparkUI to 10.67.237.124, and started at http://10.67.237.124:48182
19/08/12 22:48:19 WARN FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.
19/08/12 22:48:19 INFO FairSchedulableBuilder: Created default pool: default, schedulingMode: FIFO, minShare: 0, weight: 1
19/08/12 22:48:20 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://10.67.237.124:7077...
19/08/12 22:48:20 INFO TransportClientFactory: Successfully created connection to /10.67.237.124:7077 after 48 ms (0 ms spent in bootstraps)
19/08/12 22:48:20 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20190812224820-0000
19/08/12 22:48:20 INFO TaskSchedulerImpl: Task preemption enabled.
19/08/12 22:48:20 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36746.
19/08/12 22:48:20 INFO NettyBlockTransferService: Server created on ip-10-67-237-124.ec2.internal:36746
19/08/12 22:48:20 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
19/08/12 22:48:20 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ip-10-67-237-124.ec2.internal, 36746, None)
19/08/12 22:48:20 INFO BlockManagerMasterEndpoint: Registering block manager ip-10-67-237-124.ec2.internal:36746 with 24.3 GB RAM, BlockManagerId(driver, ip-10-67-237-124.ec2.internal, 36746, None)
19/08/12 22:48:20 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ip-10-67-237-124.ec2.internal, 36746, None)
19/08/12 22:48:20 INFO BlockManager: external shuffle service port = 4048
19/08/12 22:48:20 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ip-10-67-237-124.ec2.internal, 36746, None)
19/08/12 22:48:20 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4dad0eed{/metrics/json,null,AVAILABLE,@Spark}
19/08/12 22:48:20 INFO DBCEventLoggingListener: Initializing DBCEventLoggingListener
19/08/12 22:48:20 INFO DBCEventLoggingListener: Logging events to eventlogs/4399150706229039107/eventlog
19/08/12 22:48:20 INFO SparkContext: Registered listener com.databricks.backend.daemon.driver.DBCEventLoggingListener
19/08/12 22:48:21 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
19/08/12 22:48:21 INFO SparkContext: Loading Spark Service RPC Server
19/08/12 22:48:21 INFO SparkServiceRPCServer: Starting Spark Service RPC Server
19/08/12 22:48:21 INFO Server: jetty-9.3.20.v20170531
19/08/12 22:48:21 INFO AbstractConnector: Started ServerConnector@58324c9f{HTTP/1.1,[http/1.1]}{0.0.0.0:15001}
19/08/12 22:48:21 INFO Server: Started @5950ms
19/08/12 22:48:21 INFO DatabricksILoop$: Successfully registered spark metrics in Prometheus registry
19/08/12 22:48:21 INFO DatabricksILoop$: Successfully initialized SparkContext
19/08/12 22:48:21 INFO SharedState: Scheduler stats enabled.
19/08/12 22:48:21 INFO SharedState: loading hive config file: file:/databricks/hive/conf/hive-site.xml
19/08/12 22:48:21 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('/user/hive/warehouse').
19/08/12 22:48:21 INFO SharedState: Warehouse path is '/user/hive/warehouse'.
19/08/12 22:48:21 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@9a20cbd{/SQL,null,AVAILABLE,@Spark}
19/08/12 22:48:21 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1af4955e{/SQL/json,null,AVAILABLE,@Spark}
19/08/12 22:48:21 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@bb25753{/SQL/execution,null,AVAILABLE,@Spark}
19/08/12 22:48:21 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@ee21292{/SQL/execution/json,null,AVAILABLE,@Spark}
19/08/12 22:48:21 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2282400e{/static/sql,null,AVAILABLE,@Spark}
19/08/12 22:48:21 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4d93f75b{/storage/iocache,null,AVAILABLE,@Spark}
19/08/12 22:48:21 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@114a5e0{/storage/iocache/json,null,AVAILABLE,@Spark}
19/08/12 22:48:21 INFO DatabricksILoop$: Finished creating throwaway interpreter
19/08/12 22:48:21 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
19/08/12 22:48:23 INFO JettyClient$: Creating new HttpClient with SSLContextFactory=None,maxRequestHeaderSize=65536, namePrefix=Some(DBFS-SHARED), idleTimeout=2 hours, useBlockingConnect: true
19/08/12 22:48:23 INFO DatabricksFileSystemV2Factory: Creating S3A file system for s3a://databrickstombresee
19/08/12 22:48:24 INFO DBFS: Initialized DBFS with DBFSV2 as the delegate.
19/08/12 22:48:24 INFO HiveUtils: Initializing execution hive, version 1.2.1
19/08/12 22:48:25 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
19/08/12 22:48:25 INFO ObjectStore: ObjectStore, initialize called
19/08/12 22:48:25 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
19/08/12 22:48:25 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
19/08/12 22:48:26 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20190812224820-0000/0 on worker-20190812224825-10.67.233.230-44554 (10.67.233.230:44554) with 8 core(s)
19/08/12 22:48:26 INFO StandaloneSchedulerBackend: Granted executor ID app-20190812224820-0000/0 on hostPort 10.67.233.230:44554 with 8 core(s), 43.6 GB RAM
19/08/12 22:48:26 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20190812224820-0000/0 is now RUNNING
19/08/12 22:48:26 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
19/08/12 22:48:28 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
19/08/12 22:48:28 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
19/08/12 22:48:28 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
19/08/12 22:48:28 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
19/08/12 22:48:28 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.67.233.230:57898) with ID 0
19/08/12 22:48:29 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
19/08/12 22:48:29 INFO ObjectStore: Initialized ObjectStore
19/08/12 22:48:29 INFO BlockManagerMasterEndpoint: Registering block manager 10.67.233.230:44409 with 23.1 GB RAM, BlockManagerId(0, 10.67.233.230, 44409, None)
19/08/12 22:48:29 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
19/08/12 22:48:29 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
19/08/12 22:48:29 INFO HiveMetaStore: Added admin role in metastore
19/08/12 22:48:29 INFO HiveMetaStore: Added public role in metastore
19/08/12 22:48:29 INFO HiveMetaStore: No user is added in admin role, since config is empty
19/08/12 22:48:29 INFO HiveMetaStore: 0: get_all_databases
19/08/12 22:48:29 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_all_databases	
19/08/12 22:48:29 INFO HiveMetaStore: 0: get_functions: db=default pat=*
19/08/12 22:48:29 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
19/08/12 22:48:29 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
19/08/12 22:48:29 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20190812224820-0000/1 on worker-20190812224829-10.67.245.78-36736 (10.67.245.78:36736) with 8 core(s)
19/08/12 22:48:29 INFO StandaloneSchedulerBackend: Granted executor ID app-20190812224820-0000/1 on hostPort 10.67.245.78:36736 with 8 core(s), 43.6 GB RAM
19/08/12 22:48:29 INFO SessionState: Created local directory: /local_disk0/tmp/root
19/08/12 22:48:29 INFO SessionState: Created local directory: /local_disk0/tmp/262f50a7-0123-4b75-a135-3990ece9b31f_resources
19/08/12 22:48:29 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20190812224820-0000/1 is now RUNNING
19/08/12 22:48:29 INFO S3AFileSystem: Making directory: /oregon-prod/978277182616062/tmp/hive/root/262f50a7-0123-4b75-a135-3990ece9b31f
19/08/12 22:48:30 INFO SessionState: Created HDFS directory: /tmp/hive/root/262f50a7-0123-4b75-a135-3990ece9b31f
19/08/12 22:48:30 INFO SessionState: Created local directory: /local_disk0/tmp/root/262f50a7-0123-4b75-a135-3990ece9b31f
19/08/12 22:48:30 INFO S3AFileSystem: Making directory: /oregon-prod/978277182616062/tmp/hive/root/262f50a7-0123-4b75-a135-3990ece9b31f/_tmp_space.db
19/08/12 22:48:30 INFO SessionState: Created HDFS directory: /tmp/hive/root/262f50a7-0123-4b75-a135-3990ece9b31f/_tmp_space.db
19/08/12 22:48:30 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.2) is /user/hive/warehouse
19/08/12 22:48:30 INFO SessionManager: Operation log root directory is created: /local_disk0/tmp/root/operation_logs
19/08/12 22:48:30 INFO SessionManager: HiveServer2: Background operation thread pool size: 100
19/08/12 22:48:30 INFO SessionManager: HiveServer2: Background operation thread wait queue size: 100
19/08/12 22:48:30 INFO SessionManager: HiveServer2: Background operation thread keepalive time: 10 seconds
19/08/12 22:48:30 INFO AbstractService: Service:OperationManager is inited.
19/08/12 22:48:30 INFO AbstractService: Service:SessionManager is inited.
19/08/12 22:48:30 INFO AbstractService: Service: CLIService is inited.
19/08/12 22:48:30 INFO AbstractService: Service:ThriftHttpCLIService is inited.
19/08/12 22:48:30 INFO AbstractService: Service: HiveServer2 is inited.
19/08/12 22:48:30 INFO AbstractService: Service:OperationManager is started.
19/08/12 22:48:30 INFO AbstractService: Service:SessionManager is started.
19/08/12 22:48:30 INFO AbstractService: Service:CLIService is started.
19/08/12 22:48:30 INFO ObjectStore: ObjectStore, initialize called
19/08/12 22:48:30 INFO Query: Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
19/08/12 22:48:30 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
19/08/12 22:48:30 INFO ObjectStore: Initialized ObjectStore
19/08/12 22:48:30 INFO HiveMetaStore: 0: get_databases: default
19/08/12 22:48:30 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_databases: default	
19/08/12 22:48:30 INFO HiveMetaStore: 0: Shutting down the object store...
19/08/12 22:48:30 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=Shutting down the object store...	
19/08/12 22:48:30 INFO HiveMetaStore: 0: Metastore shutdown complete.
19/08/12 22:48:30 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
19/08/12 22:48:30 INFO AbstractService: Service:ThriftHttpCLIService is started.
19/08/12 22:48:30 INFO AbstractService: Service:HiveServer2 is started.
19/08/12 22:48:30 INFO ThriftCLIService: HTTP Server SSL: adding excluded protocols: [SSLv2, SSLv3]
19/08/12 22:48:30 INFO ThriftCLIService: HTTP Server SSL: SslContextFactory.getExcludeProtocols = [SSL, SSLv2, SSLv2Hello, SSLv3]
19/08/12 22:48:30 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1259b2a5{/sqlserver,null,AVAILABLE,@Spark}
19/08/12 22:48:30 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6779af40{/sqlserver/json,null,AVAILABLE,@Spark}
19/08/12 22:48:30 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1d0acb8f{/sqlserver/session,null,AVAILABLE,@Spark}
19/08/12 22:48:30 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6abaa14b{/sqlserver/session/json,null,AVAILABLE,@Spark}
19/08/12 22:48:30 INFO DriverDaemon: Starting driver daemon...
19/08/12 22:48:30 INFO SparkConfUtils$: Customize spark config according to file /tmp/custom-spark.conf
19/08/12 22:48:30 INFO SparkConfUtils$: new spark config: spark.sql.files.maxPartitionBytes -> 1099511627776
19/08/12 22:48:30 INFO SparkConfUtils$: new spark config: spark.sql.files.openCostInBytes -> 1099511627776
19/08/12 22:48:30 INFO SparkConfUtils$: new spark config: spark.executor.tempDirectory -> /local_disk0/tmp
19/08/12 22:48:30 INFO SparkConfUtils$: new spark config: spark.hadoop.mapreduce.input.fileinputformat.split.minsize -> 1099511627776
19/08/12 22:48:30 INFO SparkConfUtils$: new spark config: spark.hadoop.parquet.block.size -> 1099511627776
19/08/12 22:48:30 INFO SparkConfUtils$: new spark config: spark.driver.tempDirectory -> /local_disk0/tmp
19/08/12 22:48:30 INFO SparkConfUtils$: new spark config: spark.hadoop.io.compression.codecs -> org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,org.apache.hadoop.io.compress.GzipCodec
19/08/12 22:48:30 WARN SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
19/08/12 22:48:30 INFO Server: jetty-9.3.20.v20170531
19/08/12 22:48:30 INFO DriverDaemon$$anon$1: Message out thread ready
19/08/12 22:48:30 INFO AbstractConnector: Started ServerConnector@1fa77770{HTTP/1.1,[http/1.1]}{0.0.0.0:6061}
19/08/12 22:48:30 INFO Server: Started @15650ms
19/08/12 22:48:30 INFO DriverDaemon: Driver daemon started.
19/08/12 22:48:30 INFO Server: jetty-9.3.20.v20170531
19/08/12 22:48:30 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@37503a76{/,null,AVAILABLE}
19/08/12 22:48:30 INFO SslContextFactory: x509=X509@38af93be(1,h=[databrickscloud.com],w=[]) for SslContextFactory@2234b20d(file:///databricks/keys/jetty-ssl-driver-keystore.jks,null)
19/08/12 22:48:30 INFO AbstractConnector: Started ServerConnector@a0ca959{SSL,[ssl, http/1.1]}{0.0.0.0:10000}
19/08/12 22:48:30 INFO Server: Started @15776ms
19/08/12 22:48:30 INFO ThriftCLIService: Started ThriftHttpCLIService in https mode on port 10000 path=/cliservice/* with 5...500 worker threads
19/08/12 22:48:31 INFO DriverCorral: Loading the root classloader
19/08/12 22:48:31 INFO DriverCorral: Starting sql repl ReplId-614e0-c5ef1-dcdf2-0
19/08/12 22:48:31 INFO DriverCorral: Starting sql repl ReplId-4e724-8bdb0-2aaed-b
19/08/12 22:48:31 WARN SQLDriverLocal: loadLibraries: Libraries failed to be installed: Set()
19/08/12 22:48:31 WARN SQLDriverLocal: loadLibraries: Libraries failed to be installed: Set()
19/08/12 22:48:31 INFO DriverCorral: Starting sql repl ReplId-35a06-4ca12-4bdac-b
19/08/12 22:48:31 WARN SQLDriverLocal: loadLibraries: Libraries failed to be installed: Set()
19/08/12 22:48:31 INFO SQLDriverWrapper: setupRepl:ReplId-4e724-8bdb0-2aaed-b: finished to load
19/08/12 22:48:31 INFO SQLDriverWrapper: setupRepl:ReplId-614e0-c5ef1-dcdf2-0: finished to load
19/08/12 22:48:31 INFO SQLDriverWrapper: setupRepl:ReplId-35a06-4ca12-4bdac-b: finished to load
19/08/12 22:48:31 INFO DriverCorral: Starting sql repl ReplId-30e7f-ad762-6fd2d-d
19/08/12 22:48:31 WARN SQLDriverLocal: loadLibraries: Libraries failed to be installed: Set()
19/08/12 22:48:31 INFO SQLDriverWrapper: setupRepl:ReplId-30e7f-ad762-6fd2d-d: finished to load
19/08/12 22:48:31 INFO DriverCorral: Starting sql repl ReplId-3adb2-098d5-0b98c-a
19/08/12 22:48:31 WARN SQLDriverLocal: loadLibraries: Libraries failed to be installed: Set()
19/08/12 22:48:31 INFO SQLDriverWrapper: setupRepl:ReplId-3adb2-098d5-0b98c-a: finished to load
19/08/12 22:48:31 INFO DriverCorral: Starting r repl ReplId-5dd1b-67f92-e0d6b-1
19/08/12 22:48:31 WARN RDriverLocal: loadLibraries: Libraries failed to be installed: Set()
19/08/12 22:48:31 INFO RDriverLocal: 1. RDriverLocal.f84c38b9-e4aa-4fdd-b5da-a468b02b9410: object created with for ReplId-5dd1b-67f92-e0d6b-1.
19/08/12 22:48:31 INFO RDriverLocal: 2. RDriverLocal.f84c38b9-e4aa-4fdd-b5da-a468b02b9410: initializing ...
19/08/12 22:48:31 INFO RDriverLocal: 3. RDriverLocal.f84c38b9-e4aa-4fdd-b5da-a468b02b9410: started RBackend thread on port 41386
19/08/12 22:48:31 INFO RDriverLocal: 4. RDriverLocal.f84c38b9-e4aa-4fdd-b5da-a468b02b9410: waiting for SparkR to be installed ...
19/08/12 22:48:32 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.67.245.78:38952) with ID 1
19/08/12 22:48:32 INFO BlockManagerMasterEndpoint: Registering block manager 10.67.245.78:35595 with 23.1 GB RAM, BlockManagerId(1, 10.67.245.78, 35595, None)
19/08/12 22:48:33 INFO DriverCorral: AttachLibraries - candidate libraries: List(JavaJarId(dbfs:/FileStore/jars/5698d3bc_4eae_4017_adc7_a960409bcd16-hail_all_spark-71440.jar,,NONE))
19/08/12 22:48:33 INFO LibraryDownloadManager: Downloading a library that was not in the cache: JavaJarId(dbfs:/FileStore/jars/5698d3bc_4eae_4017_adc7_a960409bcd16-hail_all_spark-71440.jar,,NONE)
19/08/12 22:48:33 INFO LibraryDownloadManager: Downloaded library dbfs:/FileStore/jars/5698d3bc_4eae_4017_adc7_a960409bcd16-hail_all_spark-71440.jar as local file /local_disk0/tmp/addedFile8128349861081662655698d3bc_4eae_4017_adc7_a960409bcd16_hail_all_spark_71440-df353.jar
19/08/12 22:48:33 INFO SharedDriverContext: Adding libraries that haven't been added before: Set(/local_disk0/tmp/addedFile8128349861081662655698d3bc_4eae_4017_adc7_a960409bcd16_hail_all_spark_71440-df353.jar)
19/08/12 22:48:33 INFO SparkContext: Added file /local_disk0/tmp/addedFile8128349861081662655698d3bc_4eae_4017_adc7_a960409bcd16_hail_all_spark_71440-df353.jar at spark://ip-10-67-237-124.ec2.internal:44982/files/addedFile8128349861081662655698d3bc_4eae_4017_adc7_a960409bcd16_hail_all_spark_71440-df353.jar with timestamp 1565650113626
19/08/12 22:48:33 INFO Utils: Copying /local_disk0/tmp/addedFile8128349861081662655698d3bc_4eae_4017_adc7_a960409bcd16_hail_all_spark_71440-df353.jar to /local_disk0/spark-7ccee8e8-33a9-4e7e-9e18-0134f7a3a528/userFiles-8b0e3ed7-00c5-4c01-aee3-7f18fcee7619/addedFile8128349861081662655698d3bc_4eae_4017_adc7_a960409bcd16_hail_all_spark_71440-df353.jar
19/08/12 22:48:33 INFO SparkContext: Added JAR /local_disk0/tmp/addedFile8128349861081662655698d3bc_4eae_4017_adc7_a960409bcd16_hail_all_spark_71440-df353.jar at spark://ip-10-67-237-124.ec2.internal:44982/jars/addedFile8128349861081662655698d3bc_4eae_4017_adc7_a960409bcd16_hail_all_spark_71440-df353.jar with timestamp 1565650113665
19/08/12 22:48:33 INFO DriverCorral: Successfully attached library dbfs:/FileStore/jars/5698d3bc_4eae_4017_adc7_a960409bcd16-hail_all_spark-71440.jar to Spark
19/08/12 22:48:33 INFO LibraryState: Successfully attached library dbfs:/FileStore/jars/5698d3bc_4eae_4017_adc7_a960409bcd16-hail_all_spark-71440.jar
19/08/12 22:48:45 INFO RDriverLocal$: SparkR installation completed.
19/08/12 22:48:45 INFO RDriverLocal: 5. RDriverLocal.f84c38b9-e4aa-4fdd-b5da-a468b02b9410: launching R process ...
19/08/12 22:48:45 INFO RDriverLocal: 6. RDriverLocal.f84c38b9-e4aa-4fdd-b5da-a468b02b9410: cgroup isolation disabled, not placing R process in REPL cgroup.
19/08/12 22:48:45 INFO RDriverLocal: 7. RDriverLocal.f84c38b9-e4aa-4fdd-b5da-a468b02b9410: starting R process on port 1100 (attempt 1) ...
19/08/12 22:48:45 INFO RDriverLocal: 8. RDriverLocal.f84c38b9-e4aa-4fdd-b5da-a468b02b9410: setting up BufferedStreamThread with bufferSize: 100.
19/08/12 22:48:46 INFO RDriverLocal: 9. RDriverLocal.f84c38b9-e4aa-4fdd-b5da-a468b02b9410: R process started with RServe listening on port 1100.
19/08/12 22:48:47 INFO RDriverLocal: 10. RDriverLocal.f84c38b9-e4aa-4fdd-b5da-a468b02b9410: starting interpreter to talk to R process ...
19/08/12 22:48:48 WARN SparkContext: Using an existing SparkContext; some configuration may not take effect.
19/08/12 22:48:48 INFO RDriverLocal: 11. RDriverLocal.f84c38b9-e4aa-4fdd-b5da-a468b02b9410: R interpretter is connected.
19/08/12 22:48:48 INFO RDriverWrapper: setupRepl:ReplId-5dd1b-67f92-e0d6b-1: finished to load
19/08/12 22:50:41 INFO DriverCorral: Starting python repl ReplId-53df2-c551f-c5fd9-5
19/08/12 22:50:41 WARN PythonDriverLocal: loadLibraries: Libraries failed to be installed: Set()
19/08/12 22:50:42 INFO LogicalPlanStats: Setting LogicalPlanStats visitor to com.databricks.sql.optimizer.statsEstimation.DatabricksLogicalPlanStatsVisitor$
19/08/12 22:50:42 INFO DriverCorral: Starting sql repl ReplId-79b8a-eba9e-f9eed-6
19/08/12 22:50:42 WARN SQLDriverLocal: loadLibraries: Libraries failed to be installed: Set()
19/08/12 22:50:42 INFO LogicalPlanStats: Setting LogicalPlanStats visitor to com.databricks.sql.optimizer.statsEstimation.DatabricksLogicalPlanStatsVisitor$
19/08/12 22:50:42 INFO HiveUtils: Initializing HiveMetastoreConnection version 0.13.0 using file:/databricks/hive/maven--spark_1.4--com.esotericsoftware.reflectasm--reflectasm-shaded--com.esotericsoftware.reflectasm__reflectasm-shaded__1.07.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.scalatest--scalatest_2.11--org.scalatest__scalatest_2.11__3.0.3.jar:file:/databricks/hive/maven--spark_1.4--org.datanucleus--datanucleus-core--org.datanucleus__datanucleus-core__3.2.10.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__okhttp__3.3.1_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/third_party--azure--org.apache.commons__commons-lang3__3.4_shaded.jar:file:/databricks/hive/maven--spark_1.4--org.spark-project.hive--hive-common--org.spark-project.hive__hive-common__0.13.1a.jar:file:/databricks/hive/maven--spark_1.4--org.antlr--ST4--org.antlr__ST4__4.0.4.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-jetty9--io.dropwizard.metrics__metrics-jetty9__3.1.5.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-security--org.eclipse.jetty__jetty-security__9.3.20.v20170531.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.slf4j--slf4j-log4j12--org.slf4j__slf4j-log4j12__1.7.16.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.datatype__jackson-datatype-joda__2.7.2_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/extern--acl--auth--auth-spark_2.4_2.11_deploy.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-sts--com.amazonaws__aws-java-sdk-sts__1.11.313.jar:file:/databricks/hive/maven--spark_1.4--org.apache.velocity--velocity--org.apache.velocity__velocity__1.5.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.htrace--htrace-core--org.apache.htrace__htrace-core__3.1.0-incubating.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-continuation--org.eclipse.jetty__jetty-continuation__9.3.20.v20170531.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-annotations__1.2.0_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/third_party--azure--com.fasterxml.jackson.core__jackson-core__2.7.2_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.twitter--util-jvm_2.11--com.twitter__util-jvm_2.11__6.23.0.jar:file:/databricks/hive/maven--log4j--log4j--log4j__log4j__1.2.17.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.trueaccord.lenses--lenses_2.11--com.trueaccord.lenses__lenses_2.11__0.3.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-log4j--io.dropwizard.metrics__metrics-log4j__3.1.5.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-20180920_b33d810-spark_2.4_2.11_deploy_shaded.jar:file:/databricks/hive/maven--commons-logging--commons-logging--commons-logging__commons-logging__1.1.3.jar:file:/databricks/hive/maven--spark_1.4--org.antlr--antlr-runtime--org.antlr__antlr-runtime__3.4.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.directory.server--apacheds-i18n--org.apache.directory.server__apacheds-i18n__2.0.0-M15.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__logging-interceptor__3.3.1_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/third_party--jetty8-shaded-client--databricks-patched-jetty-client-jar_shaded.jar:file:/databricks/hive/maven--spark_1.4--org.spark-project.hive--hive-serde--org.spark-project.hive__hive-serde__0.13.1a.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--software.amazon.ion--ion-java--software.amazon.ion__ion-java__1.0.2.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.scala-lang.modules--scala-xml_2.11--org.scala-lang.modules__scala-xml_2.11__1.0.5.jar:file:/databricks/hive/maven--commons-io--commons-io--commons-io__commons-io__2.5.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--javax.inject__javax.inject__1_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.xerial.snappy--snappy-java--org.xerial.snappy__snappy-java__1.1.7.3.jar:file:/databricks/hive/maven--commons-codec--commons-codec--commons-codec__commons-codec__1.8.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.microsoft.azure--azure-data-lake-store-sdk--com.microsoft.azure__azure-data-lake-store-sdk__2.2.8.jar:file:/databricks/hive/maven--spark_1.4--org.apache.avro--avro--org.apache.avro__avro__1.8.2.jar:file:/databricks/hive/maven--spark_1.4--org.datanucleus--datanucleus-api-jdo--org.datanucleus__datanucleus-api-jdo__3.2.6.jar:file:/databricks/hive/maven--spark_1.4--javax.jdo--jdo-api--javax.jdo__jdo-api__3.0.1.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.fasterxml--classmate--com.fasterxml__classmate__1.0.0.jar:file:/databricks/hive/maven--org.slf4j--slf4j-log4j12--org.slf4j__slf4j-log4j12__1.7.5.jar:file:/databricks/hive/third_party--datalake--datalake-spark_2.4_2.11_deploy.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.joda--joda-convert--org.joda__joda-convert__1.7.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-servlets--io.dropwizard.metrics__metrics-servlets__3.1.5.jar:file:/databricks/hive/maven--spark_1.4--javax.transaction--jta--javax.transaction__jta__1.1.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--javax.xml.bind__jaxb-api__2.2.2_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/maven--org.json--json--org.json__json__20090211.jar:file:/databricks/hive/api-base--api-base-spark_2.4_2.11_deploy.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.typesafe.scala-logging--scala-logging-slf4j_2.11--com.typesafe.scala-logging__scala-logging-slf4j_2.11__2.1.2.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpclient__4.5.2_2.11_shaded_20180920_b33d810.jar:file:/databricks/hive/maven--spark_1.4--com.esotericsoftware.minlog--minlog--com.esotericsoftware.minlog__minlog__1.2.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.directory.api--api-asn1-api--org.apache.directory.api__api-asn1-api__1.0.0-M20.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--commons-collections--commons-collections--commons-collections__commons-collections__3.2.2.jar:file:/databricks/hive/maven--spark_1.4--org.spark-project.hive--hive-cli--org.spark-project.hive__hive-cli__0.13.1a.jar:file:/databricks/hive/third_party--jetty8-shaded-client--jetty-jmx_shaded.jar:file:/databricks/hive/common--jetty--client--client-spark_2.4_2.11_deploy.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.databricks.scalapb--scalapb-runtime_2.11--com.databricks.scalapb__scalapb-runtime_2.11__0.4.15-9.jar:file:/databricks/hive/maven--spark_1.4--commons-cli--commons-cli--commons-cli__commons-cli__1.2.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--javax.activation__activation__1.1_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/maven--spark_1.4--org.codehaus.jackson--jackson-core-asl--org.codehaus.jackson__jackson-core-asl__1.9.13.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.fasterxml.jackson.core--jackson-core--com.fasterxml.jackson.core__jackson-core__2.6.7.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-json--io.dropwizard.metrics__metrics-json__3.1.5.jar:file:/databricks/hive/maven--org.apache.httpcomponents--httpclient--org.apache.httpcomponents__httpclient__4.4.1.jar:file:/databricks/hive/maven--spark_1.4--io.netty--netty--io.netty__netty__3.8.0.Final.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.twitter--util-app_2.11--com.twitter__util-app_2.11__6.23.0.jar:file:/databricks/hive/maven--spark_1.4--org.spark-project.hive--hive-shims--org.spark-project.hive__hive-shims__0.13.1a.jar:file:/databricks/hive/maven--spark_1.4--org.codehaus.jackson--jackson-mapper-asl--org.codehaus.jackson__jackson-mapper-asl__1.9.13.jar:file:/databricks/hive/daemon--data--client--client-spark_2.4_2.11_deploy.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.twitter--util-core_2.11--com.twitter__util-core_2.11__6.23.0.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--log4j--log4j--log4j__log4j__1.2.17.jar:file:/databricks/hive/common--path--path-spark_2.4_2.11_deploy.jar:file:/databricks/hive/maven--spark_1.4--net.sf.jpam--jpam--net.sf.jpam__jpam__1.1.jar:file:/databricks/hive/third_party--prometheus-client--simpleclient-spark_2.4_2.11_deploy.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-storage__7.0.0_2.11_shaded_20180920_b33d810.jar:file:/databricks/hive/third_party--jackson--guava_only_shaded.jar:file:/databricks/hive/maven--org.apache.commons--commons-lang3--org.apache.commons__commons-lang3__3.4.jar:file:/databricks/hive/maven--spark_1.4--org.spark-project.hive--hive-ant--org.spark-project.hive__hive-ant__0.13.1a.jar:file:/databricks/hive/maven--spark_1.4--com.twitter--parquet-hadoop-bundle--com.twitter__parquet-hadoop-bundle__1.3.2.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--log4j--apache-log4j-extras--log4j__apache-log4j-extras__1.2.17.jar:file:/databricks/hive/jsonutil--jsonutil-spark_2.4_2.11_deploy.jar:file:/databricks/hive/daemon--data--data-common--data-common-spark_2.4_2.11_deploy.jar:file:/databricks/hive/extern--extern-spark_2.4_2.11_deploy.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.jdbi--jdbi--org.jdbi__jdbi__2.63.1.jar:file:/databricks/hive/s3commit--common--common-spark_2.4_2.11_deploy.jar:file:/databricks/hive/maven--org.slf4j--slf4j-api--org.slf4j__slf4j-api__1.7.5.jar:file:/databricks/hive/maven--spark_1.4--org.tukaani--xz--org.tukaani__xz__1.5.jar:file:/databricks/hive/maven--org.apache.httpcomponents--httpcore--org.apache.httpcomponents__httpcore__4.4.1.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.tukaani--xz--org.tukaani__xz__1.5.jar:file:/databricks/hive/maven--spark_1.4--com.esotericsoftware.kryo--kryo--com.esotericsoftware.kryo__kryo__2.21.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--io.reactivex__rxjava__1.2.4_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--javax.el--javax.el-api--javax.el__javax.el-api__2.2.4.jar:file:/databricks/hive/maven--spark_1.4--org.apache.thrift--libfb303--org.apache.thrift__libfb303__0.9.0.jar:file:/databricks/hive/third_party--jackson--jsr305_only_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-annotations__2.7.0_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-annotations--org.apache.hadoop__hadoop-annotations__2.7.3.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.commons--commons-math3--org.apache.commons__commons-math3__3.4.1.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.scalactic--scalactic_2.11--org.scalactic__scalactic_2.11__3.0.3.jar:file:/databricks/hive/third_party--jetty8-shaded-client--jetty-util_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-core__2.7.2_2.11_shaded_20180920_b33d810.jar:file:/databricks/hive/maven--spark_1.4--org.spark-project.hive--hive-jdbc--org.spark-project.hive__hive-jdbc__0.13.1a.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__adapter-rxjava__2.1.0_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.httpcomponents--httpcore--org.apache.httpcomponents__httpcore__4.4.8.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--commons-logging__commons-logging__1.2_2.11_shaded_20180920_b33d810.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.hibernate--hibernate-validator--org.hibernate__hibernate-validator__5.1.1.Final.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--commons-beanutils--commons-beanutils--commons-beanutils__commons-beanutils__1.9.3.jar:file:/databricks/hive/logging--log4j-mod--log4j-mod-spark_2.4_2.11_deploy.jar:file:/databricks/hive/maven--spark_1.4--org.codehaus.groovy--groovy-all--org.codehaus.groovy__groovy-all__2.1.6.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.scala-lang--scala-library_2.11--org.scala-lang__scala-library__2.11.12.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-core--com.amazonaws__aws-java-sdk-core__1.11.313.jar:file:/databricks/hive/maven--spark_1.4--org.spark-project.hive--hive-metastore--org.spark-project.hive__hive-metastore__0.13.1a.jar:file:/databricks/hive/maven--spark_1.4--org.spark-project.hive.shims--hive-shims-common-secure--org.spark-project.hive.shims__hive-shims-common-secure__0.13.1a.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.directory.api--api-util--org.apache.directory.api__api-util__1.0.0-M20.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-ganglia--io.dropwizard.metrics__metrics-ganglia__3.1.5.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--commons-lang--commons-lang--commons-lang__commons-lang__2.6.jar:file:/databricks/hive/----jackson_annotations_shaded--libjackson-annotations.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.springframework--spring-core--org.springframework__spring-core__4.1.4.RELEASE.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.google.inject__guice__3.0_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--javax.servlet--javax.servlet-api--javax.servlet__javax.servlet-api__3.1.0.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.amazonaws--jmespath-java--com.amazonaws__jmespath-java__1.11.313.jar:file:/databricks/hive/maven--commons-lang--commons-lang--commons-lang__commons-lang__2.6.jar:file:/databricks/hive/maven--stax--stax-api--stax__stax-api__1.0.1.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--javax.servlet.jsp--jsp-api--javax.servlet.jsp__jsp-api__2.1.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.rest__client-runtime__1.1.0_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.httpcomponents--httpclient--org.apache.httpcomponents__httpclient__4.5.4.jar:file:/databricks/hive/common--client--client-spark_2.4_2.11_deploy.jar:file:/databricks/hive/maven--spark_1.4--org.spark-project.hive--hive-service--org.spark-project.hive__hive-service__0.13.1a.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.codehaus.jackson--jackson-core-asl--org.codehaus.jackson__jackson-core-asl__1.9.13.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-databind__2.7.2_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.google.guava__guava__11.0.2_2.11_shaded_20180920_b33d810.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-util--org.eclipse.jetty__jetty-util__9.3.20.v20170531.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.sun.xml.bind__jaxb-impl__2.2.3-1_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/maven--spark_1.4--org.spark-project.protobuf--protobuf-java--org.spark-project.protobuf__protobuf-java__2.5.0-spark.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.slf4j--slf4j-api--org.slf4j__slf4j-api__1.7.16.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.fasterxml.jackson.core--jackson-annotations--com.fasterxml.jackson.core__jackson-annotations__2.6.7.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.avro--avro--org.apache.avro__avro__1.8.2.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.google.code.gson--gson--com.google.code.gson__gson__2.2.4.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-core--io.dropwizard.metrics__metrics-core__3.1.5.jar:file:/databricks/hive/maven--spark_1.4--org.iq80.snappy--snappy--org.iq80.snappy__snappy__0.2.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpcore__4.4.4_2.11_shaded_20180920_b33d810.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.google.protobuf--protobuf-java--com.google.protobuf__protobuf-java__2.6.1.jar:file:/databricks/hive/maven--com.thoughtworks.paranamer--paranamer--com.thoughtworks.paranamer__paranamer__2.8.jar:file:/databricks/hive/maven--spark_1.4--org.apache.commons--commons-compress--org.apache.commons__commons-compress__1.9.jar:file:/databricks/hive/maven--spark_1.4--org.apache.zookeeper--zookeeper--org.apache.zookeeper__zookeeper__3.4.6.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.google.code.findbugs--jsr305--com.google.code.findbugs__jsr305__2.0.1.jar:file:/databricks/hive/third_party--jackson--jackson-module-scala-shaded_2.11_deploy.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.fasterxml.jackson.core--jackson-databind--com.fasterxml.jackson.core__jackson-databind__2.6.7.1.jar:file:/databricks/hive/api-base--api-base_java-spark_2.4_2.11_deploy.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-common--org.apache.hadoop__hadoop-common__2.7.3.jar:file:/databricks/hive/maven--spark_1.4--jline--jline--jline__jline__0.9.94.jar:file:/databricks/hive/maven--spark_1.4--org.datanucleus--datanucleus-rdbms--org.datanucleus__datanucleus-rdbms__3.2.9.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.commons--commons-compress--org.apache.commons__commons-compress__1.8.1.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.scala-lang.modules--scala-parser-combinators_2.11--org.scala-lang.modules__scala-parser-combinators_2.11__1.1.0.jar:file:/databricks/hive/extern--libaws-regions.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.zookeeper--zookeeper--org.apache.zookeeper__zookeeper__3.4.6.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-http--org.eclipse.jetty__jetty-http__9.3.20.v20170531.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.google.guava__guava__16.0_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-jvm--io.dropwizard.metrics__metrics-jvm__3.1.5.jar:file:/databricks/hive/maven--spark_1.4--org.spark-project.hive--hive-exec--org.spark-project.hive__hive-exec__0.13.1a.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.thoughtworks.paranamer--paranamer--com.thoughtworks.paranamer__paranamer__2.8.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okio__okio__1.8.0_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--javax.xml.stream__stax-api__1.0-2_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/maven--spark_1.4--org.apache.derby--derby--org.apache.derby__derby__10.10.1.1.jar:file:/databricks/hive/maven--spark_1.4--org.apache.thrift--libthrift--org.apache.thrift__libthrift__0.9.2.jar:file:/databricks/hive/maven--spark_1.4--com.googlecode.javaewah--JavaEWAH--com.googlecode.javaewah__JavaEWAH__0.3.2.jar:file:/databricks/hive/third_party--jetty8-shaded-client--jetty-io_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__retrofit__2.1.0_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.typesafe.scala-logging--scala-logging-api_2.11--com.typesafe.scala-logging__scala-logging-api_2.11__2.1.2.jar:file:/databricks/hive/----jackson_databind_shaded--libjackson-databind.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.google.guava--guava--com.google.guava__guava__15.0.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--commons-logging--commons-logging--commons-logging__commons-logging__1.1.3.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.curator--curator-recipes--org.apache.curator__curator-recipes__2.7.1.jar:file:/databricks/hive/daemon--data--client--conf--conf-spark_2.4_2.11_deploy.jar:file:/databricks/hive/maven--org.apache.ant--ant--org.apache.ant__ant__1.9.2.jar:file:/databricks/hive/third_party--jetty8-shaded-client--databricks-patched-jetty-http-jar_shaded.jar:file:/databricks/hive/maven--spark_1.4--org.objenesis--objenesis--org.objenesis__objenesis__1.2.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--aopalliance__aopalliance__1.0_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-healthchecks--io.dropwizard.metrics__metrics-healthchecks__3.1.5.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--org.apache.htrace__htrace-core__3.1.0-incubating_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/third_party--prometheus-client--simpleclient_dropwizard-spark_2.4_2.11_deploy.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__converter-jackson__2.1.0_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/maven--spark_1.4--org.spark-project.hive--hive-beeline--org.spark-project.hive__hive-beeline__0.13.1a.jar:file:/databricks/hive/common--credentials--credentials-spark_2.4_2.11_deploy.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--commons-io--commons-io--commons-io__commons-io__2.4.jar:file:/databricks/hive/maven--spark_1.4--org.spark-project.hive.shims--hive-shims-0.20S--org.spark-project.hive.shims__hive-shims-0.20S__0.13.1a.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--commons-codec__commons-codec__1.9_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--commons-cli--commons-cli--commons-cli__commons-cli__1.2.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.curator--curator-client--org.apache.curator__curator-client__2.7.1.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--io.netty__netty-all__4.0.52.Final_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/s3--s3-spark_2.4_2.11_deploy.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--xmlenc--xmlenc--xmlenc__xmlenc__0.52.jar:file:/databricks/hive/maven--spark_1.4--junit--junit--junit__junit__3.8.1.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.jboss.logging--jboss-logging--org.jboss.logging__jboss-logging__3.1.3.GA.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__okhttp-urlconnection__3.3.1_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/third_party--azure--com.microsoft.azure__azure-keyvault-core__1.0.0_shaded.jar:file:/databricks/hive/maven--org.apache.ant--ant-launcher--org.apache.ant__ant-launcher__1.9.2.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--commons-net--commons-net--commons-net__commons-net__3.1.jar:file:/databricks/hive/common--hadoop--hadoop-spark_2.4_2.11_deploy.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-kms--com.amazonaws__aws-java-sdk-kms__1.11.313.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--joda-time--joda-time--joda-time__joda-time__2.9.3.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.acplt--oncrpc--org.acplt__oncrpc__1.0.7.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-io--org.eclipse.jetty__jetty-io__9.3.20.v20170531.jar:file:/databricks/hive/----jackson_core_shaded--libjackson-core.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-servlet--org.eclipse.jetty__jetty-servlet__9.3.20.v20170531.jar:file:/databricks/hive/maven--spark_1.4--oro--oro--oro__oro__2.0.8.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-proxy--org.eclipse.jetty__jetty-proxy__9.3.20.v20170531.jar:file:/databricks/hive/maven--spark_1.4--com.jolbox--bonecp--com.jolbox__bonecp__0.8.0.RELEASE.jar:file:/databricks/hive/third_party--jackson--paranamer_only_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.google.code.findbugs__jsr305__1.3.9_2.11_shaded_20180920_b33d810.jar:file:/databricks/hive/maven--org.xerial.snappy--snappy-java--org.xerial.snappy__snappy-java__1.1.2.6.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-keyvault-core__1.0.0_2.11_shaded_20180920_b33d810.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--info.ganglia.gmetric4j--gmetric4j--info.ganglia.gmetric4j__gmetric4j__1.0.7.jar:file:/databricks/hive/s3commit--client--client-spark_2.4_2.11_deploy.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--commons-digester--commons-digester--commons-digester__commons-digester__1.8.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.curator--curator-framework--org.apache.curator__curator-framework__2.7.1.jar:file:/databricks/hive/maven--antlr--antlr--antlr__antlr__2.7.7.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-20180625_3682417-spark_2.4_2.11_deploy_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.typesafe--config--com.typesafe__config__1.2.1.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--joda-time__joda-time__2.4_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.fasterxml.jackson.dataformat--jackson-dataformat-cbor--com.fasterxml.jackson.dataformat__jackson-dataformat-cbor__2.6.7.jar:file:/databricks/hive/----jackson_datatype_joda_shaded--libjackson-datatype-joda.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-server--org.eclipse.jetty__jetty-server__9.3.20.v20170531.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-client--org.eclipse.jetty__jetty-client__9.3.20.v20170531.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--commons-codec--commons-codec--commons-codec__commons-codec__1.10.jar:file:/databricks/hive/third_party--prometheus-client--simpleclient-jetty9-hadoop1_2.11_deploy.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--commons-logging__commons-logging__1.2_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.databricks--jets3t--com.databricks__jets3t__0.7.1-0.jar:file:/databricks/hive/maven--commons-httpclient--commons-httpclient--commons-httpclient__commons-httpclient__3.1.jar:file:/databricks/hive/maven--org.antlr--stringtemplate--org.antlr__stringtemplate__3.2.1.jar:file:/databricks/hive/maven--spark_1.4--org.spark-project.hive.shims--hive-shims-common--org.spark-project.hive.shims__hive-shims-common__0.13.1a.jar:file:/databricks/hive/maven--spark_1.4--javolution--javolution--javolution__javolution__5.5.1.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-auth--org.apache.hadoop__hadoop-auth__2.7.3.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--commons-configuration--commons-configuration--commons-configuration__commons-configuration__1.6.jar:file:/databricks/hive/maven--spark_1.4--org.spark-project.hive.shims--hive-shims-0.23--org.spark-project.hive.shims__hive-shims-0.23__0.13.1a.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--io.netty--netty-all--io.netty__netty-all__4.1.17.Final.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--io.netty--netty--io.netty__netty__3.9.9.Final.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.databricks.scalapb--compilerplugin_2.11--com.databricks.scalapb__compilerplugin_2.11__0.4.15-9.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.springframework--spring-test--org.springframework__spring-test__4.1.4.RELEASE.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpcore__4.4.4_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpclient__4.5.2_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--javax.validation--validation-api--javax.validation__validation-api__1.1.0.Final.jar:file:/databricks/hive/maven--spark_1.4--org.spark-project.hive.shims--hive-shims-0.20--org.spark-project.hive.shims__hive-shims-0.20__0.13.1a.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.directory.server--apacheds-kerberos-codec--org.apache.directory.server__apacheds-kerberos-codec__2.0.0-M15.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--commons-httpclient--commons-httpclient--commons-httpclient__commons-httpclient__3.1.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-servlets--org.eclipse.jetty__jetty-servlets__9.3.20.v20170531.jar:file:/databricks/hive/maven--commons-collections--commons-collections--commons-collections__commons-collections__3.2.2.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-s3--com.amazonaws__aws-java-sdk-s3__1.11.313.jar:file:/databricks/hive/common--lazy--lazy-spark_2.4_2.11_deploy.jar:file:/databricks/hive/dbfs--utils--dbfs-utils-spark_2.4_2.11_deploy.jar:file:/databricks/hive/third_party--azure--com.microsoft.azure__azure-storage__5.2.0_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.scala-lang--scala-reflect_2.11--org.scala-lang__scala-reflect__2.11.12.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--commons-codec__commons-codec__1.9_2.11_shaded_20180920_b33d810.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-core__2.7.2_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/bonecp-configs.jar
19/08/12 22:50:42 ERROR Utils: uncaught error in thread spark-listener-group-shared, stopping SparkContext   


what the fuck   <------------------------------------------------------

i think this is normal:
The SparkContext or SparkSession (Spark >= 2.0.0) should be stopped when the Spark code is run by adding sc.stop or spark.stop (Spark >= 2.0.0) at the end of the code.






java.lang.NoSuchMethodError: org.json4s.jackson.JsonMethods$.parse$default$3()Z
	at org.apache.spark.util.JsonProtocol$.sparkEventToJson(JsonProtocol.scala:119)
	at org.apache.spark.util.PublicJsonProtocol$.sparkEventToJson(PublicJsonProtocol.scala:12)
	at com.databricks.backend.daemon.driver.DBCEventLoggingListener.onEvent(DBCEventLoggingListener.scala:249)
	at org.apache.spark.SparkFirehoseListener.onOtherEvent(SparkFirehoseListener.java:161)
	at org.apache.spark.scheduler.SparkListenerBus$class.doPostEvent(SparkListenerBus.scala:80)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)
	at org.apache.spark.util.ListenerBus$class.postToAll(ListenerBus.scala:93)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$super$postToAll(AsyncEventQueue.scala:92)
	at org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply$mcJ$sp(AsyncEventQueue.scala:92)
	at org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply(AsyncEventQueue.scala:87)
	at org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply(AsyncEventQueue.scala:87)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:87)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$1$$anonfun$run$1.apply$mcV$sp(AsyncEventQueue.scala:83)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1468)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$1.run(AsyncEventQueue.scala:82)



19/08/12 22:50:42 ERROR Utils: throw uncaught fatal error in thread spark-listener-group-shared
java.lang.NoSuchMethodError: org.json4s.jackson.JsonMethods$.parse$default$3()Z
	at org.apache.spark.util.JsonProtocol$.sparkEventToJson(JsonProtocol.scala:119)
	at org.apache.spark.util.PublicJsonProtocol$.sparkEventToJson(PublicJsonProtocol.scala:12)
	at com.databricks.backend.daemon.driver.DBCEventLoggingListener.onEvent(DBCEventLoggingListener.scala:249)
	at org.apache.spark.SparkFirehoseListener.onOtherEvent(SparkFirehoseListener.java:161)
	at org.apache.spark.scheduler.SparkListenerBus$class.doPostEvent(SparkListenerBus.scala:80)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)
	at org.apache.spark.util.ListenerBus$class.postToAll(ListenerBus.scala:93)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$super$postToAll(AsyncEventQueue.scala:92)
	at org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply$mcJ$sp(AsyncEventQueue.scala:92)
	at org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply(AsyncEventQueue.scala:87)
	at org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply(AsyncEventQueue.scala:87)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:87)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$1$$anonfun$run$1.apply$mcV$sp(AsyncEventQueue.scala:83)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1468)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$1.run(AsyncEventQueue.scala:82)
19/08/12 22:50:42 ERROR DatabricksMain$DBUncaughtExceptionHandler: Uncaught exception in thread spark-listener-group-shared!
java.lang.NoSuchMethodError: org.json4s.jackson.JsonMethods$.parse$default$3()Z
	at org.apache.spark.util.JsonProtocol$.sparkEventToJson(JsonProtocol.scala:119)
	at org.apache.spark.util.PublicJsonProtocol$.sparkEventToJson(PublicJsonProtocol.scala:12)
	at com.databricks.backend.daemon.driver.DBCEventLoggingListener.onEvent(DBCEventLoggingListener.scala:249)
	at org.apache.spark.SparkFirehoseListener.onOtherEvent(SparkFirehoseListener.java:161)
	at org.apache.spark.scheduler.SparkListenerBus$class.doPostEvent(SparkListenerBus.scala:80)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)
	at org.apache.spark.util.ListenerBus$class.postToAll(ListenerBus.scala:93)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$super$postToAll(AsyncEventQueue.scala:92)
	at org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply$mcJ$sp(AsyncEventQueue.scala:92)
	at org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply(AsyncEventQueue.scala:87)
	at org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply(AsyncEventQueue.scala:87)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:87)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$1$$anonfun$run$1.apply$mcV$sp(AsyncEventQueue.scala:83)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1468)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$1.run(AsyncEventQueue.scala:82)









19/08/12 22:50:42 INFO AbstractConnector: Stopped Spark@6ba060f3{HTTP/1.1,[http/1.1]}{10.67.237.124:48182}






19/08/12 22:50:42 INFO SparkUI: Stopped Spark web UI at http://10.67.237.124:48182







19/08/12 22:50:42 INFO StandaloneSchedulerBackend: Shutting down all executors







19/08/12 22:50:42 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down





19/08/12 22:50:42 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
19/08/12 22:50:42 INFO MemoryStore: MemoryStore cleared
19/08/12 22:50:42 INFO BlockManager: BlockManager stopped
19/08/12 22:50:42 INFO BlockManagerMaster: BlockManagerMaster stopped
19/08/12 22:50:42 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
19/08/12 22:50:42 INFO SparkContext: Successfully stopped SparkContext







19/08/12 22:50:42 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
19/08/12 22:50:42 INFO ObjectStore: ObjectStore, initialize called
19/08/12 22:50:42 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
19/08/12 22:50:42 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
19/08/12 22:50:43 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
19/08/12 22:50:43 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
19/08/12 22:50:43 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
19/08/12 22:50:43 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
19/08/12 22:50:43 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
19/08/12 22:50:43 INFO Query: Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
19/08/12 22:50:43 INFO ObjectStore: Initialized ObjectStore
19/08/12 22:50:44 INFO HiveMetaStore: Added admin role in metastore
19/08/12 22:50:44 INFO HiveMetaStore: Added public role in metastore
19/08/12 22:50:44 INFO HiveMetaStore: No user is added in admin role, since config is empty
19/08/12 22:50:44 INFO SessionState: No Tez session required at this point. hive.execution.engine=mr.
19/08/12 22:50:44 INFO HiveClientImpl: Warehouse location for Hive client (version 0.13.1) is /user/hive/warehouse
19/08/12 22:50:44 INFO HiveMetaStore: 0: get_database: default
19/08/12 22:50:44 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
19/08/12 22:50:44 WARN SparkContext: The jar /local_disk0/tmp/addedFile8128349861081662655698d3bc_4eae_4017_adc7_a960409bcd16_hail_all_spark_71440-df353.jar has been added already. Overwriting of added jars is not supported in the current version.
19/08/12 22:50:44 WARN SparkContext: The jar /local_disk0/tmp/addedFile8128349861081662655698d3bc_4eae_4017_adc7_a960409bcd16_hail_all_spark_71440-df353.jar has been added already. Overwriting of added jars is not supported in the current version.






19/08/12 22:50:44 WARN SQLExecution: Error executing advisor analysis 
java.lang.NullPointerException
	at com.databricks.sql.InstanceSpecificConf$.workerInstanceTypeFromSparkConf(InstanceSpecificConf.scala:71)
	at com.databricks.sql.InstanceSpecificConf$.getDefaultIOCacheEnabled(InstanceSpecificConf.scala:349)
	at com.databricks.sql.DatabricksSQLConf$$anonfun$7.apply$mcZ$sp(DatabricksSQLConf.scala:926)
	at com.databricks.sql.DatabricksSQLConf$$anonfun$7.apply(DatabricksSQLConf.scala:926)
	at com.databricks.sql.DatabricksSQLConf$$anonfun$7.apply(DatabricksSQLConf.scala:926)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.internal.config.ConfigEntryWithDefaultFunction.readFrom(ConfigEntry.scala:103)
	at org.apache.spark.sql.internal.SQLConf.getConf(SQLConf.scala:2146)
	at com.databricks.sql.io.caching.CachingUtils$.isParquetCachingOn(CachingUtils.scala:64)
	at com.databricks.sql.advice.AdviceGenerator.com$databricks$sql$advice$AdviceGenerator$$analyzeForUseDBIOCacheAdvice(AdviceGenerator.scala:101)
	at com.databricks.sql.advice.AdviceGenerator$$anonfun$11.apply(AdviceGenerator.scala:238)
	at com.databricks.sql.advice.AdviceGenerator$$anonfun$11.apply(AdviceGenerator.scala:238)
	at com.databricks.sql.advice.AdviceGenerator$$anonfun$analyzeAndGenerateAdvice$1.apply(AdviceGenerator.scala:248)
	at com.databricks.sql.advice.AdviceGenerator$$anonfun$analyzeAndGenerateAdvice$1.apply(AdviceGenerator.scala:248)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at com.databricks.sql.advice.AdviceGenerator.analyzeAndGenerateAdvice(AdviceGenerator.scala:248)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1.apply(SQLExecution.scala:138)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:240)
	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:170)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3425)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:205)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:696)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:716)
	at com.databricks.backend.daemon.driver.DriverLocal$$anonfun$4$$anonfun$apply$1.apply(DriverLocal.scala:153)
	at com.databricks.backend.daemon.driver.DriverLocal$$anonfun$4$$anonfun$apply$1.apply(DriverLocal.scala:153)
	at com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:763)
	at com.databricks.backend.daemon.driver.DriverLocal$$anonfun$4.apply(DriverLocal.scala:152)
	at com.databricks.backend.daemon.driver.DriverLocal$$anonfun$4.apply(DriverLocal.scala:137)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at com.databricks.backend.daemon.driver.DriverLocal.<init>(DriverLocal.scala:137)
	at com.databricks.backend.daemon.driver.PythonDriverLocal.<init>(PythonDriverLocal.scala:91)
	at com.databricks.backend.daemon.driver.PythonDriverWrapper.instantiateDriver(DriverWrapper.scala:728)
	at com.databricks.backend.daemon.driver.DriverWrapper.setupRepl(DriverWrapper.scala:299)
	at com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:218)
	at java.lang.Thread.run(Thread.java:748)
19/08/12 22:50:44 WARN SQLExecution: Error executing advisor analysis 
java.lang.NullPointerException
	at com.databricks.sql.InstanceSpecificConf$.workerInstanceTypeFromSparkConf(InstanceSpecificConf.scala:71)
	at com.databricks.sql.InstanceSpecificConf$.getDefaultIOCacheEnabled(InstanceSpecificConf.scala:349)
	at com.databricks.sql.DatabricksSQLConf$$anonfun$7.apply$mcZ$sp(DatabricksSQLConf.scala:926)
	at com.databricks.sql.DatabricksSQLConf$$anonfun$7.apply(DatabricksSQLConf.scala:926)
	at com.databricks.sql.DatabricksSQLConf$$anonfun$7.apply(DatabricksSQLConf.scala:926)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.internal.config.ConfigEntryWithDefaultFunction.readFrom(ConfigEntry.scala:103)
	at org.apache.spark.sql.internal.SQLConf.getConf(SQLConf.scala:2146)
	at com.databricks.sql.io.caching.CachingUtils$.isParquetCachingOn(CachingUtils.scala:64)
	at com.databricks.sql.advice.AdviceGenerator.com$databricks$sql$advice$AdviceGenerator$$analyzeForUseDBIOCacheAdvice(AdviceGenerator.scala:101)
	at com.databricks.sql.advice.AdviceGenerator$$anonfun$11.apply(AdviceGenerator.scala:238)
	at com.databricks.sql.advice.AdviceGenerator$$anonfun$11.apply(AdviceGenerator.scala:238)
	at com.databricks.sql.advice.AdviceGenerator$$anonfun$analyzeAndGenerateAdvice$1.apply(AdviceGenerator.scala:248)
	at com.databricks.sql.advice.AdviceGenerator$$anonfun$analyzeAndGenerateAdvice$1.apply(AdviceGenerator.scala:248)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at com.databricks.sql.advice.AdviceGenerator.analyzeAndGenerateAdvice(AdviceGenerator.scala:248)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1.apply(SQLExecution.scala:138)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:240)
	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:170)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3425)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:205)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:696)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:716)
	at com.databricks.backend.daemon.driver.DriverLocal$$anonfun$4$$anonfun$apply$1.apply(DriverLocal.scala:153)
	at com.databricks.backend.daemon.driver.DriverLocal$$anonfun$4$$anonfun$apply$1.apply(DriverLocal.scala:153)
	at com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:763)
	at com.databricks.backend.daemon.driver.DriverLocal$$anonfun$4.apply(DriverLocal.scala:152)
	at com.databricks.backend.daemon.driver.DriverLocal$$anonfun$4.apply(DriverLocal.scala:137)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at com.databricks.backend.daemon.driver.DriverLocal.<init>(DriverLocal.scala:137)
	at com.databricks.backend.daemon.driver.SQLDriverLocal.<init>(SQLDriverLocal.scala:23)
	at com.databricks.backend.daemon.driver.SQLDriverWrapper.instantiateDriver(DriverWrapper.scala:796)
	at com.databricks.backend.daemon.driver.DriverWrapper.setupRepl(DriverWrapper.scala:299)
	at com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:218)
	at java.lang.Thread.run(Thread.java:748)
19/08/12 22:50:44 INFO SQLDriverWrapper: setupRepl:ReplId-79b8a-eba9e-f9eed-6: finished to load
19/08/12 22:50:44 WARN PythonDriverWrapper: Failed to start repl ReplId-53df2-c551f-c5fd9-5
java.lang.NullPointerException
	at org.apache.spark.util.DatabricksUtils$.getPythonVirtualEnvDir(DatabricksUtils.scala:389)
	at org.apache.spark.util.DatabricksUtils$.getOrCreatePythonVirtualEnvDir(DatabricksUtils.scala:605)
	at com.databricks.backend.daemon.driver.PythonDriverLocal$.com$databricks$backend$daemon$driver$PythonDriverLocal$$createPythonSubprocess(PythonDriverLocal.scala:1296)
	at com.databricks.backend.daemon.driver.PythonDriverLocal.launchPython(PythonDriverLocal.scala:426)
	at com.databricks.backend.daemon.driver.PythonDriverLocal.startPython0(PythonDriverLocal.scala:1070)
	at com.databricks.backend.daemon.driver.PythonDriverLocal.startPython(PythonDriverLocal.scala:1121)
	at com.databricks.backend.daemon.driver.PythonDriverLocal.init(PythonDriverLocal.scala:1154)
	at com.databricks.backend.daemon.driver.PythonDriverLocal.<init>(PythonDriverLocal.scala:1162)
	at com.databricks.backend.daemon.driver.PythonDriverWrapper.instantiateDriver(DriverWrapper.scala:728)
	at com.databricks.backend.daemon.driver.DriverWrapper.setupRepl(DriverWrapper.scala:299)
	at com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:218)
	at java.lang.Thread.run(Thread.java:748)
19/08/12 22:50:44 WARN PythonDriverWrapper: setupRepl:ReplId-53df2-c551f-c5fd9-5: at the end, the status is Error(ReplId-53df2-c551f-c5fd9-5,java.lang.NullPointerException)
19/08/12 22:50:44 WARN SQLDriverWrapper: Spark is detected to be down
19/08/12 22:50:44 WARN SQLDriverWrapper: Fatal exception (spark down) in ReplId-79b8a-eba9e-f9eed-6
com.databricks.backend.common.rpc.SparkStoppedException: Spark down: 
	at com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:385)
	at com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:337)
	at com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:219)
	at java.lang.Thread.run(Thread.java:748)
19/08/12 22:50:44 INFO DriverCorral$: Cleaning the wrapper ReplId-53df2-c551f-c5fd9-5 (currently in status Stopped(ReplId-53df2-c551f-c5fd9-5))
19/08/12 22:50:44 INFO DriverCorral$: sending shutdown signal for REPL ReplId-53df2-c551f-c5fd9-5
19/08/12 22:50:44 WARN PythonDriverWrapper: Repl ReplId-53df2-c551f-c5fd9-5 is already shutting down: Stopped(ReplId-53df2-c551f-c5fd9-5)
19/08/12 22:50:44 INFO DriverCorral$: sending the interrupt signal for REPL ReplId-53df2-c551f-c5fd9-5
19/08/12 22:50:44 INFO DriverCorral$: waiting for localThread to stop for REPL ReplId-53df2-c551f-c5fd9-5
19/08/12 22:50:44 INFO DriverCorral$: ReplId-53df2-c551f-c5fd9-5 successfully discarded
19/08/12 22:50:44 ERROR DatabricksMain$DBUncaughtExceptionHandler: Uncaught exception in thread pool-38-thread-1!
java.lang.NullPointerException
	at org.apache.spark.util.DatabricksUtils$.getPythonVirtualEnvDir(DatabricksUtils.scala:389)
	at com.databricks.backend.daemon.driver.DriverCorral$$anon$1.run(DriverCorral.scala:328)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
19/08/12 22:50:46 INFO ShutdownHookManager: Shutdown hook called
19/08/12 22:50:46 INFO ShutdownHookManager: Deleting directory /local_disk0/tmp/spark-a4b7f940-dc11-43e0-93ad-a527f2d647e3
19/08/12 22:50:46 INFO ShutdownHookManager: Deleting directory /local_disk0/tmp/spark-6b07ee90-acc1-4fb9-a581-0f729d8989c8
19/08/12 22:50:46 INFO ShutdownHookManager: Deleting directory /local_disk0/spark-7ccee8e8-33a9-4e7e-9e18-0134f7a3a528
19/08/12 22:50:46 INFO ShutdownHookManager: Deleting directory /local_disk0/tmp/spark-c150c085-6625-4970-8cbb-67e76ea2529e
19/08/12 22:50:46 INFO S3AFileSystem: Delete path /oregon-prod/978277182616062/tmp/hive/root/262f50a7-0123-4b75-a135-3990ece9b31f - recursive true
19/08/12 22:50:51 INFO StaticConf$: DB_HOME: /databricks
19/08/12 22:50:52 INFO DriverDaemon$: ========== driver starting up ==========
19/08/12 22:50:52 INFO DriverDaemon$: Java: Oracle Corporation 1.8.0_212
19/08/12 22:50:52 INFO DriverDaemon$: OS: Linux/amd64 4.4.0-1087-aws
19/08/12 22:50:52 INFO DriverDaemon$: CWD: /databricks/driver
19/08/12 22:50:52 INFO DriverDaemon$: Mem: Max: 43.8G loaded GCs: PS Scavenge, PS MarkSweep
19/08/12 22:50:52 INFO DriverDaemon$: Logging multibyte characters: ✓
19/08/12 22:50:52 INFO DriverDaemon$: 'publicFile' appender in root logger: class com.databricks.logging.RedactionRollingFileAppender
19/08/12 22:50:52 INFO DriverDaemon$: 'org.apache.log4j.Appender' appender in root logger: class com.codahale.metrics.log4j.InstrumentedAppender
19/08/12 22:50:52 INFO DriverDaemon$: 'null' appender in root logger: class com.databricks.logging.RequestTracker
19/08/12 22:50:52 INFO DriverDaemon$: == Modules:
19/08/12 22:50:52 INFO DriverDaemon$: Starting prometheus metrics log export timer
19/08/12 22:50:52 INFO DriverDaemon$: Universe Git Hash: 9fe869e25b36fd138ce6237260383d9acb7597f5
19/08/12 22:50:52 INFO DriverDaemon$: Spark Git Hash: da21e1ae0fc26ce7b8930f2fa37323adf8ff376c
19/08/12 22:50:52 WARN RunHelpers$: Missing tag isolation client: java.util.NoSuchElementException: key not found: TagDefinition(clientType,The client type for a request, used for isolating resources for the request.)
19/08/12 22:50:52 INFO DatabricksILoop$: Creating throwaway interpreter
19/08/12 22:50:52 INFO SparkConfUtils$: Customize spark config according to file /tmp/custom-spark.conf
19/08/12 22:50:52 INFO SparkConfUtils$: new spark config: spark.sql.files.maxPartitionBytes -> 1099511627776
19/08/12 22:50:52 INFO SparkConfUtils$: new spark config: spark.sql.files.openCostInBytes -> 1099511627776
19/08/12 22:50:52 INFO SparkConfUtils$: new spark config: spark.executor.tempDirectory -> /local_disk0/tmp
19/08/12 22:50:52 INFO SparkConfUtils$: new spark config: spark.hadoop.mapreduce.input.fileinputformat.split.minsize -> 1099511627776
19/08/12 22:50:52 INFO SparkConfUtils$: new spark config: spark.hadoop.parquet.block.size -> 1099511627776
19/08/12 22:50:52 INFO SparkConfUtils$: new spark config: spark.driver.tempDirectory -> /local_disk0/tmp
19/08/12 22:50:52 INFO SparkConfUtils$: new spark config: spark.hadoop.io.compression.codecs -> org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,org.apache.hadoop.io.compress.GzipCodec
19/08/12 22:50:52 INFO MetastoreMonitor$: Internal internal metastore configured (config=DbMetastoreConfig{host=md13pes28u6ilc4.chkweekm4xjq.us-east-1.rds.amazonaws.com, port=3306, dbName=organization978277182616062, user=wAYERvGGkTCcCyiR})
19/08/12 22:50:52 INFO JettyClient$: Creating new HttpClient with SSLContextFactory=None,maxRequestHeaderSize=65536, namePrefix=Some(DriverDaemon), idleTimeout=2 hours, useBlockingConnect: true
19/08/12 22:50:52 INFO HikariDataSource: metastore-monitor - Starting...
19/08/12 22:50:52 INFO HikariDataSource: metastore-monitor - Start completed.
19/08/12 22:50:53 INFO DriverCorral: Creating the driver context
19/08/12 22:50:53 INFO DatabricksILoop$: Class Server Dir: /local_disk0/tmp/repl/spark-5819881122578669368-e5b8560d-c243-4560-a0c9-0df092c9b2d9
19/08/12 22:50:53 INFO HikariDataSource: metastore-monitor - Shutdown initiated...
19/08/12 22:50:53 INFO HikariDataSource: metastore-monitor - Shutdown completed.
19/08/12 22:50:53 INFO MetastoreMonitor: Metastore healthcheck successful (connection duration = 622 milliseconds)
19/08/12 22:50:53 INFO SparkConfUtils$: Customize spark config according to file /tmp/custom-spark.conf
19/08/12 22:50:53 INFO SparkConfUtils$: new spark config: spark.sql.files.maxPartitionBytes -> 1099511627776
19/08/12 22:50:53 INFO SparkConfUtils$: new spark config: spark.sql.files.openCostInBytes -> 1099511627776
19/08/12 22:50:53 INFO SparkConfUtils$: new spark config: spark.executor.tempDirectory -> /local_disk0/tmp
19/08/12 22:50:53 INFO SparkConfUtils$: new spark config: spark.hadoop.mapreduce.input.fileinputformat.split.minsize -> 1099511627776
19/08/12 22:50:53 INFO SparkConfUtils$: new spark config: spark.hadoop.parquet.block.size -> 1099511627776
19/08/12 22:50:53 INFO SparkConfUtils$: new spark config: spark.driver.tempDirectory -> /local_disk0/tmp
19/08/12 22:50:53 INFO SparkConfUtils$: new spark config: spark.hadoop.io.compression.codecs -> org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,org.apache.hadoop.io.compress.GzipCodec
19/08/12 22:50:53 WARN SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
19/08/12 22:50:53 INFO SparkContext: Running Spark version 2.4.3
19/08/12 22:50:53 WARN SparkConf: Detected deprecated memory fraction settings: [spark.storage.memoryFraction, spark.shuffle.memoryFraction]. As of Spark 1.6, execution and storage memory management are unified. All memory fractions used in the old model are now deprecated and no longer read. If you wish to use the old memory management, you may explicitly enable `spark.memory.useLegacyMode` (not recommended).
19/08/12 22:50:53 INFO SparkContext: Submitted application: Databricks Shell
19/08/12 22:50:53 INFO SparkContext: Spark configuration:
eventLog.rolloverIntervalSeconds=3600
spark.akka.frameSize=256
spark.app.name=Databricks Shell
spark.cleaner.referenceTracking.blocking=false
spark.databricks.acl.client=com.databricks.spark.sql.acl.client.SparkSqlAclClient
spark.databricks.acl.provider=com.databricks.sql.acl.ReflectionBackedAclProvider
spark.databricks.cloudProvider=AWS
spark.databricks.clusterSource=UI
spark.databricks.clusterUsageTags.autoTerminationMinutes=45
spark.databricks.clusterUsageTags.clusterAllTags=[{"key":"Vendor","value":"Databricks"},{"key":"Creator","value":"tbresee@mail.smu.edu"},{"key":"ClusterName","value":"Temp"},{"key":"ClusterId","value":"0812-224616-grad803"},{"key":"Name","value":"978277182616062-1eb54f5d-0990-42ac-b078-0e5707e67a1d-worker"}]
spark.databricks.clusterUsageTags.clusterAvailability=SPOT_WITH_FALLBACK
spark.databricks.clusterUsageTags.clusterCreator=Webapp
spark.databricks.clusterUsageTags.clusterEbsVolumeCount=3
spark.databricks.clusterUsageTags.clusterEbsVolumeSize=100
spark.databricks.clusterUsageTags.clusterEbsVolumeType=GENERAL_PURPOSE_SSD
spark.databricks.clusterUsageTags.clusterFirstOnDemand=1
spark.databricks.clusterUsageTags.clusterGeneration=0
spark.databricks.clusterUsageTags.clusterId=0812-224616-grad803
spark.databricks.clusterUsageTags.clusterLogDeliveryEnabled=false
spark.databricks.clusterUsageTags.clusterLogDestination=
spark.databricks.clusterUsageTags.clusterMaxWorkers=8
spark.databricks.clusterUsageTags.clusterMetastoreAccessType=RDS_DIRECT
spark.databricks.clusterUsageTags.clusterMinWorkers=2
spark.databricks.clusterUsageTags.clusterName=Temp
spark.databricks.clusterUsageTags.clusterNoDriverDaemon=false
spark.databricks.clusterUsageTags.clusterNodeType=r4.2xlarge
spark.databricks.clusterUsageTags.clusterNumSshKeys=0
spark.databricks.clusterUsageTags.clusterOwnerOrgId=978277182616062
spark.databricks.clusterUsageTags.clusterOwnerUserId=*********(redacted)
spark.databricks.clusterUsageTags.clusterPinned=false
spark.databricks.clusterUsageTags.clusterPythonVersion=3
spark.databricks.clusterUsageTags.clusterResourceClass=default
spark.databricks.clusterUsageTags.clusterScalingType=autoscaling
spark.databricks.clusterUsageTags.clusterSku=STANDARD_SKU
spark.databricks.clusterUsageTags.clusterSpotBidPricePercent=100
spark.databricks.clusterUsageTags.clusterState=Pending
spark.databricks.clusterUsageTags.clusterStateMessage=Starting Spark
spark.databricks.clusterUsageTags.clusterTargetWorkers=2
spark.databricks.clusterUsageTags.clusterWorkers=2
spark.databricks.clusterUsageTags.containerType=LXC
spark.databricks.clusterUsageTags.containerZoneId=us-east-1c
spark.databricks.clusterUsageTags.driverContainerId=c748523573e64c9a93a4002c44d9bbf2
spark.databricks.clusterUsageTags.driverContainerPrivateIp=10.67.237.124
spark.databricks.clusterUsageTags.driverInstanceId=i-07369000e410090b3
spark.databricks.clusterUsageTags.driverInstancePrivateIp=10.67.231.19
spark.databricks.clusterUsageTags.driverNodeType=r4.2xlarge
spark.databricks.clusterUsageTags.driverPublicDns=ec2-3-223-191-117.compute-1.amazonaws.com
spark.databricks.clusterUsageTags.enableCredentialPassthrough=*********(redacted)
spark.databricks.clusterUsageTags.enableDfAcls=false
spark.databricks.clusterUsageTags.enableElasticDisk=false
spark.databricks.clusterUsageTags.enableJdbcAutoStart=true
spark.databricks.clusterUsageTags.enableJobsAutostart=true
spark.databricks.clusterUsageTags.enableSqlAclsOnly=false
spark.databricks.clusterUsageTags.numPerClusterInitScriptsV2=0
spark.databricks.clusterUsageTags.sparkVersion=5.5.x-scala2.11
spark.databricks.clusterUsageTags.userProvidedRemoteVolumeCount=*********(redacted)
spark.databricks.clusterUsageTags.userProvidedRemoteVolumeSizeGb=*********(redacted)
spark.databricks.clusterUsageTags.userProvidedRemoteVolumeType=*********(redacted)
spark.databricks.clusterUsageTags.workerEnvironmentId=978277182616062-1eb54f5d-0990-42ac-b078-0e5707e67a1d
spark.databricks.credential.redactor=*********(redacted)
spark.databricks.delta.logStore.crossCloud.fatal=true
spark.databricks.delta.multiClusterWrites.enabled=true
spark.databricks.driverNodeTypeId=r4.2xlarge
spark.databricks.eventLog.dir=eventlogs
spark.databricks.io.directoryCommit.enableLogicalDelete=false
spark.databricks.overrideDefaultCommitProtocol=org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol
spark.databricks.passthrough.adls.gen2.tokenProviderClassName=*********(redacted)
spark.databricks.passthrough.adls.tokenProviderClassName=*********(redacted)
spark.databricks.passthrough.glue.credentialsProviderFactoryClassName=*********(redacted)
spark.databricks.passthrough.glue.executorServiceFactoryClassName=com.databricks.backend.daemon.driver.GlueClientExecutorServiceFactory
spark.databricks.passthrough.s3a.tokenProviderClassName=*********(redacted)
spark.databricks.preemption.enabled=true
spark.databricks.redactor=com.databricks.spark.util.DatabricksSparkLogRedactorProxy
spark.databricks.session.share=false
spark.databricks.sparkContextId=5819881122578669368
spark.databricks.tahoe.logStore.aws.class=com.databricks.tahoe.store.MultiClusterLogStore
spark.databricks.tahoe.logStore.azure.class=com.databricks.tahoe.store.AzureLogStore
spark.databricks.tahoe.logStore.class=com.databricks.tahoe.store.DelegatingLogStore
spark.databricks.workerNodeTypeId=r4.2xlarge
spark.driver.allowMultipleContexts=false
spark.driver.maxResultSize=4g
spark.driver.tempDirectory=/local_disk0/tmp
spark.eventLog.enabled=false
spark.executor.extraClassPath=*********(redacted)
spark.executor.extraJavaOptions=-Djava.io.tmpdir=/local_disk0/tmp -XX:ReservedCodeCacheSize=256m -XX:+UseCodeCacheFlushing -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:+PrintFlagsFinal -XX:+PrintGCDateStamps -verbose:gc -XX:+PrintGCDetails -Xss4m -Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.DatatypeFactoryImpl -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -Djavax.xml.validation.SchemaFactory:http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory -Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplementationSourceImpl -Djavax.net.ssl.sessionCacheSize=10000 -Ddatabricks.serviceName=spark-executor-1
spark.executor.memory=44632m
spark.executor.tempDirectory=/local_disk0/tmp
spark.extraListeners=com.databricks.backend.daemon.driver.DBCEventLoggingListener
spark.files.fetchFailure.unRegisterOutputOnHost=true
spark.files.overwrite=true
spark.files.useFetchCache=false
spark.hadoop.databricks.dbfs.client.version=v2
spark.hadoop.databricks.s3commit.client.sslTrustAll=false
spark.hadoop.fs.abfs.impl=shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem
spark.hadoop.fs.abfs.impl.disable.cache=true
spark.hadoop.fs.abfss.impl=shaded.databricks.v20180920_b33d810.org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem
spark.hadoop.fs.abfss.impl.disable.cache=true
spark.hadoop.fs.adl.impl=com.databricks.adl.AdlFileSystem
spark.hadoop.fs.adl.impl.disable.cache=true
spark.hadoop.fs.azure.skip.metrics=true
spark.hadoop.fs.s3.impl=com.databricks.s3a.S3AFileSystem
spark.hadoop.fs.s3a.connection.maximum=200
spark.hadoop.fs.s3a.fast.upload=true
spark.hadoop.fs.s3a.fast.upload.default=true
spark.hadoop.fs.s3a.impl=com.databricks.s3a.S3AFileSystem
spark.hadoop.fs.s3a.multipart.size=10485760
spark.hadoop.fs.s3a.multipart.threshold=104857600
spark.hadoop.fs.s3a.threads.max=136
spark.hadoop.fs.s3n.impl=com.databricks.s3a.S3AFileSystem
spark.hadoop.fs.wasb.impl=shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem
spark.hadoop.fs.wasb.impl.disable.cache=true
spark.hadoop.fs.wasbs.impl=shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem
spark.hadoop.fs.wasbs.impl.disable.cache=true
spark.hadoop.hive.server2.enable.doAs=false
spark.hadoop.hive.server2.idle.operation.timeout=7200000
spark.hadoop.hive.server2.idle.session.timeout=900000
spark.hadoop.hive.server2.keystore.password=*********(redacted)
spark.hadoop.hive.server2.keystore.path=/databricks/keys/jetty-ssl-driver-keystore.jks
spark.hadoop.hive.server2.session.check.interval=60000
spark.hadoop.hive.server2.thrift.http.cookie.auth.enabled=false
spark.hadoop.hive.server2.thrift.http.port=10000
spark.hadoop.hive.server2.transport.mode=http
spark.hadoop.hive.server2.use.SSL=true
spark.hadoop.hive.warehouse.subdir.inherit.perms=false
spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,org.apache.hadoop.io.compress.GzipCodec
spark.hadoop.mapred.output.committer.class=com.databricks.backend.daemon.data.client.DirectOutputCommitter
spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1099511627776
spark.hadoop.parquet.block.size=1099511627776
spark.hadoop.parquet.memory.pool.ratio=0.5
spark.hadoop.spark.sql.parquet.output.committer.class=org.apache.spark.sql.parquet.DirectParquetOutputCommitter
spark.hadoop.spark.sql.sources.outputCommitterClass=com.databricks.backend.daemon.data.client.MapReduceDirectOutputCommitter
spark.hadoop.spark.thriftserver.customHeadersToProperties=*********(redacted)
spark.home=/databricks/spark
spark.logConf=true
spark.master=spark://10.67.237.124:7077
spark.metrics.conf=/databricks/spark/conf/metrics.properties
spark.r.backendConnectionTimeout=604800
spark.r.numRBackendThreads=1
spark.rdd.compress=true
spark.repl.class.outputDir=/local_disk0/tmp/repl/spark-5819881122578669368-e5b8560d-c243-4560-a0c9-0df092c9b2d9
spark.rpc.message.maxSize=256
spark.scheduler.listenerbus.eventqueue.capacity=20000
spark.scheduler.mode=FAIR
spark.serializer.objectStreamReset=100
spark.shuffle.manager=SORT
spark.shuffle.memoryFraction=0.2
spark.shuffle.reduceLocality.enabled=false
spark.shuffle.service.enabled=true
spark.shuffle.service.port=4048
spark.sparkr.use.daemon=false
spark.speculation=false
spark.speculation.multiplier=3
spark.speculation.quantile=0.9
spark.sql.allowMultipleContexts=false
spark.sql.files.maxPartitionBytes=1099511627776
spark.sql.files.openCostInBytes=1099511627776
spark.sql.hive.convertCTAS=true
spark.sql.hive.convertMetastoreParquet=true
spark.sql.hive.metastore.jars=/databricks/hive/*
spark.sql.hive.metastore.sharedPrefixes=org.mariadb.jdbc,com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,microsoft.sql.DateTimeOffset,microsoft.sql.Types,com.databricks,com.codahale,com.fasterxml.jackson,shaded.databricks
spark.sql.hive.metastore.version=0.13.0
spark.sql.parquet.cacheMetadata=true
spark.sql.parquet.compression.codec=snappy
spark.sql.sources.commitProtocolClass=com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol
spark.sql.streaming.checkpointFileManagerClass=com.databricks.spark.sql.streaming.DatabricksCheckpointFileManager
spark.sql.ui.retainedExecutions=100
spark.sql.warehouse.dir=*********(redacted)
spark.storage.blockManagerTimeoutIntervalMs=300000
spark.storage.memoryFraction=0.5
spark.streaming.driver.writeAheadLog.allowBatching=true
spark.streaming.driver.writeAheadLog.closeFileAfterWrite=true
spark.task.reaper.enabled=true
spark.task.reaper.killTimeout=60s
spark.ui.port=48182
spark.worker.cleanup.enabled=false
19/08/12 22:50:53 INFO SecurityManager: Changing view acls to: root
19/08/12 22:50:53 INFO SecurityManager: Changing modify acls to: root
19/08/12 22:50:53 INFO SecurityManager: Changing view acls groups to: 
19/08/12 22:50:53 INFO SecurityManager: Changing modify acls groups to: 
19/08/12 22:50:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()
19/08/12 22:50:54 INFO Utils: Successfully started service 'sparkDriver' on port 40498.
19/08/12 22:50:54 INFO SparkEnv: Registering MapOutputTracker
19/08/12 22:50:54 INFO SparkEnv: Registering BlockManagerMaster
19/08/12 22:50:54 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
19/08/12 22:50:54 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
19/08/12 22:50:54 INFO DiskBlockManager: Created local directory at /local_disk0/blockmgr-8f065df8-9153-4520-bac4-c111b227c65a
19/08/12 22:50:54 INFO MemoryStore: MemoryStore started with capacity 24.3 GB
19/08/12 22:50:54 INFO SparkEnv: Registering OutputCommitCoordinator
19/08/12 22:50:54 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
19/08/12 22:50:54 INFO log: Logging initialized @4423ms
19/08/12 22:50:54 INFO Server: jetty-9.3.20.v20170531
19/08/12 22:50:54 INFO Server: Started @4530ms
19/08/12 22:50:54 INFO AbstractConnector: Started ServerConnector@591a4f8e{HTTP/1.1,[http/1.1]}{10.67.237.124:48182}
19/08/12 22:50:54 INFO Utils: Successfully started service 'SparkUI' on port 48182.
19/08/12 22:50:54 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6dc1dc69{/jobs,null,AVAILABLE,@Spark}
19/08/12 22:50:54 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@777d191f{/jobs/json,null,AVAILABLE,@Spark}
19/08/12 22:50:54 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7fc420b8{/jobs/job,null,AVAILABLE,@Spark}
19/08/12 22:50:54 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@e38f0b7{/jobs/job/json,null,AVAILABLE,@Spark}
19/08/12 22:50:54 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1192b58e{/stages,null,AVAILABLE,@Spark}
19/08/12 22:50:54 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4f8d86e4{/stages/json,null,AVAILABLE,@Spark}
19/08/12 22:50:54 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5f631ca0{/stages/stage,null,AVAILABLE,@Spark}
19/08/12 22:50:54 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3134153d{/stages/stage/json,null,AVAILABLE,@Spark}
19/08/12 22:50:54 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@767599a7{/stages/pool,null,AVAILABLE,@Spark}
19/08/12 22:50:54 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5f5effb0{/stages/pool/json,null,AVAILABLE,@Spark}
19/08/12 22:50:54 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@25d0cb3a{/storage,null,AVAILABLE,@Spark}
19/08/12 22:50:54 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@345cbf40{/storage/json,null,AVAILABLE,@Spark}
19/08/12 22:50:54 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6e3eb0cd{/storage/rdd,null,AVAILABLE,@Spark}
19/08/12 22:50:54 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@463561c5{/storage/rdd/json,null,AVAILABLE,@Spark}
19/08/12 22:50:54 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@659feb22{/environment,null,AVAILABLE,@Spark}
19/08/12 22:50:54 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@3468ee6e{/environment/json,null,AVAILABLE,@Spark}
19/08/12 22:50:54 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2f4b98f6{/executors,null,AVAILABLE,@Spark}
19/08/12 22:50:54 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@421def93{/executors/json,null,AVAILABLE,@Spark}
19/08/12 22:50:54 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@58c1da09{/executors/threadDump,null,AVAILABLE,@Spark}
19/08/12 22:50:54 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2b2954e1{/executors/threadDump/json,null,AVAILABLE,@Spark}
19/08/12 22:50:54 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@58d6e55a{/executors/heapHistogram,null,AVAILABLE,@Spark}
19/08/12 22:50:54 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@751ae8a4{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
19/08/12 22:50:54 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@235d659c{/static,null,AVAILABLE,@Spark}
19/08/12 22:50:54 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4bcaa195{/,null,AVAILABLE,@Spark}
19/08/12 22:50:54 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@d08edc5{/api,null,AVAILABLE,@Spark}
19/08/12 22:50:54 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@6bc24e72{/jobs/job/kill,null,AVAILABLE,@Spark}
19/08/12 22:50:54 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@724aefc3{/stages/stage/kill,null,AVAILABLE,@Spark}
19/08/12 22:50:54 INFO SparkUI: Bound SparkUI to 10.67.237.124, and started at http://10.67.237.124:48182
19/08/12 22:50:54 WARN FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.
19/08/12 22:50:54 INFO FairSchedulableBuilder: Created default pool: default, schedulingMode: FIFO, minShare: 0, weight: 1
19/08/12 22:50:54 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://10.67.237.124:7077...
19/08/12 22:50:55 INFO TransportClientFactory: Successfully created connection to /10.67.237.124:7077 after 367 ms (0 ms spent in bootstraps)
19/08/12 22:50:55 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20190812225055-0001
19/08/12 22:50:55 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20190812225055-0001/0 on worker-20190812224825-10.67.233.230-44554 (10.67.233.230:44554) with 8 core(s)
19/08/12 22:50:55 INFO StandaloneSchedulerBackend: Granted executor ID app-20190812225055-0001/0 on hostPort 10.67.233.230:44554 with 8 core(s), 43.6 GB RAM
19/08/12 22:50:55 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20190812225055-0001/1 on worker-20190812224829-10.67.245.78-36736 (10.67.245.78:36736) with 8 core(s)
19/08/12 22:50:55 INFO StandaloneSchedulerBackend: Granted executor ID app-20190812225055-0001/1 on hostPort 10.67.245.78:36736 with 8 core(s), 43.6 GB RAM
19/08/12 22:50:55 INFO TaskSchedulerImpl: Task preemption enabled.
19/08/12 22:50:55 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36183.
19/08/12 22:50:55 INFO NettyBlockTransferService: Server created on ip-10-67-237-124.ec2.internal:36183
19/08/12 22:50:55 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
19/08/12 22:50:55 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ip-10-67-237-124.ec2.internal, 36183, None)
19/08/12 22:50:55 INFO BlockManagerMasterEndpoint: Registering block manager ip-10-67-237-124.ec2.internal:36183 with 24.3 GB RAM, BlockManagerId(driver, ip-10-67-237-124.ec2.internal, 36183, None)
19/08/12 22:50:55 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20190812225055-0001/0 is now RUNNING
19/08/12 22:50:55 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20190812225055-0001/1 is now RUNNING
19/08/12 22:50:55 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ip-10-67-237-124.ec2.internal, 36183, None)
19/08/12 22:50:55 INFO BlockManager: external shuffle service port = 4048
19/08/12 22:50:55 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ip-10-67-237-124.ec2.internal, 36183, None)
19/08/12 22:50:55 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@4735d6e5{/metrics/json,null,AVAILABLE,@Spark}
19/08/12 22:50:55 INFO DBCEventLoggingListener: Initializing DBCEventLoggingListener
19/08/12 22:50:55 INFO DBCEventLoggingListener: Logging events to eventlogs/5819881122578669368/eventlog
19/08/12 22:50:55 INFO SparkContext: Registered listener com.databricks.backend.daemon.driver.DBCEventLoggingListener
19/08/12 22:50:55 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
19/08/12 22:50:55 INFO SparkContext: Loading Spark Service RPC Server
19/08/12 22:50:55 INFO SparkServiceRPCServer: Starting Spark Service RPC Server
19/08/12 22:50:55 INFO Server: jetty-9.3.20.v20170531
19/08/12 22:50:55 INFO AbstractConnector: Started ServerConnector@68ab6ab0{HTTP/1.1,[http/1.1]}{0.0.0.0:15001}
19/08/12 22:50:55 INFO Server: Started @5686ms
19/08/12 22:50:55 INFO DatabricksILoop$: Successfully registered spark metrics in Prometheus registry
19/08/12 22:50:55 INFO DatabricksILoop$: Successfully initialized SparkContext
19/08/12 22:50:55 INFO SharedState: Scheduler stats enabled.
19/08/12 22:50:55 INFO SharedState: loading hive config file: file:/databricks/hive/conf/hive-site.xml
19/08/12 22:50:55 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('/user/hive/warehouse').
19/08/12 22:50:55 INFO SharedState: Warehouse path is '/user/hive/warehouse'.
19/08/12 22:50:55 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@309dcdf3{/SQL,null,AVAILABLE,@Spark}
19/08/12 22:50:55 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7573b9ee{/SQL/json,null,AVAILABLE,@Spark}
19/08/12 22:50:55 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2c0c4c0a{/SQL/execution,null,AVAILABLE,@Spark}
19/08/12 22:50:55 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@35d26ad2{/SQL/execution/json,null,AVAILABLE,@Spark}
19/08/12 22:50:55 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@7f73ce28{/static/sql,null,AVAILABLE,@Spark}
19/08/12 22:50:55 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@149b4d20{/storage/iocache,null,AVAILABLE,@Spark}
19/08/12 22:50:55 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@664e848c{/storage/iocache/json,null,AVAILABLE,@Spark}
19/08/12 22:50:56 INFO DatabricksILoop$: Finished creating throwaway interpreter
19/08/12 22:50:56 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
19/08/12 22:50:58 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.67.233.230:58376) with ID 0
19/08/12 22:50:58 INFO BlockManagerMasterEndpoint: Registering block manager 10.67.233.230:46822 with 23.1 GB RAM, BlockManagerId(0, 10.67.233.230, 46822, None)
19/08/12 22:50:58 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.67.245.78:49300) with ID 1
19/08/12 22:50:58 INFO BlockManagerMasterEndpoint: Registering block manager 10.67.245.78:44570 with 23.1 GB RAM, BlockManagerId(1, 10.67.245.78, 44570, None)
19/08/12 22:50:58 INFO JettyClient$: Creating new HttpClient with SSLContextFactory=None,maxRequestHeaderSize=65536, namePrefix=Some(DBFS-SHARED), idleTimeout=2 hours, useBlockingConnect: true
19/08/12 22:50:59 INFO DatabricksFileSystemV2Factory: Creating S3A file system for s3a://databrickstombresee
19/08/12 22:50:59 INFO DBFS: Initialized DBFS with DBFSV2 as the delegate.
19/08/12 22:51:00 INFO HiveUtils: Initializing execution hive, version 1.2.1
19/08/12 22:51:00 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
19/08/12 22:51:00 INFO ObjectStore: ObjectStore, initialize called
19/08/12 22:51:00 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
19/08/12 22:51:00 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
19/08/12 22:51:02 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
19/08/12 22:51:03 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
19/08/12 22:51:03 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
19/08/12 22:51:04 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
19/08/12 22:51:04 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
19/08/12 22:51:04 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
19/08/12 22:51:04 INFO ObjectStore: Initialized ObjectStore
19/08/12 22:51:04 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
19/08/12 22:51:04 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
19/08/12 22:51:04 INFO HiveMetaStore: Added admin role in metastore
19/08/12 22:51:04 INFO HiveMetaStore: Added public role in metastore
19/08/12 22:51:04 INFO HiveMetaStore: No user is added in admin role, since config is empty
19/08/12 22:51:04 INFO HiveMetaStore: 0: get_all_databases
19/08/12 22:51:04 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_all_databases	
19/08/12 22:51:04 INFO HiveMetaStore: 0: get_functions: db=default pat=*
19/08/12 22:51:04 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
19/08/12 22:51:04 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
19/08/12 22:51:04 INFO SessionState: Created local directory: /local_disk0/tmp/6a8b809a-c83f-47e1-9d1b-a8853867bddd_resources
19/08/12 22:51:05 INFO S3AFileSystem: Making directory: /oregon-prod/978277182616062/tmp/hive/root/6a8b809a-c83f-47e1-9d1b-a8853867bddd
19/08/12 22:51:05 INFO SessionState: Created HDFS directory: /tmp/hive/root/6a8b809a-c83f-47e1-9d1b-a8853867bddd
19/08/12 22:51:05 INFO SessionState: Created local directory: /local_disk0/tmp/root/6a8b809a-c83f-47e1-9d1b-a8853867bddd
19/08/12 22:51:05 INFO S3AFileSystem: Making directory: /oregon-prod/978277182616062/tmp/hive/root/6a8b809a-c83f-47e1-9d1b-a8853867bddd/_tmp_space.db
19/08/12 22:51:05 INFO SessionState: Created HDFS directory: /tmp/hive/root/6a8b809a-c83f-47e1-9d1b-a8853867bddd/_tmp_space.db
19/08/12 22:51:05 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.2) is /user/hive/warehouse
19/08/12 22:51:05 INFO SessionManager: Operation log root directory is created: /local_disk0/tmp/root/operation_logs
19/08/12 22:51:05 INFO SessionManager: HiveServer2: Background operation thread pool size: 100
19/08/12 22:51:05 INFO SessionManager: HiveServer2: Background operation thread wait queue size: 100
19/08/12 22:51:05 INFO SessionManager: HiveServer2: Background operation thread keepalive time: 10 seconds
19/08/12 22:51:05 INFO AbstractService: Service:OperationManager is inited.
19/08/12 22:51:05 INFO AbstractService: Service:SessionManager is inited.
19/08/12 22:51:05 INFO AbstractService: Service: CLIService is inited.
19/08/12 22:51:05 INFO AbstractService: Service:ThriftHttpCLIService is inited.
19/08/12 22:51:05 INFO AbstractService: Service: HiveServer2 is inited.
19/08/12 22:51:05 INFO AbstractService: Service:OperationManager is started.
19/08/12 22:51:05 INFO AbstractService: Service:SessionManager is started.
19/08/12 22:51:05 INFO AbstractService: Service:CLIService is started.
19/08/12 22:51:05 INFO ObjectStore: ObjectStore, initialize called
19/08/12 22:51:05 INFO Query: Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
19/08/12 22:51:05 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
19/08/12 22:51:05 INFO ObjectStore: Initialized ObjectStore
19/08/12 22:51:05 INFO HiveMetaStore: 0: get_databases: default
19/08/12 22:51:05 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_databases: default	
19/08/12 22:51:05 INFO HiveMetaStore: 0: Shutting down the object store...
19/08/12 22:51:05 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=Shutting down the object store...	
19/08/12 22:51:05 INFO HiveMetaStore: 0: Metastore shutdown complete.
19/08/12 22:51:05 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=Metastore shutdown complete.	
19/08/12 22:51:05 INFO AbstractService: Service:ThriftHttpCLIService is started.
19/08/12 22:51:05 INFO AbstractService: Service:HiveServer2 is started.
19/08/12 22:51:05 INFO ThriftCLIService: HTTP Server SSL: adding excluded protocols: [SSLv2, SSLv3]
19/08/12 22:51:05 INFO ThriftCLIService: HTTP Server SSL: SslContextFactory.getExcludeProtocols = [SSL, SSLv2, SSLv2Hello, SSLv3]
19/08/12 22:51:05 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@53086bdc{/sqlserver,null,AVAILABLE,@Spark}
19/08/12 22:51:05 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@254210b1{/sqlserver/json,null,AVAILABLE,@Spark}
19/08/12 22:51:05 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@1d1da00b{/sqlserver/session,null,AVAILABLE,@Spark}
19/08/12 22:51:05 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@2db4a84a{/sqlserver/session/json,null,AVAILABLE,@Spark}
19/08/12 22:51:05 INFO DriverDaemon: Starting driver daemon...
19/08/12 22:51:05 INFO SparkConfUtils$: Customize spark config according to file /tmp/custom-spark.conf
19/08/12 22:51:05 INFO SparkConfUtils$: new spark config: spark.sql.files.maxPartitionBytes -> 1099511627776
19/08/12 22:51:05 INFO SparkConfUtils$: new spark config: spark.sql.files.openCostInBytes -> 1099511627776
19/08/12 22:51:05 INFO SparkConfUtils$: new spark config: spark.executor.tempDirectory -> /local_disk0/tmp
19/08/12 22:51:05 INFO SparkConfUtils$: new spark config: spark.hadoop.mapreduce.input.fileinputformat.split.minsize -> 1099511627776
19/08/12 22:51:05 INFO SparkConfUtils$: new spark config: spark.hadoop.parquet.block.size -> 1099511627776
19/08/12 22:51:05 INFO SparkConfUtils$: new spark config: spark.driver.tempDirectory -> /local_disk0/tmp
19/08/12 22:51:05 INFO SparkConfUtils$: new spark config: spark.hadoop.io.compression.codecs -> org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,org.apache.hadoop.io.compress.GzipCodec
19/08/12 22:51:05 WARN SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
19/08/12 22:51:05 INFO Server: jetty-9.3.20.v20170531
19/08/12 22:51:05 INFO DriverDaemon$$anon$1: Message out thread ready
19/08/12 22:51:05 INFO AbstractConnector: Started ServerConnector@22c0c0bc{HTTP/1.1,[http/1.1]}{0.0.0.0:6061}
19/08/12 22:51:05 INFO Server: Started @15492ms
19/08/12 22:51:05 INFO DriverDaemon: Driver daemon started.
19/08/12 22:51:05 INFO Server: jetty-9.3.20.v20170531
19/08/12 22:51:05 INFO ContextHandler: Started o.e.j.s.ServletContextHandler@5184024b{/,null,AVAILABLE}
19/08/12 22:51:05 INFO SslContextFactory: x509=X509@3b37cbba(1,h=[databrickscloud.com],w=[]) for SslContextFactory@4c53de3b(file:///databricks/keys/jetty-ssl-driver-keystore.jks,null)
19/08/12 22:51:05 INFO AbstractConnector: Started ServerConnector@4a2c0a35{SSL,[ssl, http/1.1]}{0.0.0.0:10000}
19/08/12 22:51:05 INFO Server: Started @15584ms
19/08/12 22:51:05 INFO ThriftCLIService: Started ThriftHttpCLIService in https mode on port 10000 path=/cliservice/* with 5...500 worker threads
19/08/12 22:51:06 INFO DriverCorral: Loading the root classloader
19/08/12 22:51:06 INFO DriverCorral: Starting sql repl ReplId-1de3f-08b41-b88f3-b
19/08/12 22:51:06 WARN SQLDriverLocal: loadLibraries: Libraries failed to be installed: Set()
19/08/12 22:51:06 INFO SQLDriverWrapper: setupRepl:ReplId-1de3f-08b41-b88f3-b: finished to load
19/08/12 22:51:06 INFO DriverCorral: Starting sql repl ReplId-3779d-3313c-2a515
19/08/12 22:51:06 WARN SQLDriverLocal: loadLibraries: Libraries failed to be installed: Set()
19/08/12 22:51:06 INFO SQLDriverWrapper: setupRepl:ReplId-3779d-3313c-2a515: finished to load
19/08/12 22:51:06 INFO DriverCorral: Starting sql repl ReplId-3a19e-4a970-7fdfb-6
19/08/12 22:51:06 WARN SQLDriverLocal: loadLibraries: Libraries failed to be installed: Set()
19/08/12 22:51:06 INFO SQLDriverWrapper: setupRepl:ReplId-3a19e-4a970-7fdfb-6: finished to load
19/08/12 22:51:06 INFO DriverCorral: Starting sql repl ReplId-7a4b0-132e8-76c55
19/08/12 22:51:06 WARN SQLDriverLocal: loadLibraries: Libraries failed to be installed: Set()
19/08/12 22:51:06 INFO SQLDriverWrapper: setupRepl:ReplId-7a4b0-132e8-76c55: finished to load
19/08/12 22:51:06 INFO DriverCorral: Starting sql repl ReplId-62db5-87639-4b2e2-1
19/08/12 22:51:06 WARN SQLDriverLocal: loadLibraries: Libraries failed to be installed: Set()
19/08/12 22:51:06 INFO SQLDriverWrapper: setupRepl:ReplId-62db5-87639-4b2e2-1: finished to load
19/08/12 22:51:06 INFO DriverCorral: Starting r repl ReplId-21958-0cf0a-5654e-d
19/08/12 22:51:06 WARN RDriverLocal: loadLibraries: Libraries failed to be installed: Set()
19/08/12 22:51:06 INFO RDriverLocal: 1. RDriverLocal.5366ea9b-bf5f-4b7a-9ecc-b80de20a13fe: object created with for ReplId-21958-0cf0a-5654e-d.
19/08/12 22:51:06 INFO RDriverLocal: 2. RDriverLocal.5366ea9b-bf5f-4b7a-9ecc-b80de20a13fe: initializing ...
19/08/12 22:51:06 INFO RDriverLocal: 3. RDriverLocal.5366ea9b-bf5f-4b7a-9ecc-b80de20a13fe: started RBackend thread on port 34360
19/08/12 22:51:06 INFO RDriverLocal: 4. RDriverLocal.5366ea9b-bf5f-4b7a-9ecc-b80de20a13fe: waiting for SparkR to be installed ...
19/08/12 22:51:20 INFO RDriverLocal$: SparkR installation completed.
19/08/12 22:51:20 INFO RDriverLocal: 5. RDriverLocal.5366ea9b-bf5f-4b7a-9ecc-b80de20a13fe: launching R process ...
19/08/12 22:51:20 INFO RDriverLocal: 6. RDriverLocal.5366ea9b-bf5f-4b7a-9ecc-b80de20a13fe: cgroup isolation disabled, not placing R process in REPL cgroup.
19/08/12 22:51:20 INFO RDriverLocal: 7. RDriverLocal.5366ea9b-bf5f-4b7a-9ecc-b80de20a13fe: starting R process on port 1100 (attempt 1) ...
19/08/12 22:51:20 INFO RDriverLocal: 8. RDriverLocal.5366ea9b-bf5f-4b7a-9ecc-b80de20a13fe: setting up BufferedStreamThread with bufferSize: 100.
19/08/12 22:51:21 INFO RDriverLocal: 9. RDriverLocal.5366ea9b-bf5f-4b7a-9ecc-b80de20a13fe: R process started with RServe listening on port 1100.
19/08/12 22:51:22 INFO RDriverLocal: 10. RDriverLocal.5366ea9b-bf5f-4b7a-9ecc-b80de20a13fe: starting interpreter to talk to R process ...
19/08/12 22:51:23 ERROR RShell: Failed to evaluate init script at path '/local_disk0/tmp/_CleanRShell.r5249546590636661749resource.r'
19/08/12 22:51:23 ERROR RDriverLocal: Starting R interpreter failed. 
com.databricks.backend.daemon.driver.RDriverLocal$RProcessFatalException
	at com.databricks.backend.daemon.driver.RShell$$anonfun$3.apply(RShell.scala:57)
	at com.databricks.backend.daemon.driver.RShell$$anonfun$3.apply(RShell.scala:50)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at com.databricks.backend.daemon.driver.RShell.<init>(RShell.scala:50)
	at com.databricks.backend.daemon.driver.RDriverLocal.init(RDriverLocal.scala:399)
	at com.databricks.backend.daemon.driver.RDriverLocal$.create(RDriverLocal.scala:714)
	at com.databricks.backend.daemon.driver.RDriverWrapper.instantiateDriver(DriverWrapper.scala:752)
	at com.databricks.backend.daemon.driver.DriverWrapper.setupRepl(DriverWrapper.scala:299)
	at com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:218)
	at java.lang.Thread.run(Thread.java:748)
19/08/12 22:51:23 INFO RDriverLocal: 11. RDriverLocal.5366ea9b-bf5f-4b7a-9ecc-b80de20a13fe: failed to start R interpreter.
19/08/12 22:51:23 INFO RDriverLocal: remove() called while R interpreter is null
19/08/12 22:51:23 WARN RDriverWrapper: Failed to start repl ReplId-21958-0cf0a-5654e-d
com.databricks.backend.daemon.driver.RDriverLocal$RProcessFatalException
	at com.databricks.backend.daemon.driver.RShell$$anonfun$3.apply(RShell.scala:57)
	at com.databricks.backend.daemon.driver.RShell$$anonfun$3.apply(RShell.scala:50)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at com.databricks.backend.daemon.driver.RShell.<init>(RShell.scala:50)
	at com.databricks.backend.daemon.driver.RDriverLocal.init(RDriverLocal.scala:399)
	at com.databricks.backend.daemon.driver.RDriverLocal$.create(RDriverLocal.scala:714)
	at com.databricks.backend.daemon.driver.RDriverWrapper.instantiateDriver(DriverWrapper.scala:752)
	at com.databricks.backend.daemon.driver.DriverWrapper.setupRepl(DriverWrapper.scala:299)
	at com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:218)
	at java.lang.Thread.run(Thread.java:748)
19/08/12 22:51:23 WARN RDriverWrapper: setupRepl:ReplId-21958-0cf0a-5654e-d: at the end, the status is Error(ReplId-21958-0cf0a-5654e-d,com.databricks.backend.daemon.driver.RDriverLocal$RProcessFatalException)
19/08/12 22:51:23 INFO DriverCorral$: Cleaning the wrapper ReplId-21958-0cf0a-5654e-d (currently in status Stopped(ReplId-21958-0cf0a-5654e-d))
19/08/12 22:51:23 INFO DriverCorral$: sending shutdown signal for REPL ReplId-21958-0cf0a-5654e-d
19/08/12 22:51:23 WARN RDriverWrapper: Repl ReplId-21958-0cf0a-5654e-d is already shutting down: Stopped(ReplId-21958-0cf0a-5654e-d)
19/08/12 22:51:23 INFO DriverCorral$: sending the interrupt signal for REPL ReplId-21958-0cf0a-5654e-d
19/08/12 22:51:23 INFO DriverCorral$: waiting for localThread to stop for REPL ReplId-21958-0cf0a-5654e-d
19/08/12 22:51:23 INFO DriverCorral$: ReplId-21958-0cf0a-5654e-d successfully discarded
19/08/12 22:51:28 INFO DriverCorral: AttachLibraries - candidate libraries: List(JavaJarId(dbfs:/FileStore/jars/5698d3bc_4eae_4017_adc7_a960409bcd16-hail_all_spark-71440.jar,,NONE))
19/08/12 22:51:28 INFO LibraryDownloadManager: Downloading a library that was not in the cache: JavaJarId(dbfs:/FileStore/jars/5698d3bc_4eae_4017_adc7_a960409bcd16-hail_all_spark-71440.jar,,NONE)
19/08/12 22:51:28 INFO LibraryDownloadManager: Downloaded library dbfs:/FileStore/jars/5698d3bc_4eae_4017_adc7_a960409bcd16-hail_all_spark-71440.jar as local file /local_disk0/tmp/addedFile44360902786793200555698d3bc_4eae_4017_adc7_a960409bcd16_hail_all_spark_71440-df353.jar
19/08/12 22:51:28 INFO SharedDriverContext: Adding libraries that haven't been added before: Set(/local_disk0/tmp/addedFile44360902786793200555698d3bc_4eae_4017_adc7_a960409bcd16_hail_all_spark_71440-df353.jar)
19/08/12 22:51:28 INFO SparkContext: Added file /local_disk0/tmp/addedFile44360902786793200555698d3bc_4eae_4017_adc7_a960409bcd16_hail_all_spark_71440-df353.jar at spark://ip-10-67-237-124.ec2.internal:40498/files/addedFile44360902786793200555698d3bc_4eae_4017_adc7_a960409bcd16_hail_all_spark_71440-df353.jar with timestamp 1565650288779
19/08/12 22:51:28 INFO Utils: Copying /local_disk0/tmp/addedFile44360902786793200555698d3bc_4eae_4017_adc7_a960409bcd16_hail_all_spark_71440-df353.jar to /local_disk0/spark-1a8c6eef-4cef-418f-8b51-ba5c8e90bcf0/userFiles-edc35486-4f52-41b6-a344-118a8def712e/addedFile44360902786793200555698d3bc_4eae_4017_adc7_a960409bcd16_hail_all_spark_71440-df353.jar
19/08/12 22:51:28 INFO SparkContext: Added JAR /local_disk0/tmp/addedFile44360902786793200555698d3bc_4eae_4017_adc7_a960409bcd16_hail_all_spark_71440-df353.jar at spark://ip-10-67-237-124.ec2.internal:40498/jars/addedFile44360902786793200555698d3bc_4eae_4017_adc7_a960409bcd16_hail_all_spark_71440-df353.jar with timestamp 1565650288822
19/08/12 22:51:28 INFO DriverCorral: Successfully attached library dbfs:/FileStore/jars/5698d3bc_4eae_4017_adc7_a960409bcd16-hail_all_spark-71440.jar to Spark
19/08/12 22:51:28 INFO LibraryState: Successfully attached library dbfs:/FileStore/jars/5698d3bc_4eae_4017_adc7_a960409bcd16-hail_all_spark-71440.jar
19/08/12 22:52:11 INFO DriverCorral: Starting python repl ReplId-6b2c8-729d3-8d77a-d
19/08/12 22:52:11 WARN PythonDriverLocal: loadLibraries: Libraries failed to be installed: Set()
19/08/12 22:52:11 INFO LogicalPlanStats: Setting LogicalPlanStats visitor to com.databricks.sql.optimizer.statsEstimation.DatabricksLogicalPlanStatsVisitor$
19/08/12 22:52:12 INFO HiveUtils: Initializing HiveMetastoreConnection version 0.13.0 using file:/databricks/hive/maven--spark_1.4--com.esotericsoftware.reflectasm--reflectasm-shaded--com.esotericsoftware.reflectasm__reflectasm-shaded__1.07.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.scalatest--scalatest_2.11--org.scalatest__scalatest_2.11__3.0.3.jar:file:/databricks/hive/maven--spark_1.4--org.datanucleus--datanucleus-core--org.datanucleus__datanucleus-core__3.2.10.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__okhttp__3.3.1_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/third_party--azure--org.apache.commons__commons-lang3__3.4_shaded.jar:file:/databricks/hive/maven--spark_1.4--org.spark-project.hive--hive-common--org.spark-project.hive__hive-common__0.13.1a.jar:file:/databricks/hive/maven--spark_1.4--org.antlr--ST4--org.antlr__ST4__4.0.4.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-jetty9--io.dropwizard.metrics__metrics-jetty9__3.1.5.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-security--org.eclipse.jetty__jetty-security__9.3.20.v20170531.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.slf4j--slf4j-log4j12--org.slf4j__slf4j-log4j12__1.7.16.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.datatype__jackson-datatype-joda__2.7.2_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/extern--acl--auth--auth-spark_2.4_2.11_deploy.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-sts--com.amazonaws__aws-java-sdk-sts__1.11.313.jar:file:/databricks/hive/maven--spark_1.4--org.apache.velocity--velocity--org.apache.velocity__velocity__1.5.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.htrace--htrace-core--org.apache.htrace__htrace-core__3.1.0-incubating.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-continuation--org.eclipse.jetty__jetty-continuation__9.3.20.v20170531.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-annotations__1.2.0_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/third_party--azure--com.fasterxml.jackson.core__jackson-core__2.7.2_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.twitter--util-jvm_2.11--com.twitter__util-jvm_2.11__6.23.0.jar:file:/databricks/hive/maven--log4j--log4j--log4j__log4j__1.2.17.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.trueaccord.lenses--lenses_2.11--com.trueaccord.lenses__lenses_2.11__0.3.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-log4j--io.dropwizard.metrics__metrics-log4j__3.1.5.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-20180920_b33d810-spark_2.4_2.11_deploy_shaded.jar:file:/databricks/hive/maven--commons-logging--commons-logging--commons-logging__commons-logging__1.1.3.jar:file:/databricks/hive/maven--spark_1.4--org.antlr--antlr-runtime--org.antlr__antlr-runtime__3.4.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.directory.server--apacheds-i18n--org.apache.directory.server__apacheds-i18n__2.0.0-M15.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__logging-interceptor__3.3.1_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/third_party--jetty8-shaded-client--databricks-patched-jetty-client-jar_shaded.jar:file:/databricks/hive/maven--spark_1.4--org.spark-project.hive--hive-serde--org.spark-project.hive__hive-serde__0.13.1a.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--software.amazon.ion--ion-java--software.amazon.ion__ion-java__1.0.2.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.scala-lang.modules--scala-xml_2.11--org.scala-lang.modules__scala-xml_2.11__1.0.5.jar:file:/databricks/hive/maven--commons-io--commons-io--commons-io__commons-io__2.5.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--javax.inject__javax.inject__1_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.xerial.snappy--snappy-java--org.xerial.snappy__snappy-java__1.1.7.3.jar:file:/databricks/hive/maven--commons-codec--commons-codec--commons-codec__commons-codec__1.8.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.microsoft.azure--azure-data-lake-store-sdk--com.microsoft.azure__azure-data-lake-store-sdk__2.2.8.jar:file:/databricks/hive/maven--spark_1.4--org.apache.avro--avro--org.apache.avro__avro__1.8.2.jar:file:/databricks/hive/maven--spark_1.4--org.datanucleus--datanucleus-api-jdo--org.datanucleus__datanucleus-api-jdo__3.2.6.jar:file:/databricks/hive/maven--spark_1.4--javax.jdo--jdo-api--javax.jdo__jdo-api__3.0.1.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.fasterxml--classmate--com.fasterxml__classmate__1.0.0.jar:file:/databricks/hive/maven--org.slf4j--slf4j-log4j12--org.slf4j__slf4j-log4j12__1.7.5.jar:file:/databricks/hive/third_party--datalake--datalake-spark_2.4_2.11_deploy.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.joda--joda-convert--org.joda__joda-convert__1.7.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-servlets--io.dropwizard.metrics__metrics-servlets__3.1.5.jar:file:/databricks/hive/maven--spark_1.4--javax.transaction--jta--javax.transaction__jta__1.1.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--javax.xml.bind__jaxb-api__2.2.2_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/maven--org.json--json--org.json__json__20090211.jar:file:/databricks/hive/api-base--api-base-spark_2.4_2.11_deploy.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.typesafe.scala-logging--scala-logging-slf4j_2.11--com.typesafe.scala-logging__scala-logging-slf4j_2.11__2.1.2.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpclient__4.5.2_2.11_shaded_20180920_b33d810.jar:file:/databricks/hive/maven--spark_1.4--com.esotericsoftware.minlog--minlog--com.esotericsoftware.minlog__minlog__1.2.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.directory.api--api-asn1-api--org.apache.directory.api__api-asn1-api__1.0.0-M20.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--commons-collections--commons-collections--commons-collections__commons-collections__3.2.2.jar:file:/databricks/hive/maven--spark_1.4--org.spark-project.hive--hive-cli--org.spark-project.hive__hive-cli__0.13.1a.jar:file:/databricks/hive/third_party--jetty8-shaded-client--jetty-jmx_shaded.jar:file:/databricks/hive/common--jetty--client--client-spark_2.4_2.11_deploy.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.databricks.scalapb--scalapb-runtime_2.11--com.databricks.scalapb__scalapb-runtime_2.11__0.4.15-9.jar:file:/databricks/hive/maven--spark_1.4--commons-cli--commons-cli--commons-cli__commons-cli__1.2.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--javax.activation__activation__1.1_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/maven--spark_1.4--org.codehaus.jackson--jackson-core-asl--org.codehaus.jackson__jackson-core-asl__1.9.13.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.fasterxml.jackson.core--jackson-core--com.fasterxml.jackson.core__jackson-core__2.6.7.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-json--io.dropwizard.metrics__metrics-json__3.1.5.jar:file:/databricks/hive/maven--org.apache.httpcomponents--httpclient--org.apache.httpcomponents__httpclient__4.4.1.jar:file:/databricks/hive/maven--spark_1.4--io.netty--netty--io.netty__netty__3.8.0.Final.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.twitter--util-app_2.11--com.twitter__util-app_2.11__6.23.0.jar:file:/databricks/hive/maven--spark_1.4--org.spark-project.hive--hive-shims--org.spark-project.hive__hive-shims__0.13.1a.jar:file:/databricks/hive/maven--spark_1.4--org.codehaus.jackson--jackson-mapper-asl--org.codehaus.jackson__jackson-mapper-asl__1.9.13.jar:file:/databricks/hive/daemon--data--client--client-spark_2.4_2.11_deploy.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.twitter--util-core_2.11--com.twitter__util-core_2.11__6.23.0.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--log4j--log4j--log4j__log4j__1.2.17.jar:file:/databricks/hive/common--path--path-spark_2.4_2.11_deploy.jar:file:/databricks/hive/maven--spark_1.4--net.sf.jpam--jpam--net.sf.jpam__jpam__1.1.jar:file:/databricks/hive/third_party--prometheus-client--simpleclient-spark_2.4_2.11_deploy.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-storage__7.0.0_2.11_shaded_20180920_b33d810.jar:file:/databricks/hive/third_party--jackson--guava_only_shaded.jar:file:/databricks/hive/maven--org.apache.commons--commons-lang3--org.apache.commons__commons-lang3__3.4.jar:file:/databricks/hive/maven--spark_1.4--org.spark-project.hive--hive-ant--org.spark-project.hive__hive-ant__0.13.1a.jar:file:/databricks/hive/maven--spark_1.4--com.twitter--parquet-hadoop-bundle--com.twitter__parquet-hadoop-bundle__1.3.2.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--log4j--apache-log4j-extras--log4j__apache-log4j-extras__1.2.17.jar:file:/databricks/hive/jsonutil--jsonutil-spark_2.4_2.11_deploy.jar:file:/databricks/hive/daemon--data--data-common--data-common-spark_2.4_2.11_deploy.jar:file:/databricks/hive/extern--extern-spark_2.4_2.11_deploy.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.jdbi--jdbi--org.jdbi__jdbi__2.63.1.jar:file:/databricks/hive/s3commit--common--common-spark_2.4_2.11_deploy.jar:file:/databricks/hive/maven--org.slf4j--slf4j-api--org.slf4j__slf4j-api__1.7.5.jar:file:/databricks/hive/maven--spark_1.4--org.tukaani--xz--org.tukaani__xz__1.5.jar:file:/databricks/hive/maven--org.apache.httpcomponents--httpcore--org.apache.httpcomponents__httpcore__4.4.1.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.tukaani--xz--org.tukaani__xz__1.5.jar:file:/databricks/hive/maven--spark_1.4--com.esotericsoftware.kryo--kryo--com.esotericsoftware.kryo__kryo__2.21.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--io.reactivex__rxjava__1.2.4_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--javax.el--javax.el-api--javax.el__javax.el-api__2.2.4.jar:file:/databricks/hive/maven--spark_1.4--org.apache.thrift--libfb303--org.apache.thrift__libfb303__0.9.0.jar:file:/databricks/hive/third_party--jackson--jsr305_only_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-annotations__2.7.0_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-annotations--org.apache.hadoop__hadoop-annotations__2.7.3.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.commons--commons-math3--org.apache.commons__commons-math3__3.4.1.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.scalactic--scalactic_2.11--org.scalactic__scalactic_2.11__3.0.3.jar:file:/databricks/hive/third_party--jetty8-shaded-client--jetty-util_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-core__2.7.2_2.11_shaded_20180920_b33d810.jar:file:/databricks/hive/maven--spark_1.4--org.spark-project.hive--hive-jdbc--org.spark-project.hive__hive-jdbc__0.13.1a.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__adapter-rxjava__2.1.0_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.httpcomponents--httpcore--org.apache.httpcomponents__httpcore__4.4.8.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--commons-logging__commons-logging__1.2_2.11_shaded_20180920_b33d810.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.hibernate--hibernate-validator--org.hibernate__hibernate-validator__5.1.1.Final.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--commons-beanutils--commons-beanutils--commons-beanutils__commons-beanutils__1.9.3.jar:file:/databricks/hive/logging--log4j-mod--log4j-mod-spark_2.4_2.11_deploy.jar:file:/databricks/hive/maven--spark_1.4--org.codehaus.groovy--groovy-all--org.codehaus.groovy__groovy-all__2.1.6.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.scala-lang--scala-library_2.11--org.scala-lang__scala-library__2.11.12.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-core--com.amazonaws__aws-java-sdk-core__1.11.313.jar:file:/databricks/hive/maven--spark_1.4--org.spark-project.hive--hive-metastore--org.spark-project.hive__hive-metastore__0.13.1a.jar:file:/databricks/hive/maven--spark_1.4--org.spark-project.hive.shims--hive-shims-common-secure--org.spark-project.hive.shims__hive-shims-common-secure__0.13.1a.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.directory.api--api-util--org.apache.directory.api__api-util__1.0.0-M20.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-ganglia--io.dropwizard.metrics__metrics-ganglia__3.1.5.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--commons-lang--commons-lang--commons-lang__commons-lang__2.6.jar:file:/databricks/hive/----jackson_annotations_shaded--libjackson-annotations.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.springframework--spring-core--org.springframework__spring-core__4.1.4.RELEASE.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.google.inject__guice__3.0_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--javax.servlet--javax.servlet-api--javax.servlet__javax.servlet-api__3.1.0.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.amazonaws--jmespath-java--com.amazonaws__jmespath-java__1.11.313.jar:file:/databricks/hive/maven--commons-lang--commons-lang--commons-lang__commons-lang__2.6.jar:file:/databricks/hive/maven--stax--stax-api--stax__stax-api__1.0.1.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--javax.servlet.jsp--jsp-api--javax.servlet.jsp__jsp-api__2.1.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.rest__client-runtime__1.1.0_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.httpcomponents--httpclient--org.apache.httpcomponents__httpclient__4.5.4.jar:file:/databricks/hive/common--client--client-spark_2.4_2.11_deploy.jar:file:/databricks/hive/maven--spark_1.4--org.spark-project.hive--hive-service--org.spark-project.hive__hive-service__0.13.1a.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.codehaus.jackson--jackson-core-asl--org.codehaus.jackson__jackson-core-asl__1.9.13.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-databind__2.7.2_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.google.guava__guava__11.0.2_2.11_shaded_20180920_b33d810.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-util--org.eclipse.jetty__jetty-util__9.3.20.v20170531.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.sun.xml.bind__jaxb-impl__2.2.3-1_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/maven--spark_1.4--org.spark-project.protobuf--protobuf-java--org.spark-project.protobuf__protobuf-java__2.5.0-spark.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.slf4j--slf4j-api--org.slf4j__slf4j-api__1.7.16.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.fasterxml.jackson.core--jackson-annotations--com.fasterxml.jackson.core__jackson-annotations__2.6.7.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.avro--avro--org.apache.avro__avro__1.8.2.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.google.code.gson--gson--com.google.code.gson__gson__2.2.4.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-core--io.dropwizard.metrics__metrics-core__3.1.5.jar:file:/databricks/hive/maven--spark_1.4--org.iq80.snappy--snappy--org.iq80.snappy__snappy__0.2.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpcore__4.4.4_2.11_shaded_20180920_b33d810.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.google.protobuf--protobuf-java--com.google.protobuf__protobuf-java__2.6.1.jar:file:/databricks/hive/maven--com.thoughtworks.paranamer--paranamer--com.thoughtworks.paranamer__paranamer__2.8.jar:file:/databricks/hive/maven--spark_1.4--org.apache.commons--commons-compress--org.apache.commons__commons-compress__1.9.jar:file:/databricks/hive/maven--spark_1.4--org.apache.zookeeper--zookeeper--org.apache.zookeeper__zookeeper__3.4.6.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.google.code.findbugs--jsr305--com.google.code.findbugs__jsr305__2.0.1.jar:file:/databricks/hive/third_party--jackson--jackson-module-scala-shaded_2.11_deploy.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.fasterxml.jackson.core--jackson-databind--com.fasterxml.jackson.core__jackson-databind__2.6.7.1.jar:file:/databricks/hive/api-base--api-base_java-spark_2.4_2.11_deploy.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-common--org.apache.hadoop__hadoop-common__2.7.3.jar:file:/databricks/hive/maven--spark_1.4--jline--jline--jline__jline__0.9.94.jar:file:/databricks/hive/maven--spark_1.4--org.datanucleus--datanucleus-rdbms--org.datanucleus__datanucleus-rdbms__3.2.9.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.commons--commons-compress--org.apache.commons__commons-compress__1.8.1.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.scala-lang.modules--scala-parser-combinators_2.11--org.scala-lang.modules__scala-parser-combinators_2.11__1.1.0.jar:file:/databricks/hive/extern--libaws-regions.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.zookeeper--zookeeper--org.apache.zookeeper__zookeeper__3.4.6.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-http--org.eclipse.jetty__jetty-http__9.3.20.v20170531.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.google.guava__guava__16.0_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-jvm--io.dropwizard.metrics__metrics-jvm__3.1.5.jar:file:/databricks/hive/maven--spark_1.4--org.spark-project.hive--hive-exec--org.spark-project.hive__hive-exec__0.13.1a.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.thoughtworks.paranamer--paranamer--com.thoughtworks.paranamer__paranamer__2.8.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okio__okio__1.8.0_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--javax.xml.stream__stax-api__1.0-2_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/maven--spark_1.4--org.apache.derby--derby--org.apache.derby__derby__10.10.1.1.jar:file:/databricks/hive/maven--spark_1.4--org.apache.thrift--libthrift--org.apache.thrift__libthrift__0.9.2.jar:file:/databricks/hive/maven--spark_1.4--com.googlecode.javaewah--JavaEWAH--com.googlecode.javaewah__JavaEWAH__0.3.2.jar:file:/databricks/hive/third_party--jetty8-shaded-client--jetty-io_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__retrofit__2.1.0_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.typesafe.scala-logging--scala-logging-api_2.11--com.typesafe.scala-logging__scala-logging-api_2.11__2.1.2.jar:file:/databricks/hive/----jackson_databind_shaded--libjackson-databind.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.google.guava--guava--com.google.guava__guava__15.0.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--commons-logging--commons-logging--commons-logging__commons-logging__1.1.3.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.curator--curator-recipes--org.apache.curator__curator-recipes__2.7.1.jar:file:/databricks/hive/daemon--data--client--conf--conf-spark_2.4_2.11_deploy.jar:file:/databricks/hive/maven--org.apache.ant--ant--org.apache.ant__ant__1.9.2.jar:file:/databricks/hive/third_party--jetty8-shaded-client--databricks-patched-jetty-http-jar_shaded.jar:file:/databricks/hive/maven--spark_1.4--org.objenesis--objenesis--org.objenesis__objenesis__1.2.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--aopalliance__aopalliance__1.0_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--io.dropwizard.metrics--metrics-healthchecks--io.dropwizard.metrics__metrics-healthchecks__3.1.5.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--org.apache.htrace__htrace-core__3.1.0-incubating_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/third_party--prometheus-client--simpleclient_dropwizard-spark_2.4_2.11_deploy.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.squareup.retrofit2__converter-jackson__2.1.0_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/maven--spark_1.4--org.spark-project.hive--hive-beeline--org.spark-project.hive__hive-beeline__0.13.1a.jar:file:/databricks/hive/common--credentials--credentials-spark_2.4_2.11_deploy.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--commons-io--commons-io--commons-io__commons-io__2.4.jar:file:/databricks/hive/maven--spark_1.4--org.spark-project.hive.shims--hive-shims-0.20S--org.spark-project.hive.shims__hive-shims-0.20S__0.13.1a.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--commons-codec__commons-codec__1.9_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--commons-cli--commons-cli--commons-cli__commons-cli__1.2.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.curator--curator-client--org.apache.curator__curator-client__2.7.1.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--io.netty__netty-all__4.0.52.Final_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/s3--s3-spark_2.4_2.11_deploy.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--xmlenc--xmlenc--xmlenc__xmlenc__0.52.jar:file:/databricks/hive/maven--spark_1.4--junit--junit--junit__junit__3.8.1.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.jboss.logging--jboss-logging--org.jboss.logging__jboss-logging__3.1.3.GA.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.squareup.okhttp3__okhttp-urlconnection__3.3.1_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/third_party--azure--com.microsoft.azure__azure-keyvault-core__1.0.0_shaded.jar:file:/databricks/hive/maven--org.apache.ant--ant-launcher--org.apache.ant__ant-launcher__1.9.2.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--commons-net--commons-net--commons-net__commons-net__3.1.jar:file:/databricks/hive/common--hadoop--hadoop-spark_2.4_2.11_deploy.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-kms--com.amazonaws__aws-java-sdk-kms__1.11.313.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--joda-time--joda-time--joda-time__joda-time__2.9.3.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.acplt--oncrpc--org.acplt__oncrpc__1.0.7.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-io--org.eclipse.jetty__jetty-io__9.3.20.v20170531.jar:file:/databricks/hive/----jackson_core_shaded--libjackson-core.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-servlet--org.eclipse.jetty__jetty-servlet__9.3.20.v20170531.jar:file:/databricks/hive/maven--spark_1.4--oro--oro--oro__oro__2.0.8.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-proxy--org.eclipse.jetty__jetty-proxy__9.3.20.v20170531.jar:file:/databricks/hive/maven--spark_1.4--com.jolbox--bonecp--com.jolbox__bonecp__0.8.0.RELEASE.jar:file:/databricks/hive/third_party--jackson--paranamer_only_shaded.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.google.code.findbugs__jsr305__1.3.9_2.11_shaded_20180920_b33d810.jar:file:/databricks/hive/maven--org.xerial.snappy--snappy-java--org.xerial.snappy__snappy-java__1.1.2.6.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.microsoft.azure__azure-keyvault-core__1.0.0_2.11_shaded_20180920_b33d810.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--info.ganglia.gmetric4j--gmetric4j--info.ganglia.gmetric4j__gmetric4j__1.0.7.jar:file:/databricks/hive/s3commit--client--client-spark_2.4_2.11_deploy.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--commons-digester--commons-digester--commons-digester__commons-digester__1.8.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.curator--curator-framework--org.apache.curator__curator-framework__2.7.1.jar:file:/databricks/hive/maven--antlr--antlr--antlr__antlr__2.7.7.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-20180625_3682417-spark_2.4_2.11_deploy_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.typesafe--config--com.typesafe__config__1.2.1.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--joda-time__joda-time__2.4_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.fasterxml.jackson.dataformat--jackson-dataformat-cbor--com.fasterxml.jackson.dataformat__jackson-dataformat-cbor__2.6.7.jar:file:/databricks/hive/----jackson_datatype_joda_shaded--libjackson-datatype-joda.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-server--org.eclipse.jetty__jetty-server__9.3.20.v20170531.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-client--org.eclipse.jetty__jetty-client__9.3.20.v20170531.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--commons-codec--commons-codec--commons-codec__commons-codec__1.10.jar:file:/databricks/hive/third_party--prometheus-client--simpleclient-jetty9-hadoop1_2.11_deploy.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--commons-logging__commons-logging__1.2_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.databricks--jets3t--com.databricks__jets3t__0.7.1-0.jar:file:/databricks/hive/maven--commons-httpclient--commons-httpclient--commons-httpclient__commons-httpclient__3.1.jar:file:/databricks/hive/maven--org.antlr--stringtemplate--org.antlr__stringtemplate__3.2.1.jar:file:/databricks/hive/maven--spark_1.4--org.spark-project.hive.shims--hive-shims-common--org.spark-project.hive.shims__hive-shims-common__0.13.1a.jar:file:/databricks/hive/maven--spark_1.4--javolution--javolution--javolution__javolution__5.5.1.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.hadoop--hadoop-auth--org.apache.hadoop__hadoop-auth__2.7.3.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--commons-configuration--commons-configuration--commons-configuration__commons-configuration__1.6.jar:file:/databricks/hive/maven--spark_1.4--org.spark-project.hive.shims--hive-shims-0.23--org.spark-project.hive.shims__hive-shims-0.23__0.13.1a.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--io.netty--netty-all--io.netty__netty-all__4.1.17.Final.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--io.netty--netty--io.netty__netty__3.9.9.Final.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.databricks.scalapb--compilerplugin_2.11--com.databricks.scalapb__compilerplugin_2.11__0.4.15-9.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.springframework--spring-test--org.springframework__spring-test__4.1.4.RELEASE.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpcore__4.4.4_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--org.apache.httpcomponents__httpclient__4.5.2_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--javax.validation--validation-api--javax.validation__validation-api__1.1.0.Final.jar:file:/databricks/hive/maven--spark_1.4--org.spark-project.hive.shims--hive-shims-0.20--org.spark-project.hive.shims__hive-shims-0.20__0.13.1a.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.apache.directory.server--apacheds-kerberos-codec--org.apache.directory.server__apacheds-kerberos-codec__2.0.0-M15.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--commons-httpclient--commons-httpclient--commons-httpclient__commons-httpclient__3.1.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.eclipse.jetty--jetty-servlets--org.eclipse.jetty__jetty-servlets__9.3.20.v20170531.jar:file:/databricks/hive/maven--commons-collections--commons-collections--commons-collections__commons-collections__3.2.2.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--com.amazonaws--aws-java-sdk-s3--com.amazonaws__aws-java-sdk-s3__1.11.313.jar:file:/databricks/hive/common--lazy--lazy-spark_2.4_2.11_deploy.jar:file:/databricks/hive/dbfs--utils--dbfs-utils-spark_2.4_2.11_deploy.jar:file:/databricks/hive/third_party--azure--com.microsoft.azure__azure-storage__5.2.0_shaded.jar:file:/databricks/hive/spark--maven-trees--spark_2.4--org.scala-lang--scala-reflect_2.11--org.scala-lang__scala-reflect__2.11.12.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--commons-codec__commons-codec__1.9_2.11_shaded_20180920_b33d810.jar:file:/databricks/hive/third_party--hadoop-azure-2.7.3-abfs--com.fasterxml.jackson.core__jackson-core__2.7.2_2.11_shaded_20180625_3682417.jar:file:/databricks/hive/bonecp-configs.jar
19/08/12 22:52:12 ERROR Utils: uncaught error in thread spark-listener-group-shared, stopping SparkContext
java.lang.NoSuchMethodError: org.json4s.jackson.JsonMethods$.parse$default$3()Z
	at org.apache.spark.util.JsonProtocol$.sparkEventToJson(JsonProtocol.scala:119)
	at org.apache.spark.util.PublicJsonProtocol$.sparkEventToJson(PublicJsonProtocol.scala:12)
	at com.databricks.backend.daemon.driver.DBCEventLoggingListener.onEvent(DBCEventLoggingListener.scala:249)
	at org.apache.spark.SparkFirehoseListener.onOtherEvent(SparkFirehoseListener.java:161)
	at org.apache.spark.scheduler.SparkListenerBus$class.doPostEvent(SparkListenerBus.scala:80)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)
	at org.apache.spark.util.ListenerBus$class.postToAll(ListenerBus.scala:93)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$super$postToAll(AsyncEventQueue.scala:92)
	at org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply$mcJ$sp(AsyncEventQueue.scala:92)
	at org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply(AsyncEventQueue.scala:87)
	at org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply(AsyncEventQueue.scala:87)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:87)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$1$$anonfun$run$1.apply$mcV$sp(AsyncEventQueue.scala:83)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1468)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$1.run(AsyncEventQueue.scala:82)
19/08/12 22:52:12 ERROR Utils: throw uncaught fatal error in thread spark-listener-group-shared
java.lang.NoSuchMethodError: org.json4s.jackson.JsonMethods$.parse$default$3()Z
	at org.apache.spark.util.JsonProtocol$.sparkEventToJson(JsonProtocol.scala:119)
	at org.apache.spark.util.PublicJsonProtocol$.sparkEventToJson(PublicJsonProtocol.scala:12)
	at com.databricks.backend.daemon.driver.DBCEventLoggingListener.onEvent(DBCEventLoggingListener.scala:249)
	at org.apache.spark.SparkFirehoseListener.onOtherEvent(SparkFirehoseListener.java:161)
	at org.apache.spark.scheduler.SparkListenerBus$class.doPostEvent(SparkListenerBus.scala:80)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)
	at org.apache.spark.util.ListenerBus$class.postToAll(ListenerBus.scala:93)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$super$postToAll(AsyncEventQueue.scala:92)
	at org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply$mcJ$sp(AsyncEventQueue.scala:92)
	at org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply(AsyncEventQueue.scala:87)
	at org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply(AsyncEventQueue.scala:87)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:87)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$1$$anonfun$run$1.apply$mcV$sp(AsyncEventQueue.scala:83)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1468)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$1.run(AsyncEventQueue.scala:82)
19/08/12 22:52:12 ERROR DatabricksMain$DBUncaughtExceptionHandler: Uncaught exception in thread spark-listener-group-shared!
java.lang.NoSuchMethodError: org.json4s.jackson.JsonMethods$.parse$default$3()Z
	at org.apache.spark.util.JsonProtocol$.sparkEventToJson(JsonProtocol.scala:119)
	at org.apache.spark.util.PublicJsonProtocol$.sparkEventToJson(PublicJsonProtocol.scala:12)
	at com.databricks.backend.daemon.driver.DBCEventLoggingListener.onEvent(DBCEventLoggingListener.scala:249)
	at org.apache.spark.SparkFirehoseListener.onOtherEvent(SparkFirehoseListener.java:161)
	at org.apache.spark.scheduler.SparkListenerBus$class.doPostEvent(SparkListenerBus.scala:80)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)
	at org.apache.spark.util.ListenerBus$class.postToAll(ListenerBus.scala:93)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$super$postToAll(AsyncEventQueue.scala:92)
	at org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply$mcJ$sp(AsyncEventQueue.scala:92)
	at org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply(AsyncEventQueue.scala:87)
	at org.apache.spark.scheduler.AsyncEventQueue$$anonfun$org$apache$spark$scheduler$AsyncEventQueue$$dispatch$1.apply(AsyncEventQueue.scala:87)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:87)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$1$$anonfun$run$1.apply$mcV$sp(AsyncEventQueue.scala:83)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1468)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$1.run(AsyncEventQueue.scala:82)
19/08/12 22:52:12 INFO AbstractConnector: Stopped Spark@591a4f8e{HTTP/1.1,[http/1.1]}{10.67.237.124:48182}
19/08/12 22:52:12 INFO SparkUI: Stopped Spark web UI at http://10.67.237.124:48182
19/08/12 22:52:12 INFO StandaloneSchedulerBackend: Shutting down all executors
19/08/12 22:52:12 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down
19/08/12 22:52:12 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
19/08/12 22:52:12 INFO MemoryStore: MemoryStore cleared
19/08/12 22:52:12 INFO BlockManager: BlockManager stopped
19/08/12 22:52:12 INFO BlockManagerMaster: BlockManagerMaster stopped
19/08/12 22:52:12 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
19/08/12 22:52:12 INFO SparkContext: Successfully stopped SparkContext
19/08/12 22:52:12 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
19/08/12 22:52:12 INFO ObjectStore: ObjectStore, initialize called
19/08/12 22:52:12 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
19/08/12 22:52:12 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
19/08/12 22:52:12 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
19/08/12 22:52:13 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
19/08/12 22:52:13 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
19/08/12 22:52:13 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
19/08/12 22:52:13 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
19/08/12 22:52:13 INFO Query: Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
19/08/12 22:52:13 INFO ObjectStore: Initialized ObjectStore
19/08/12 22:52:13 INFO HiveMetaStore: Added admin role in metastore
19/08/12 22:52:13 INFO HiveMetaStore: Added public role in metastore
19/08/12 22:52:13 INFO HiveMetaStore: No user is added in admin role, since config is empty
19/08/12 22:52:13 INFO SessionState: No Tez session required at this point. hive.execution.engine=mr.
19/08/12 22:52:13 INFO HiveClientImpl: Warehouse location for Hive client (version 0.13.1) is /user/hive/warehouse
19/08/12 22:52:13 INFO HiveMetaStore: 0: get_database: default
19/08/12 22:52:13 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
19/08/12 22:52:13 WARN SparkContext: The jar /local_disk0/tmp/addedFile44360902786793200555698d3bc_4eae_4017_adc7_a960409bcd16_hail_all_spark_71440-df353.jar has been added already. Overwriting of added jars is not supported in the current version.
19/08/12 22:52:13 WARN SQLExecution: Error executing advisor analysis 
java.lang.NullPointerException
	at com.databricks.sql.InstanceSpecificConf$.workerInstanceTypeFromSparkConf(InstanceSpecificConf.scala:71)
	at com.databricks.sql.InstanceSpecificConf$.getDefaultIOCacheEnabled(InstanceSpecificConf.scala:349)
	at com.databricks.sql.DatabricksSQLConf$$anonfun$7.apply$mcZ$sp(DatabricksSQLConf.scala:926)
	at com.databricks.sql.DatabricksSQLConf$$anonfun$7.apply(DatabricksSQLConf.scala:926)
	at com.databricks.sql.DatabricksSQLConf$$anonfun$7.apply(DatabricksSQLConf.scala:926)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.internal.config.ConfigEntryWithDefaultFunction.readFrom(ConfigEntry.scala:103)
	at org.apache.spark.sql.internal.SQLConf.getConf(SQLConf.scala:2146)
	at com.databricks.sql.io.caching.CachingUtils$.isParquetCachingOn(CachingUtils.scala:64)
	at com.databricks.sql.advice.AdviceGenerator.com$databricks$sql$advice$AdviceGenerator$$analyzeForUseDBIOCacheAdvice(AdviceGenerator.scala:101)
	at com.databricks.sql.advice.AdviceGenerator$$anonfun$11.apply(AdviceGenerator.scala:238)
	at com.databricks.sql.advice.AdviceGenerator$$anonfun$11.apply(AdviceGenerator.scala:238)
	at com.databricks.sql.advice.AdviceGenerator$$anonfun$analyzeAndGenerateAdvice$1.apply(AdviceGenerator.scala:248)
	at com.databricks.sql.advice.AdviceGenerator$$anonfun$analyzeAndGenerateAdvice$1.apply(AdviceGenerator.scala:248)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
	at scala.collection.immutable.List.flatMap(List.scala:355)
	at com.databricks.sql.advice.AdviceGenerator.analyzeAndGenerateAdvice(AdviceGenerator.scala:248)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1.apply(SQLExecution.scala:138)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:240)
	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:170)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3425)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:205)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:696)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:716)
	at com.databricks.backend.daemon.driver.DriverLocal$$anonfun$4$$anonfun$apply$1.apply(DriverLocal.scala:153)
	at com.databricks.backend.daemon.driver.DriverLocal$$anonfun$4$$anonfun$apply$1.apply(DriverLocal.scala:153)
	at com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:763)
	at com.databricks.backend.daemon.driver.DriverLocal$$anonfun$4.apply(DriverLocal.scala:152)
	at com.databricks.backend.daemon.driver.DriverLocal$$anonfun$4.apply(DriverLocal.scala:137)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at com.databricks.backend.daemon.driver.DriverLocal.<init>(DriverLocal.scala:137)
	at com.databricks.backend.daemon.driver.PythonDriverLocal.<init>(PythonDriverLocal.scala:91)
	at com.databricks.backend.daemon.driver.PythonDriverWrapper.instantiateDriver(DriverWrapper.scala:728)
	at com.databricks.backend.daemon.driver.DriverWrapper.setupRepl(DriverWrapper.scala:299)
	at com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:218)
	at java.lang.Thread.run(Thread.java:748)
19/08/12 22:52:13 WARN PythonDriverWrapper: Failed to start repl ReplId-6b2c8-729d3-8d77a-d
java.lang.NullPointerException
	at org.apache.spark.util.DatabricksUtils$.getPythonVirtualEnvDir(DatabricksUtils.scala:389)
	at org.apache.spark.util.DatabricksUtils$.getOrCreatePythonVirtualEnvDir(DatabricksUtils.scala:605)
	at com.databricks.backend.daemon.driver.PythonDriverLocal$.com$databricks$backend$daemon$driver$PythonDriverLocal$$createPythonSubprocess(PythonDriverLocal.scala:1296)
	at com.databricks.backend.daemon.driver.PythonDriverLocal.launchPython(PythonDriverLocal.scala:426)
	at com.databricks.backend.daemon.driver.PythonDriverLocal.startPython0(PythonDriverLocal.scala:1070)
	at com.databricks.backend.daemon.driver.PythonDriverLocal.startPython(PythonDriverLocal.scala:1121)
	at com.databricks.backend.daemon.driver.PythonDriverLocal.init(PythonDriverLocal.scala:1154)
	at com.databricks.backend.daemon.driver.PythonDriverLocal.<init>(PythonDriverLocal.scala:1162)
	at com.databricks.backend.daemon.driver.PythonDriverWrapper.instantiateDriver(DriverWrapper.scala:728)
	at com.databricks.backend.daemon.driver.DriverWrapper.setupRepl(DriverWrapper.scala:299)
	at com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:218)
	at java.lang.Thread.run(Thread.java:748)
19/08/12 22:52:13 WARN PythonDriverWrapper: setupRepl:ReplId-6b2c8-729d3-8d77a-d: at the end, the status is Error(ReplId-6b2c8-729d3-8d77a-d,java.lang.NullPointerException)
19/08/12 22:52:13 INFO DriverCorral$: Cleaning the wrapper ReplId-6b2c8-729d3-8d77a-d (currently in status Stopped(ReplId-6b2c8-729d3-8d77a-d))
19/08/12 22:52:13 INFO DriverCorral$: sending shutdown signal for REPL ReplId-6b2c8-729d3-8d77a-d
19/08/12 22:52:13 WARN PythonDriverWrapper: Repl ReplId-6b2c8-729d3-8d77a-d is already shutting down: Stopped(ReplId-6b2c8-729d3-8d77a-d)
19/08/12 22:52:13 INFO DriverCorral$: sending the interrupt signal for REPL ReplId-6b2c8-729d3-8d77a-d
19/08/12 22:52:13 INFO DriverCorral$: waiting for localThread to stop for REPL ReplId-6b2c8-729d3-8d77a-d
19/08/12 22:52:13 INFO DriverCorral$: ReplId-6b2c8-729d3-8d77a-d successfully discarded
19/08/12 22:52:13 ERROR DatabricksMain$DBUncaughtExceptionHandler: Uncaught exception in thread pool-38-thread-1!
java.lang.NullPointerException
	at org.apache.spark.util.DatabricksUtils$.getPythonVirtualEnvDir(DatabricksUtils.scala:389)
	at com.databricks.backend.daemon.driver.DriverCorral$$anon$1.run(DriverCorral.scala:328)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
19/08/12 22:52:18 ERROR DriverCorral: Failure when launching REPL ReplId-9501e-e4b33-0d2bf
java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SparkContext.<init>(SparkContext.scala:90)
com.databricks.backend.daemon.driver.DatabricksILoop$$anonfun$4.apply(DatabricksILoop.scala:345)
com.databricks.backend.daemon.driver.DatabricksILoop$$anonfun$4.apply(DatabricksILoop.scala:345)
com.databricks.backend.daemon.driver.ClassLoaders$.withContextClassLoader(ClassLoaders.scala:29)
com.databricks.backend.daemon.driver.DatabricksILoop$.initializeSharedDriverContext(DatabricksILoop.scala:344)
com.databricks.backend.daemon.driver.DatabricksILoop$.getOrCreateSharedDriverContext(DatabricksILoop.scala:273)
com.databricks.backend.daemon.driver.DriverCorral.com$databricks$backend$daemon$driver$DriverCorral$$driverContext(DriverCorral.scala:175)
com.databricks.backend.daemon.driver.DriverCorral.<init>(DriverCorral.scala:211)
com.databricks.backend.daemon.driver.DriverDaemon.<init>(DriverDaemon.scala:34)
com.databricks.backend.daemon.driver.DriverDaemon$.create(DriverDaemon.scala:150)
com.databricks.backend.daemon.driver.DriverDaemon$.wrappedMain(DriverDaemon.scala:155)
com.databricks.DatabricksMain$$anonfun$main$1.apply$mcV$sp(DatabricksMain.scala:88)
com.databricks.DatabricksMain$$anonfun$main$1.apply(DatabricksMain.scala:87)
com.databricks.DatabricksMain$$anonfun$main$1.apply(DatabricksMain.scala:87)
com.databricks.DatabricksMain$$anonfun$1.apply(DatabricksMain.scala:245)
com.databricks.logging.UsageLogging$$anonfun$recordOperation$1.apply(UsageLogging.scala:369)
com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)
scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)
com.databricks.DatabricksMain.withAttributionContext(DatabricksMain.scala:57)

The currently active SparkContext was created at:

(No active SparkContext.)
         
	at org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:114)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:114)
	at org.apache.spark.sql.SparkSession.newSession(SparkSession.scala:270)
	at org.apache.spark.sql.hive.HiveContext.newSession(HiveContext.scala:49)
	at org.apache.spark.sql.hive.HiveContext.newSession(HiveContext.scala:32)
	at com.databricks.backend.daemon.driver.DriverSparkHooks.sessionizedDriverHooks(DriverSparkHooks.scala:164)
	at com.databricks.backend.daemon.driver.DriverCorral.com$databricks$backend$daemon$driver$DriverCorral$$createDriverSparkHooks(DriverCorral.scala:286)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$6.apply(DriverCorral.scala:298)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$6.apply(DriverCorral.scala:298)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:79)
	at com.databricks.backend.daemon.driver.DriverCorral.addReplToExecutionContext(DriverCorral.scala:296)
	at com.databricks.backend.daemon.driver.DriverCorral.com$databricks$backend$daemon$driver$DriverCorral$$handleRequest(DriverCorral.scala:390)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:695)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:691)
	at com.databricks.rpc.ServerBackend$$anonfun$internalReceive$1$$anonfun$apply$1.apply(ServerBackend.scala:44)
	at com.databricks.rpc.ServerBackend$$anonfun$internalReceive$1$$anonfun$apply$1.apply(ServerBackend.scala:40)
	at com.databricks.rpc.ServerBackend$$anonfun$com$databricks$rpc$ServerBackend$$commonReceive$1.applyOrElse(ServerBackend.scala:61)
	at com.databricks.rpc.ServerBackend$$anonfun$com$databricks$rpc$ServerBackend$$commonReceive$1.applyOrElse(ServerBackend.scala:61)
	at com.databricks.rpc.ServerBackend$$anonfun$internalReceive$1.apply(ServerBackend.scala:39)
	at com.databricks.logging.UsageLogging$$anonfun$recordOperation$1.apply(UsageLogging.scala:369)
	at com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)
	at com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:13)
	at com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:271)
	at com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:13)
	at com.databricks.logging.UsageLogging$class.recordOperation(UsageLogging.scala:350)
	at com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:13)
	at com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:38)
	at com.databricks.rpc.JettyServer$RequestManager$$anonfun$11.apply(JettyServer.scala:399)
	at scala.util.Try$.apply(Try.scala:192)
	at com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:399)
	at com.databricks.rpc.JettyServer$RequestManager.com$databricks$rpc$JettyServer$RequestManager$$handleRequestAndRespond(JettyServer.scala:302)
	at com.databricks.rpc.JettyServer$RequestManager$$anonfun$handleHttp$1.apply$mcV$sp(JettyServer.scala:190)
	at com.databricks.rpc.JettyServer$RequestManager$$anonfun$handleHttp$1.apply(JettyServer.scala:181)
	at com.databricks.rpc.JettyServer$RequestManager$$anonfun$handleHttp$1.apply(JettyServer.scala:181)
	at com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)
	at com.databricks.rpc.JettyServer$.withAttributionContext(JettyServer.scala:116)
	at com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:271)
	at com.databricks.rpc.JettyServer$.withAttributionTags(JettyServer.scala:116)
	at com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:180)
	at com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:143)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:584)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:514)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)
	at org.eclipse.jetty.server.Server.handle(Server.java:534)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)
	at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)
	at java.lang.Thread.run(Thread.java:748)
19/08/12 22:52:23 ERROR DriverCorral: Failure when launching REPL ReplId-33ff6-7bf41-8de81-3
java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SparkContext.<init>(SparkContext.scala:90)
com.databricks.backend.daemon.driver.DatabricksILoop$$anonfun$4.apply(DatabricksILoop.scala:345)
com.databricks.backend.daemon.driver.DatabricksILoop$$anonfun$4.apply(DatabricksILoop.scala:345)
com.databricks.backend.daemon.driver.ClassLoaders$.withContextClassLoader(ClassLoaders.scala:29)
com.databricks.backend.daemon.driver.DatabricksILoop$.initializeSharedDriverContext(DatabricksILoop.scala:344)
com.databricks.backend.daemon.driver.DatabricksILoop$.getOrCreateSharedDriverContext(DatabricksILoop.scala:273)
com.databricks.backend.daemon.driver.DriverCorral.com$databricks$backend$daemon$driver$DriverCorral$$driverContext(DriverCorral.scala:175)
com.databricks.backend.daemon.driver.DriverCorral.<init>(DriverCorral.scala:211)
com.databricks.backend.daemon.driver.DriverDaemon.<init>(DriverDaemon.scala:34)
com.databricks.backend.daemon.driver.DriverDaemon$.create(DriverDaemon.scala:150)
com.databricks.backend.daemon.driver.DriverDaemon$.wrappedMain(DriverDaemon.scala:155)
com.databricks.DatabricksMain$$anonfun$main$1.apply$mcV$sp(DatabricksMain.scala:88)
com.databricks.DatabricksMain$$anonfun$main$1.apply(DatabricksMain.scala:87)
com.databricks.DatabricksMain$$anonfun$main$1.apply(DatabricksMain.scala:87)
com.databricks.DatabricksMain$$anonfun$1.apply(DatabricksMain.scala:245)
com.databricks.logging.UsageLogging$$anonfun$recordOperation$1.apply(UsageLogging.scala:369)
com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)
scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)
com.databricks.DatabricksMain.withAttributionContext(DatabricksMain.scala:57)

The currently active SparkContext was created at:

(No active SparkContext.)
         
	at org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:114)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:114)
	at org.apache.spark.sql.SparkSession.newSession(SparkSession.scala:270)
	at org.apache.spark.sql.hive.HiveContext.newSession(HiveContext.scala:49)
	at org.apache.spark.sql.hive.HiveContext.newSession(HiveContext.scala:32)
	at com.databricks.backend.daemon.driver.DriverSparkHooks.sessionizedDriverHooks(DriverSparkHooks.scala:164)
	at com.databricks.backend.daemon.driver.DriverCorral.com$databricks$backend$daemon$driver$DriverCorral$$createDriverSparkHooks(DriverCorral.scala:286)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$6.apply(DriverCorral.scala:298)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$6.apply(DriverCorral.scala:298)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:79)
	at com.databricks.backend.daemon.driver.DriverCorral.addReplToExecutionContext(DriverCorral.scala:296)
	at com.databricks.backend.daemon.driver.DriverCorral.com$databricks$backend$daemon$driver$DriverCorral$$handleRequest(DriverCorral.scala:390)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:695)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:691)
	at com.databricks.rpc.ServerBackend$$anonfun$internalReceive$1$$anonfun$apply$1.apply(ServerBackend.scala:44)
	at com.databricks.rpc.ServerBackend$$anonfun$internalReceive$1$$anonfun$apply$1.apply(ServerBackend.scala:40)
	at com.databricks.rpc.ServerBackend$$anonfun$com$databricks$rpc$ServerBackend$$commonReceive$1.applyOrElse(ServerBackend.scala:61)
	at com.databricks.rpc.ServerBackend$$anonfun$com$databricks$rpc$ServerBackend$$commonReceive$1.applyOrElse(ServerBackend.scala:61)
	at com.databricks.rpc.ServerBackend$$anonfun$internalReceive$1.apply(ServerBackend.scala:39)
	at com.databricks.logging.UsageLogging$$anonfun$recordOperation$1.apply(UsageLogging.scala:369)
	at com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)
	at com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:13)
	at com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:271)
	at com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:13)
	at com.databricks.logging.UsageLogging$class.recordOperation(UsageLogging.scala:350)
	at com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:13)
	at com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:38)
	at com.databricks.rpc.JettyServer$RequestManager$$anonfun$11.apply(JettyServer.scala:399)
	at scala.util.Try$.apply(Try.scala:192)
	at com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:399)
	at com.databricks.rpc.JettyServer$RequestManager.com$databricks$rpc$JettyServer$RequestManager$$handleRequestAndRespond(JettyServer.scala:302)
	at com.databricks.rpc.JettyServer$RequestManager$$anonfun$handleHttp$1.apply$mcV$sp(JettyServer.scala:190)
	at com.databricks.rpc.JettyServer$RequestManager$$anonfun$handleHttp$1.apply(JettyServer.scala:181)
	at com.databricks.rpc.JettyServer$RequestManager$$anonfun$handleHttp$1.apply(JettyServer.scala:181)
	at com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)
	at com.databricks.rpc.JettyServer$.withAttributionContext(JettyServer.scala:116)
	at com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:271)
	at com.databricks.rpc.JettyServer$.withAttributionTags(JettyServer.scala:116)
	at com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:180)
	at com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:143)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:584)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:514)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)
	at org.eclipse.jetty.server.Server.handle(Server.java:534)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)
	at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)
	at java.lang.Thread.run(Thread.java:748)
19/08/12 22:52:29 ERROR DriverCorral: Failure when launching REPL ReplId-6477f-79f0f-f4438-1
java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SparkContext.<init>(SparkContext.scala:90)
com.databricks.backend.daemon.driver.DatabricksILoop$$anonfun$4.apply(DatabricksILoop.scala:345)
com.databricks.backend.daemon.driver.DatabricksILoop$$anonfun$4.apply(DatabricksILoop.scala:345)
com.databricks.backend.daemon.driver.ClassLoaders$.withContextClassLoader(ClassLoaders.scala:29)
com.databricks.backend.daemon.driver.DatabricksILoop$.initializeSharedDriverContext(DatabricksILoop.scala:344)
com.databricks.backend.daemon.driver.DatabricksILoop$.getOrCreateSharedDriverContext(DatabricksILoop.scala:273)
com.databricks.backend.daemon.driver.DriverCorral.com$databricks$backend$daemon$driver$DriverCorral$$driverContext(DriverCorral.scala:175)
com.databricks.backend.daemon.driver.DriverCorral.<init>(DriverCorral.scala:211)
com.databricks.backend.daemon.driver.DriverDaemon.<init>(DriverDaemon.scala:34)
com.databricks.backend.daemon.driver.DriverDaemon$.create(DriverDaemon.scala:150)
com.databricks.backend.daemon.driver.DriverDaemon$.wrappedMain(DriverDaemon.scala:155)
com.databricks.DatabricksMain$$anonfun$main$1.apply$mcV$sp(DatabricksMain.scala:88)
com.databricks.DatabricksMain$$anonfun$main$1.apply(DatabricksMain.scala:87)
com.databricks.DatabricksMain$$anonfun$main$1.apply(DatabricksMain.scala:87)
com.databricks.DatabricksMain$$anonfun$1.apply(DatabricksMain.scala:245)
com.databricks.logging.UsageLogging$$anonfun$recordOperation$1.apply(UsageLogging.scala:369)
com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)
scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)
com.databricks.DatabricksMain.withAttributionContext(DatabricksMain.scala:57)

The currently active SparkContext was created at:

(No active SparkContext.)
         
	at org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:114)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:114)
	at org.apache.spark.sql.SparkSession.newSession(SparkSession.scala:270)
	at org.apache.spark.sql.hive.HiveContext.newSession(HiveContext.scala:49)
	at org.apache.spark.sql.hive.HiveContext.newSession(HiveContext.scala:32)
	at com.databricks.backend.daemon.driver.DriverSparkHooks.sessionizedDriverHooks(DriverSparkHooks.scala:164)
	at com.databricks.backend.daemon.driver.DriverCorral.com$databricks$backend$daemon$driver$DriverCorral$$createDriverSparkHooks(DriverCorral.scala:286)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$6.apply(DriverCorral.scala:298)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$6.apply(DriverCorral.scala:298)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:79)
	at com.databricks.backend.daemon.driver.DriverCorral.addReplToExecutionContext(DriverCorral.scala:296)
	at com.databricks.backend.daemon.driver.DriverCorral.com$databricks$backend$daemon$driver$DriverCorral$$handleRequest(DriverCorral.scala:390)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:695)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:691)
	at com.databricks.rpc.ServerBackend$$anonfun$internalReceive$1$$anonfun$apply$1.apply(ServerBackend.scala:44)
	at com.databricks.rpc.ServerBackend$$anonfun$internalReceive$1$$anonfun$apply$1.apply(ServerBackend.scala:40)
	at com.databricks.rpc.ServerBackend$$anonfun$com$databricks$rpc$ServerBackend$$commonReceive$1.applyOrElse(ServerBackend.scala:61)
	at com.databricks.rpc.ServerBackend$$anonfun$com$databricks$rpc$ServerBackend$$commonReceive$1.applyOrElse(ServerBackend.scala:61)
	at com.databricks.rpc.ServerBackend$$anonfun$internalReceive$1.apply(ServerBackend.scala:39)
	at com.databricks.logging.UsageLogging$$anonfun$recordOperation$1.apply(UsageLogging.scala:369)
	at com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)
	at com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:13)
	at com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:271)
	at com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:13)
	at com.databricks.logging.UsageLogging$class.recordOperation(UsageLogging.scala:350)
	at com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:13)
	at com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:38)
	at com.databricks.rpc.JettyServer$RequestManager$$anonfun$11.apply(JettyServer.scala:399)
	at scala.util.Try$.apply(Try.scala:192)
	at com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:399)
	at com.databricks.rpc.JettyServer$RequestManager.com$databricks$rpc$JettyServer$RequestManager$$handleRequestAndRespond(JettyServer.scala:302)
	at com.databricks.rpc.JettyServer$RequestManager$$anonfun$handleHttp$1.apply$mcV$sp(JettyServer.scala:190)
	at com.databricks.rpc.JettyServer$RequestManager$$anonfun$handleHttp$1.apply(JettyServer.scala:181)
	at com.databricks.rpc.JettyServer$RequestManager$$anonfun$handleHttp$1.apply(JettyServer.scala:181)
	at com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)
	at com.databricks.rpc.JettyServer$.withAttributionContext(JettyServer.scala:116)
	at com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:271)
	at com.databricks.rpc.JettyServer$.withAttributionTags(JettyServer.scala:116)
	at com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:180)
	at com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:143)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:584)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:514)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)
	at org.eclipse.jetty.server.Server.handle(Server.java:534)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)
	at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)
	at java.lang.Thread.run(Thread.java:748)
19/08/12 22:52:34 ERROR DriverCorral: Failure when launching REPL ReplId-75c9a-bc5fd-bc4aa-0
java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SparkContext.<init>(SparkContext.scala:90)
com.databricks.backend.daemon.driver.DatabricksILoop$$anonfun$4.apply(DatabricksILoop.scala:345)
com.databricks.backend.daemon.driver.DatabricksILoop$$anonfun$4.apply(DatabricksILoop.scala:345)
com.databricks.backend.daemon.driver.ClassLoaders$.withContextClassLoader(ClassLoaders.scala:29)
com.databricks.backend.daemon.driver.DatabricksILoop$.initializeSharedDriverContext(DatabricksILoop.scala:344)
com.databricks.backend.daemon.driver.DatabricksILoop$.getOrCreateSharedDriverContext(DatabricksILoop.scala:273)
com.databricks.backend.daemon.driver.DriverCorral.com$databricks$backend$daemon$driver$DriverCorral$$driverContext(DriverCorral.scala:175)
com.databricks.backend.daemon.driver.DriverCorral.<init>(DriverCorral.scala:211)
com.databricks.backend.daemon.driver.DriverDaemon.<init>(DriverDaemon.scala:34)
com.databricks.backend.daemon.driver.DriverDaemon$.create(DriverDaemon.scala:150)
com.databricks.backend.daemon.driver.DriverDaemon$.wrappedMain(DriverDaemon.scala:155)
com.databricks.DatabricksMain$$anonfun$main$1.apply$mcV$sp(DatabricksMain.scala:88)
com.databricks.DatabricksMain$$anonfun$main$1.apply(DatabricksMain.scala:87)
com.databricks.DatabricksMain$$anonfun$main$1.apply(DatabricksMain.scala:87)
com.databricks.DatabricksMain$$anonfun$1.apply(DatabricksMain.scala:245)
com.databricks.logging.UsageLogging$$anonfun$recordOperation$1.apply(UsageLogging.scala:369)
com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)
scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)
com.databricks.DatabricksMain.withAttributionContext(DatabricksMain.scala:57)

The currently active SparkContext was created at:

(No active SparkContext.)
         
	at org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:114)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:114)
	at org.apache.spark.sql.SparkSession.newSession(SparkSession.scala:270)
	at org.apache.spark.sql.hive.HiveContext.newSession(HiveContext.scala:49)
	at org.apache.spark.sql.hive.HiveContext.newSession(HiveContext.scala:32)
	at com.databricks.backend.daemon.driver.DriverSparkHooks.sessionizedDriverHooks(DriverSparkHooks.scala:164)
	at com.databricks.backend.daemon.driver.DriverCorral.com$databricks$backend$daemon$driver$DriverCorral$$createDriverSparkHooks(DriverCorral.scala:286)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$6.apply(DriverCorral.scala:298)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$6.apply(DriverCorral.scala:298)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:79)
	at com.databricks.backend.daemon.driver.DriverCorral.addReplToExecutionContext(DriverCorral.scala:296)
	at com.databricks.backend.daemon.driver.DriverCorral.com$databricks$backend$daemon$driver$DriverCorral$$handleRequest(DriverCorral.scala:390)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:695)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:691)
	at com.databricks.rpc.ServerBackend$$anonfun$internalReceive$1$$anonfun$apply$1.apply(ServerBackend.scala:44)
	at com.databricks.rpc.ServerBackend$$anonfun$internalReceive$1$$anonfun$apply$1.apply(ServerBackend.scala:40)
	at com.databricks.rpc.ServerBackend$$anonfun$com$databricks$rpc$ServerBackend$$commonReceive$1.applyOrElse(ServerBackend.scala:61)
	at com.databricks.rpc.ServerBackend$$anonfun$com$databricks$rpc$ServerBackend$$commonReceive$1.applyOrElse(ServerBackend.scala:61)
	at com.databricks.rpc.ServerBackend$$anonfun$internalReceive$1.apply(ServerBackend.scala:39)
	at com.databricks.logging.UsageLogging$$anonfun$recordOperation$1.apply(UsageLogging.scala:369)
	at com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)
	at com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:13)
	at com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:271)
	at com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:13)
	at com.databricks.logging.UsageLogging$class.recordOperation(UsageLogging.scala:350)
	at com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:13)
	at com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:38)
	at com.databricks.rpc.JettyServer$RequestManager$$anonfun$11.apply(JettyServer.scala:399)
	at scala.util.Try$.apply(Try.scala:192)
	at com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:399)
	at com.databricks.rpc.JettyServer$RequestManager.com$databricks$rpc$JettyServer$RequestManager$$handleRequestAndRespond(JettyServer.scala:302)
	at com.databricks.rpc.JettyServer$RequestManager$$anonfun$handleHttp$1.apply$mcV$sp(JettyServer.scala:190)
	at com.databricks.rpc.JettyServer$RequestManager$$anonfun$handleHttp$1.apply(JettyServer.scala:181)
	at com.databricks.rpc.JettyServer$RequestManager$$anonfun$handleHttp$1.apply(JettyServer.scala:181)
	at com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)
	at com.databricks.rpc.JettyServer$.withAttributionContext(JettyServer.scala:116)
	at com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:271)
	at com.databricks.rpc.JettyServer$.withAttributionTags(JettyServer.scala:116)
	at com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:180)
	at com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:143)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:584)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:514)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)
	at org.eclipse.jetty.server.Server.handle(Server.java:534)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)
	at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)
	at java.lang.Thread.run(Thread.java:748)
19/08/12 22:52:52 WARN UntrustedUtils$: Uncaught exception in thread PrometheusMetricsExporter
java.lang.NullPointerException
	at org.apache.spark.storage.BlockManagerMaster.getStorageStatus(BlockManagerMaster.scala:169)
	at org.apache.spark.storage.BlockManagerSource$$anon$2.get(BlockManagerSource.scala:41)
	at org.apache.spark.storage.BlockManagerSource$$anon$2.get(BlockManagerSource.scala:39)
	at com.google.common.base.Suppliers$ExpiringMemoizingSupplier.get(Suppliers.java:192)
	at org.apache.spark.storage.BlockManagerSource$$anonfun$1$$anon$1.getValue(BlockManagerSource.scala:72)
	at org.apache.spark.storage.BlockManagerSource$$anonfun$1$$anon$1.getValue(BlockManagerSource.scala:71)
	at com.databricks.backend.daemon.driver.SparkMetricsExporter$$anonfun$fromGauge$1.apply(SparkMetricsExporter.scala:131)
	at com.databricks.backend.daemon.driver.SparkMetricsExporter$$anonfun$fromGauge$1.apply(SparkMetricsExporter.scala:129)
	at scala.Option.map(Option.scala:146)
	at com.databricks.backend.daemon.driver.SparkMetricsExporter$.fromGauge(SparkMetricsExporter.scala:129)
	at com.databricks.backend.daemon.driver.SparkMetricsExporter$$anonfun$collect$1.apply(SparkMetricsExporter.scala:51)
	at com.databricks.backend.daemon.driver.SparkMetricsExporter$$anonfun$collect$1.apply(SparkMetricsExporter.scala:47)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:104)
	at com.databricks.backend.daemon.driver.SparkMetricsExporter.collect(SparkMetricsExporter.scala:47)
	at io.prometheus.client.CollectorRegistry$MetricFamilySamplesEnumeration.findNextElement(CollectorRegistry.java:199)
	at io.prometheus.client.CollectorRegistry$MetricFamilySamplesEnumeration.nextElement(CollectorRegistry.java:232)
	at io.prometheus.client.CollectorRegistry$MetricFamilySamplesEnumeration.nextElement(CollectorRegistry.java:153)
	at io.prometheus.client.exporter.common.TextFormat.write004(TextFormat.java:22)
	at com.databricks.DatabricksMain.encodedMetricsBlob(DatabricksMain.scala:363)
	at com.databricks.DatabricksMain$$anonfun$startPrometheusMetricsExport$1.apply$mcV$sp(DatabricksMain.scala:341)
	at com.databricks.DatabricksMain$$anonfun$startPrometheusMetricsExport$1.apply(DatabricksMain.scala:340)
	at com.databricks.DatabricksMain$$anonfun$startPrometheusMetricsExport$1.apply(DatabricksMain.scala:340)
	at com.databricks.util.UntrustedUtils$.tryLog(UntrustedUtils.scala:98)
	at com.databricks.threading.NamedTimer$$anon$1$$anonfun$run$1.apply(NamedTimer.scala:54)
	at com.databricks.threading.NamedTimer$$anon$1$$anonfun$run$1.apply(NamedTimer.scala:54)
	at com.databricks.logging.UsageLogging$$anonfun$recordOperation$1.apply(UsageLogging.scala:369)
	at com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)
	at com.databricks.threading.NamedTimer$$anon$1.withAttributionContext(NamedTimer.scala:51)
	at com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:271)
	at com.databricks.threading.NamedTimer$$anon$1.withAttributionTags(NamedTimer.scala:51)
	at com.databricks.logging.UsageLogging$class.recordOperation(UsageLogging.scala:350)
	at com.databricks.threading.NamedTimer$$anon$1.recordOperation(NamedTimer.scala:51)
	at com.databricks.threading.NamedTimer$$anon$1.run(NamedTimer.scala:53)
	at java.util.TimerThread.mainLoop(Timer.java:555)
	at java.util.TimerThread.run(Timer.java:505)
19/08/12 22:53:52 WARN UntrustedUtils$: Uncaught exception in thread PrometheusMetricsExporter
java.lang.NullPointerException
	at org.apache.spark.storage.BlockManagerMaster.getStorageStatus(BlockManagerMaster.scala:169)
	at org.apache.spark.storage.BlockManagerSource$$anon$2.get(BlockManagerSource.scala:41)
	at org.apache.spark.storage.BlockManagerSource$$anon$2.get(BlockManagerSource.scala:39)
	at com.google.common.base.Suppliers$ExpiringMemoizingSupplier.get(Suppliers.java:192)
	at org.apache.spark.storage.BlockManagerSource$$anonfun$1$$anon$1.getValue(BlockManagerSource.scala:72)
	at org.apache.spark.storage.BlockManagerSource$$anonfun$1$$anon$1.getValue(BlockManagerSource.scala:71)
	at com.databricks.backend.daemon.driver.SparkMetricsExporter$$anonfun$fromGauge$1.apply(SparkMetricsExporter.scala:131)
	at com.databricks.backend.daemon.driver.SparkMetricsExporter$$anonfun$fromGauge$1.apply(SparkMetricsExporter.scala:129)
	at scala.Option.map(Option.scala:146)
	at com.databricks.backend.daemon.driver.SparkMetricsExporter$.fromGauge(SparkMetricsExporter.scala:129)
	at com.databricks.backend.daemon.driver.SparkMetricsExporter$$anonfun$collect$1.apply(SparkMetricsExporter.scala:51)
	at com.databricks.backend.daemon.driver.SparkMetricsExporter$$anonfun$collect$1.apply(SparkMetricsExporter.scala:47)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:104)
	at com.databricks.backend.daemon.driver.SparkMetricsExporter.collect(SparkMetricsExporter.scala:47)
	at io.prometheus.client.CollectorRegistry$MetricFamilySamplesEnumeration.findNextElement(CollectorRegistry.java:199)
	at io.prometheus.client.CollectorRegistry$MetricFamilySamplesEnumeration.nextElement(CollectorRegistry.java:232)
	at io.prometheus.client.CollectorRegistry$MetricFamilySamplesEnumeration.nextElement(CollectorRegistry.java:153)
	at io.prometheus.client.exporter.common.TextFormat.write004(TextFormat.java:22)
	at com.databricks.DatabricksMain.encodedMetricsBlob(DatabricksMain.scala:363)
	at com.databricks.DatabricksMain$$anonfun$startPrometheusMetricsExport$1.apply$mcV$sp(DatabricksMain.scala:341)
	at com.databricks.DatabricksMain$$anonfun$startPrometheusMetricsExport$1.apply(DatabricksMain.scala:340)
	at com.databricks.DatabricksMain$$anonfun$startPrometheusMetricsExport$1.apply(DatabricksMain.scala:340)
	at com.databricks.util.UntrustedUtils$.tryLog(UntrustedUtils.scala:98)
	at com.databricks.threading.NamedTimer$$anon$1$$anonfun$run$1.apply(NamedTimer.scala:54)
	at com.databricks.threading.NamedTimer$$anon$1$$anonfun$run$1.apply(NamedTimer.scala:54)
	at com.databricks.logging.UsageLogging$$anonfun$recordOperation$1.apply(UsageLogging.scala:369)
	at com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)
	at com.databricks.threading.NamedTimer$$anon$1.withAttributionContext(NamedTimer.scala:51)
	at com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:271)
	at com.databricks.threading.NamedTimer$$anon$1.withAttributionTags(NamedTimer.scala:51)
	at com.databricks.logging.UsageLogging$class.recordOperation(UsageLogging.scala:350)
	at com.databricks.threading.NamedTimer$$anon$1.recordOperation(NamedTimer.scala:51)
	at com.databricks.threading.NamedTimer$$anon$1.run(NamedTimer.scala:53)
	at java.util.TimerThread.mainLoop(Timer.java:555)
	at java.util.TimerThread.run(Timer.java:505)
19/08/12 22:54:26 ERROR DriverCorral: Failure when launching REPL ReplId-2cc4f-ff05d-56ea2
java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SparkContext.<init>(SparkContext.scala:90)
com.databricks.backend.daemon.driver.DatabricksILoop$$anonfun$4.apply(DatabricksILoop.scala:345)
com.databricks.backend.daemon.driver.DatabricksILoop$$anonfun$4.apply(DatabricksILoop.scala:345)
com.databricks.backend.daemon.driver.ClassLoaders$.withContextClassLoader(ClassLoaders.scala:29)
com.databricks.backend.daemon.driver.DatabricksILoop$.initializeSharedDriverContext(DatabricksILoop.scala:344)
com.databricks.backend.daemon.driver.DatabricksILoop$.getOrCreateSharedDriverContext(DatabricksILoop.scala:273)
com.databricks.backend.daemon.driver.DriverCorral.com$databricks$backend$daemon$driver$DriverCorral$$driverContext(DriverCorral.scala:175)
com.databricks.backend.daemon.driver.DriverCorral.<init>(DriverCorral.scala:211)
com.databricks.backend.daemon.driver.DriverDaemon.<init>(DriverDaemon.scala:34)
com.databricks.backend.daemon.driver.DriverDaemon$.create(DriverDaemon.scala:150)
com.databricks.backend.daemon.driver.DriverDaemon$.wrappedMain(DriverDaemon.scala:155)
com.databricks.DatabricksMain$$anonfun$main$1.apply$mcV$sp(DatabricksMain.scala:88)
com.databricks.DatabricksMain$$anonfun$main$1.apply(DatabricksMain.scala:87)
com.databricks.DatabricksMain$$anonfun$main$1.apply(DatabricksMain.scala:87)
com.databricks.DatabricksMain$$anonfun$1.apply(DatabricksMain.scala:245)
com.databricks.logging.UsageLogging$$anonfun$recordOperation$1.apply(UsageLogging.scala:369)
com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)
scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)
com.databricks.DatabricksMain.withAttributionContext(DatabricksMain.scala:57)

The currently active SparkContext was created at:

(No active SparkContext.)
         
	at org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:114)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:114)
	at org.apache.spark.sql.SparkSession.newSession(SparkSession.scala:270)
	at org.apache.spark.sql.hive.HiveContext.newSession(HiveContext.scala:49)
	at org.apache.spark.sql.hive.HiveContext.newSession(HiveContext.scala:32)
	at com.databricks.backend.daemon.driver.DriverSparkHooks.sessionizedDriverHooks(DriverSparkHooks.scala:164)
	at com.databricks.backend.daemon.driver.DriverCorral.com$databricks$backend$daemon$driver$DriverCorral$$createDriverSparkHooks(DriverCorral.scala:286)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$6.apply(DriverCorral.scala:298)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$6.apply(DriverCorral.scala:298)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:79)
	at com.databricks.backend.daemon.driver.DriverCorral.addReplToExecutionContext(DriverCorral.scala:296)
	at com.databricks.backend.daemon.driver.DriverCorral.com$databricks$backend$daemon$driver$DriverCorral$$handleRequest(DriverCorral.scala:390)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:695)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:691)
	at com.databricks.rpc.ServerBackend$$anonfun$internalReceive$1$$anonfun$apply$1.apply(ServerBackend.scala:44)
	at com.databricks.rpc.ServerBackend$$anonfun$internalReceive$1$$anonfun$apply$1.apply(ServerBackend.scala:40)
	at com.databricks.rpc.ServerBackend$$anonfun$com$databricks$rpc$ServerBackend$$commonReceive$1.applyOrElse(ServerBackend.scala:61)
	at com.databricks.rpc.ServerBackend$$anonfun$com$databricks$rpc$ServerBackend$$commonReceive$1.applyOrElse(ServerBackend.scala:61)
	at com.databricks.rpc.ServerBackend$$anonfun$internalReceive$1.apply(ServerBackend.scala:39)
	at com.databricks.logging.UsageLogging$$anonfun$recordOperation$1.apply(UsageLogging.scala:369)
	at com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)
	at com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:13)
	at com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:271)
	at com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:13)
	at com.databricks.logging.UsageLogging$class.recordOperation(UsageLogging.scala:350)
	at com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:13)
	at com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:38)
	at com.databricks.rpc.JettyServer$RequestManager$$anonfun$11.apply(JettyServer.scala:399)
	at scala.util.Try$.apply(Try.scala:192)
	at com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:399)
	at com.databricks.rpc.JettyServer$RequestManager.com$databricks$rpc$JettyServer$RequestManager$$handleRequestAndRespond(JettyServer.scala:302)
	at com.databricks.rpc.JettyServer$RequestManager$$anonfun$handleHttp$1.apply$mcV$sp(JettyServer.scala:190)
	at com.databricks.rpc.JettyServer$RequestManager$$anonfun$handleHttp$1.apply(JettyServer.scala:181)
	at com.databricks.rpc.JettyServer$RequestManager$$anonfun$handleHttp$1.apply(JettyServer.scala:181)
	at com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)
	at com.databricks.rpc.JettyServer$.withAttributionContext(JettyServer.scala:116)
	at com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:271)
	at com.databricks.rpc.JettyServer$.withAttributionTags(JettyServer.scala:116)
	at com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:180)
	at com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:143)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:584)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:514)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)
	at org.eclipse.jetty.server.Server.handle(Server.java:534)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)
	at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)
	at java.lang.Thread.run(Thread.java:748)
19/08/12 22:54:52 WARN UntrustedUtils$: Uncaught exception in thread PrometheusMetricsExporter
java.lang.NullPointerException
	at org.apache.spark.storage.BlockManagerMaster.getStorageStatus(BlockManagerMaster.scala:169)
	at org.apache.spark.storage.BlockManagerSource$$anon$2.get(BlockManagerSource.scala:41)
	at org.apache.spark.storage.BlockManagerSource$$anon$2.get(BlockManagerSource.scala:39)
	at com.google.common.base.Suppliers$ExpiringMemoizingSupplier.get(Suppliers.java:192)
	at org.apache.spark.storage.BlockManagerSource$$anonfun$1$$anon$1.getValue(BlockManagerSource.scala:72)
	at org.apache.spark.storage.BlockManagerSource$$anonfun$1$$anon$1.getValue(BlockManagerSource.scala:71)
	at com.databricks.backend.daemon.driver.SparkMetricsExporter$$anonfun$fromGauge$1.apply(SparkMetricsExporter.scala:131)
	at com.databricks.backend.daemon.driver.SparkMetricsExporter$$anonfun$fromGauge$1.apply(SparkMetricsExporter.scala:129)
	at scala.Option.map(Option.scala:146)
	at com.databricks.backend.daemon.driver.SparkMetricsExporter$.fromGauge(SparkMetricsExporter.scala:129)
	at com.databricks.backend.daemon.driver.SparkMetricsExporter$$anonfun$collect$1.apply(SparkMetricsExporter.scala:51)
	at com.databricks.backend.daemon.driver.SparkMetricsExporter$$anonfun$collect$1.apply(SparkMetricsExporter.scala:47)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:104)
	at com.databricks.backend.daemon.driver.SparkMetricsExporter.collect(SparkMetricsExporter.scala:47)
	at io.prometheus.client.CollectorRegistry$MetricFamilySamplesEnumeration.findNextElement(CollectorRegistry.java:199)
	at io.prometheus.client.CollectorRegistry$MetricFamilySamplesEnumeration.nextElement(CollectorRegistry.java:232)
	at io.prometheus.client.CollectorRegistry$MetricFamilySamplesEnumeration.nextElement(CollectorRegistry.java:153)
	at io.prometheus.client.exporter.common.TextFormat.write004(TextFormat.java:22)
	at com.databricks.DatabricksMain.encodedMetricsBlob(DatabricksMain.scala:363)
	at com.databricks.DatabricksMain$$anonfun$startPrometheusMetricsExport$1.apply$mcV$sp(DatabricksMain.scala:341)
	at com.databricks.DatabricksMain$$anonfun$startPrometheusMetricsExport$1.apply(DatabricksMain.scala:340)
	at com.databricks.DatabricksMain$$anonfun$startPrometheusMetricsExport$1.apply(DatabricksMain.scala:340)
	at com.databricks.util.UntrustedUtils$.tryLog(UntrustedUtils.scala:98)
	at com.databricks.threading.NamedTimer$$anon$1$$anonfun$run$1.apply(NamedTimer.scala:54)
	at com.databricks.threading.NamedTimer$$anon$1$$anonfun$run$1.apply(NamedTimer.scala:54)
	at com.databricks.logging.UsageLogging$$anonfun$recordOperation$1.apply(UsageLogging.scala:369)
	at com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)
	at com.databricks.threading.NamedTimer$$anon$1.withAttributionContext(NamedTimer.scala:51)
	at com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:271)
	at com.databricks.threading.NamedTimer$$anon$1.withAttributionTags(NamedTimer.scala:51)
	at com.databricks.logging.UsageLogging$class.recordOperation(UsageLogging.scala:350)
	at com.databricks.threading.NamedTimer$$anon$1.recordOperation(NamedTimer.scala:51)
	at com.databricks.threading.NamedTimer$$anon$1.run(NamedTimer.scala:53)
	at java.util.TimerThread.mainLoop(Timer.java:555)
	at java.util.TimerThread.run(Timer.java:505)
19/08/12 22:54:53 ERROR DriverCorral: Failure when launching REPL ReplId-36e31-e98b6-33adf-5
java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SparkContext.<init>(SparkContext.scala:90)
com.databricks.backend.daemon.driver.DatabricksILoop$$anonfun$4.apply(DatabricksILoop.scala:345)
com.databricks.backend.daemon.driver.DatabricksILoop$$anonfun$4.apply(DatabricksILoop.scala:345)
com.databricks.backend.daemon.driver.ClassLoaders$.withContextClassLoader(ClassLoaders.scala:29)
com.databricks.backend.daemon.driver.DatabricksILoop$.initializeSharedDriverContext(DatabricksILoop.scala:344)
com.databricks.backend.daemon.driver.DatabricksILoop$.getOrCreateSharedDriverContext(DatabricksILoop.scala:273)
com.databricks.backend.daemon.driver.DriverCorral.com$databricks$backend$daemon$driver$DriverCorral$$driverContext(DriverCorral.scala:175)
com.databricks.backend.daemon.driver.DriverCorral.<init>(DriverCorral.scala:211)
com.databricks.backend.daemon.driver.DriverDaemon.<init>(DriverDaemon.scala:34)
com.databricks.backend.daemon.driver.DriverDaemon$.create(DriverDaemon.scala:150)
com.databricks.backend.daemon.driver.DriverDaemon$.wrappedMain(DriverDaemon.scala:155)
com.databricks.DatabricksMain$$anonfun$main$1.apply$mcV$sp(DatabricksMain.scala:88)
com.databricks.DatabricksMain$$anonfun$main$1.apply(DatabricksMain.scala:87)
com.databricks.DatabricksMain$$anonfun$main$1.apply(DatabricksMain.scala:87)
com.databricks.DatabricksMain$$anonfun$1.apply(DatabricksMain.scala:245)
com.databricks.logging.UsageLogging$$anonfun$recordOperation$1.apply(UsageLogging.scala:369)
com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)
scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)
com.databricks.DatabricksMain.withAttributionContext(DatabricksMain.scala:57)

The currently active SparkContext was created at:

(No active SparkContext.)
         
	at org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:114)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:114)
	at org.apache.spark.sql.SparkSession.newSession(SparkSession.scala:270)
	at org.apache.spark.sql.hive.HiveContext.newSession(HiveContext.scala:49)
	at org.apache.spark.sql.hive.HiveContext.newSession(HiveContext.scala:32)
	at com.databricks.backend.daemon.driver.DriverSparkHooks.sessionizedDriverHooks(DriverSparkHooks.scala:164)
	at com.databricks.backend.daemon.driver.DriverCorral.com$databricks$backend$daemon$driver$DriverCorral$$createDriverSparkHooks(DriverCorral.scala:286)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$6.apply(DriverCorral.scala:298)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$6.apply(DriverCorral.scala:298)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:79)
	at com.databricks.backend.daemon.driver.DriverCorral.addReplToExecutionContext(DriverCorral.scala:296)
	at com.databricks.backend.daemon.driver.DriverCorral.com$databricks$backend$daemon$driver$DriverCorral$$handleRequest(DriverCorral.scala:390)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:695)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:691)
	at com.databricks.rpc.ServerBackend$$anonfun$internalReceive$1$$anonfun$apply$1.apply(ServerBackend.scala:44)
	at com.databricks.rpc.ServerBackend$$anonfun$internalReceive$1$$anonfun$apply$1.apply(ServerBackend.scala:40)
	at com.databricks.rpc.ServerBackend$$anonfun$com$databricks$rpc$ServerBackend$$commonReceive$1.applyOrElse(ServerBackend.scala:61)
	at com.databricks.rpc.ServerBackend$$anonfun$com$databricks$rpc$ServerBackend$$commonReceive$1.applyOrElse(ServerBackend.scala:61)
	at com.databricks.rpc.ServerBackend$$anonfun$internalReceive$1.apply(ServerBackend.scala:39)
	at com.databricks.logging.UsageLogging$$anonfun$recordOperation$1.apply(UsageLogging.scala:369)
	at com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)
	at com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:13)
	at com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:271)
	at com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:13)
	at com.databricks.logging.UsageLogging$class.recordOperation(UsageLogging.scala:350)
	at com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:13)
	at com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:38)
	at com.databricks.rpc.JettyServer$RequestManager$$anonfun$11.apply(JettyServer.scala:399)
	at scala.util.Try$.apply(Try.scala:192)
	at com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:399)
	at com.databricks.rpc.JettyServer$RequestManager.com$databricks$rpc$JettyServer$RequestManager$$handleRequestAndRespond(JettyServer.scala:302)
	at com.databricks.rpc.JettyServer$RequestManager$$anonfun$handleHttp$1.apply$mcV$sp(JettyServer.scala:190)
	at com.databricks.rpc.JettyServer$RequestManager$$anonfun$handleHttp$1.apply(JettyServer.scala:181)
	at com.databricks.rpc.JettyServer$RequestManager$$anonfun$handleHttp$1.apply(JettyServer.scala:181)
	at com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)
	at com.databricks.rpc.JettyServer$.withAttributionContext(JettyServer.scala:116)
	at com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:271)
	at com.databricks.rpc.JettyServer$.withAttributionTags(JettyServer.scala:116)
	at com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:180)
	at com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:143)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:584)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:514)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)
	at org.eclipse.jetty.server.Server.handle(Server.java:534)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)
	at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)
	at java.lang.Thread.run(Thread.java:748)
19/08/12 22:54:57 ERROR DriverCorral: Failure when launching REPL ReplId-102de-dcc23-966d7-2
java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SparkContext.<init>(SparkContext.scala:90)
com.databricks.backend.daemon.driver.DatabricksILoop$$anonfun$4.apply(DatabricksILoop.scala:345)
com.databricks.backend.daemon.driver.DatabricksILoop$$anonfun$4.apply(DatabricksILoop.scala:345)
com.databricks.backend.daemon.driver.ClassLoaders$.withContextClassLoader(ClassLoaders.scala:29)
com.databricks.backend.daemon.driver.DatabricksILoop$.initializeSharedDriverContext(DatabricksILoop.scala:344)
com.databricks.backend.daemon.driver.DatabricksILoop$.getOrCreateSharedDriverContext(DatabricksILoop.scala:273)
com.databricks.backend.daemon.driver.DriverCorral.com$databricks$backend$daemon$driver$DriverCorral$$driverContext(DriverCorral.scala:175)
com.databricks.backend.daemon.driver.DriverCorral.<init>(DriverCorral.scala:211)
com.databricks.backend.daemon.driver.DriverDaemon.<init>(DriverDaemon.scala:34)
com.databricks.backend.daemon.driver.DriverDaemon$.create(DriverDaemon.scala:150)
com.databricks.backend.daemon.driver.DriverDaemon$.wrappedMain(DriverDaemon.scala:155)
com.databricks.DatabricksMain$$anonfun$main$1.apply$mcV$sp(DatabricksMain.scala:88)
com.databricks.DatabricksMain$$anonfun$main$1.apply(DatabricksMain.scala:87)
com.databricks.DatabricksMain$$anonfun$main$1.apply(DatabricksMain.scala:87)
com.databricks.DatabricksMain$$anonfun$1.apply(DatabricksMain.scala:245)
com.databricks.logging.UsageLogging$$anonfun$recordOperation$1.apply(UsageLogging.scala:369)
com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)
scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)
com.databricks.DatabricksMain.withAttributionContext(DatabricksMain.scala:57)

The currently active SparkContext was created at:

(No active SparkContext.)
         
	at org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:114)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:114)
	at org.apache.spark.sql.SparkSession.newSession(SparkSession.scala:270)
	at org.apache.spark.sql.hive.HiveContext.newSession(HiveContext.scala:49)
	at org.apache.spark.sql.hive.HiveContext.newSession(HiveContext.scala:32)
	at com.databricks.backend.daemon.driver.DriverSparkHooks.sessionizedDriverHooks(DriverSparkHooks.scala:164)
	at com.databricks.backend.daemon.driver.DriverCorral.com$databricks$backend$daemon$driver$DriverCorral$$createDriverSparkHooks(DriverCorral.scala:286)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$6.apply(DriverCorral.scala:298)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$6.apply(DriverCorral.scala:298)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:79)
	at com.databricks.backend.daemon.driver.DriverCorral.addReplToExecutionContext(DriverCorral.scala:296)
	at com.databricks.backend.daemon.driver.DriverCorral.com$databricks$backend$daemon$driver$DriverCorral$$handleRequest(DriverCorral.scala:390)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:695)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:691)
	at com.databricks.rpc.ServerBackend$$anonfun$internalReceive$1$$anonfun$apply$1.apply(ServerBackend.scala:44)
	at com.databricks.rpc.ServerBackend$$anonfun$internalReceive$1$$anonfun$apply$1.apply(ServerBackend.scala:40)
	at com.databricks.rpc.ServerBackend$$anonfun$com$databricks$rpc$ServerBackend$$commonReceive$1.applyOrElse(ServerBackend.scala:61)
	at com.databricks.rpc.ServerBackend$$anonfun$com$databricks$rpc$ServerBackend$$commonReceive$1.applyOrElse(ServerBackend.scala:61)
	at com.databricks.rpc.ServerBackend$$anonfun$internalReceive$1.apply(ServerBackend.scala:39)
	at com.databricks.logging.UsageLogging$$anonfun$recordOperation$1.apply(UsageLogging.scala:369)
	at com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)
	at com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:13)
	at com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:271)
	at com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:13)
	at com.databricks.logging.UsageLogging$class.recordOperation(UsageLogging.scala:350)
	at com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:13)
	at com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:38)
	at com.databricks.rpc.JettyServer$RequestManager$$anonfun$11.apply(JettyServer.scala:399)
	at scala.util.Try$.apply(Try.scala:192)
	at com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:399)
	at com.databricks.rpc.JettyServer$RequestManager.com$databricks$rpc$JettyServer$RequestManager$$handleRequestAndRespond(JettyServer.scala:302)
	at com.databricks.rpc.JettyServer$RequestManager$$anonfun$handleHttp$1.apply$mcV$sp(JettyServer.scala:190)
	at com.databricks.rpc.JettyServer$RequestManager$$anonfun$handleHttp$1.apply(JettyServer.scala:181)
	at com.databricks.rpc.JettyServer$RequestManager$$anonfun$handleHttp$1.apply(JettyServer.scala:181)
	at com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)
	at com.databricks.rpc.JettyServer$.withAttributionContext(JettyServer.scala:116)
	at com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:271)
	at com.databricks.rpc.JettyServer$.withAttributionTags(JettyServer.scala:116)
	at com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:180)
	at com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:143)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:584)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:514)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)
	at org.eclipse.jetty.server.Server.handle(Server.java:534)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)
	at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)
	at java.lang.Thread.run(Thread.java:748)
19/08/12 22:55:09 ERROR DriverCorral: Failure when launching REPL ReplId-4a0bf-5bbe0-dbc19-0
java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SparkContext.<init>(SparkContext.scala:90)
com.databricks.backend.daemon.driver.DatabricksILoop$$anonfun$4.apply(DatabricksILoop.scala:345)
com.databricks.backend.daemon.driver.DatabricksILoop$$anonfun$4.apply(DatabricksILoop.scala:345)
com.databricks.backend.daemon.driver.ClassLoaders$.withContextClassLoader(ClassLoaders.scala:29)
com.databricks.backend.daemon.driver.DatabricksILoop$.initializeSharedDriverContext(DatabricksILoop.scala:344)
com.databricks.backend.daemon.driver.DatabricksILoop$.getOrCreateSharedDriverContext(DatabricksILoop.scala:273)
com.databricks.backend.daemon.driver.DriverCorral.com$databricks$backend$daemon$driver$DriverCorral$$driverContext(DriverCorral.scala:175)
com.databricks.backend.daemon.driver.DriverCorral.<init>(DriverCorral.scala:211)
com.databricks.backend.daemon.driver.DriverDaemon.<init>(DriverDaemon.scala:34)
com.databricks.backend.daemon.driver.DriverDaemon$.create(DriverDaemon.scala:150)
com.databricks.backend.daemon.driver.DriverDaemon$.wrappedMain(DriverDaemon.scala:155)
com.databricks.DatabricksMain$$anonfun$main$1.apply$mcV$sp(DatabricksMain.scala:88)
com.databricks.DatabricksMain$$anonfun$main$1.apply(DatabricksMain.scala:87)
com.databricks.DatabricksMain$$anonfun$main$1.apply(DatabricksMain.scala:87)
com.databricks.DatabricksMain$$anonfun$1.apply(DatabricksMain.scala:245)
com.databricks.logging.UsageLogging$$anonfun$recordOperation$1.apply(UsageLogging.scala:369)
com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)
scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)
com.databricks.DatabricksMain.withAttributionContext(DatabricksMain.scala:57)

The currently active SparkContext was created at:

(No active SparkContext.)
         
	at org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:114)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:114)
	at org.apache.spark.sql.SparkSession.newSession(SparkSession.scala:270)
	at org.apache.spark.sql.hive.HiveContext.newSession(HiveContext.scala:49)
	at org.apache.spark.sql.hive.HiveContext.newSession(HiveContext.scala:32)
	at com.databricks.backend.daemon.driver.DriverSparkHooks.sessionizedDriverHooks(DriverSparkHooks.scala:164)
	at com.databricks.backend.daemon.driver.DriverCorral.com$databricks$backend$daemon$driver$DriverCorral$$createDriverSparkHooks(DriverCorral.scala:286)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$6.apply(DriverCorral.scala:298)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$6.apply(DriverCorral.scala:298)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:79)
	at com.databricks.backend.daemon.driver.DriverCorral.addReplToExecutionContext(DriverCorral.scala:296)
	at com.databricks.backend.daemon.driver.DriverCorral.com$databricks$backend$daemon$driver$DriverCorral$$handleRequest(DriverCorral.scala:390)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:695)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:691)
	at com.databricks.rpc.ServerBackend$$anonfun$internalReceive$1$$anonfun$apply$1.apply(ServerBackend.scala:44)
	at com.databricks.rpc.ServerBackend$$anonfun$internalReceive$1$$anonfun$apply$1.apply(ServerBackend.scala:40)
	at com.databricks.rpc.ServerBackend$$anonfun$com$databricks$rpc$ServerBackend$$commonReceive$1.applyOrElse(ServerBackend.scala:61)
	at com.databricks.rpc.ServerBackend$$anonfun$com$databricks$rpc$ServerBackend$$commonReceive$1.applyOrElse(ServerBackend.scala:61)
	at com.databricks.rpc.ServerBackend$$anonfun$internalReceive$1.apply(ServerBackend.scala:39)
	at com.databricks.logging.UsageLogging$$anonfun$recordOperation$1.apply(UsageLogging.scala:369)
	at com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)
	at com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:13)
	at com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:271)
	at com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:13)
	at com.databricks.logging.UsageLogging$class.recordOperation(UsageLogging.scala:350)
	at com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:13)
	at com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:38)
	at com.databricks.rpc.JettyServer$RequestManager$$anonfun$11.apply(JettyServer.scala:399)
	at scala.util.Try$.apply(Try.scala:192)
	at com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:399)
	at com.databricks.rpc.JettyServer$RequestManager.com$databricks$rpc$JettyServer$RequestManager$$handleRequestAndRespond(JettyServer.scala:302)
	at com.databricks.rpc.JettyServer$RequestManager$$anonfun$handleHttp$1.apply$mcV$sp(JettyServer.scala:190)
	at com.databricks.rpc.JettyServer$RequestManager$$anonfun$handleHttp$1.apply(JettyServer.scala:181)
	at com.databricks.rpc.JettyServer$RequestManager$$anonfun$handleHttp$1.apply(JettyServer.scala:181)
	at com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)
	at com.databricks.rpc.JettyServer$.withAttributionContext(JettyServer.scala:116)
	at com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:271)
	at com.databricks.rpc.JettyServer$.withAttributionTags(JettyServer.scala:116)
	at com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:180)
	at com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:143)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:584)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:514)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)
	at org.eclipse.jetty.server.Server.handle(Server.java:534)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)
	at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)
	at java.lang.Thread.run(Thread.java:748)
19/08/12 22:55:13 ERROR DriverCorral: Failure when launching REPL ReplId-684d8-72ae2-6dcf0-8
java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SparkContext.<init>(SparkContext.scala:90)
com.databricks.backend.daemon.driver.DatabricksILoop$$anonfun$4.apply(DatabricksILoop.scala:345)
com.databricks.backend.daemon.driver.DatabricksILoop$$anonfun$4.apply(DatabricksILoop.scala:345)
com.databricks.backend.daemon.driver.ClassLoaders$.withContextClassLoader(ClassLoaders.scala:29)
com.databricks.backend.daemon.driver.DatabricksILoop$.initializeSharedDriverContext(DatabricksILoop.scala:344)
com.databricks.backend.daemon.driver.DatabricksILoop$.getOrCreateSharedDriverContext(DatabricksILoop.scala:273)
com.databricks.backend.daemon.driver.DriverCorral.com$databricks$backend$daemon$driver$DriverCorral$$driverContext(DriverCorral.scala:175)
com.databricks.backend.daemon.driver.DriverCorral.<init>(DriverCorral.scala:211)
com.databricks.backend.daemon.driver.DriverDaemon.<init>(DriverDaemon.scala:34)
com.databricks.backend.daemon.driver.DriverDaemon$.create(DriverDaemon.scala:150)
com.databricks.backend.daemon.driver.DriverDaemon$.wrappedMain(DriverDaemon.scala:155)
com.databricks.DatabricksMain$$anonfun$main$1.apply$mcV$sp(DatabricksMain.scala:88)
com.databricks.DatabricksMain$$anonfun$main$1.apply(DatabricksMain.scala:87)
com.databricks.DatabricksMain$$anonfun$main$1.apply(DatabricksMain.scala:87)
com.databricks.DatabricksMain$$anonfun$1.apply(DatabricksMain.scala:245)
com.databricks.logging.UsageLogging$$anonfun$recordOperation$1.apply(UsageLogging.scala:369)
com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)
scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)
com.databricks.DatabricksMain.withAttributionContext(DatabricksMain.scala:57)

The currently active SparkContext was created at:

(No active SparkContext.)
         
	at org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:114)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:114)
	at org.apache.spark.sql.SparkSession.newSession(SparkSession.scala:270)
	at org.apache.spark.sql.hive.HiveContext.newSession(HiveContext.scala:49)
	at org.apache.spark.sql.hive.HiveContext.newSession(HiveContext.scala:32)
	at com.databricks.backend.daemon.driver.DriverSparkHooks.sessionizedDriverHooks(DriverSparkHooks.scala:164)
	at com.databricks.backend.daemon.driver.DriverCorral.com$databricks$backend$daemon$driver$DriverCorral$$createDriverSparkHooks(DriverCorral.scala:286)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$6.apply(DriverCorral.scala:298)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$6.apply(DriverCorral.scala:298)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:79)
	at com.databricks.backend.daemon.driver.DriverCorral.addReplToExecutionContext(DriverCorral.scala:296)
	at com.databricks.backend.daemon.driver.DriverCorral.com$databricks$backend$daemon$driver$DriverCorral$$handleRequest(DriverCorral.scala:390)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:695)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:691)
	at com.databricks.rpc.ServerBackend$$anonfun$internalReceive$1$$anonfun$apply$1.apply(ServerBackend.scala:44)
	at com.databricks.rpc.ServerBackend$$anonfun$internalReceive$1$$anonfun$apply$1.apply(ServerBackend.scala:40)
	at com.databricks.rpc.ServerBackend$$anonfun$com$databricks$rpc$ServerBackend$$commonReceive$1.applyOrElse(ServerBackend.scala:61)
	at com.databricks.rpc.ServerBackend$$anonfun$com$databricks$rpc$ServerBackend$$commonReceive$1.applyOrElse(ServerBackend.scala:61)
	at com.databricks.rpc.ServerBackend$$anonfun$internalReceive$1.apply(ServerBackend.scala:39)
	at com.databricks.logging.UsageLogging$$anonfun$recordOperation$1.apply(UsageLogging.scala:369)
	at com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)
	at com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:13)
	at com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:271)
	at com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:13)
	at com.databricks.logging.UsageLogging$class.recordOperation(UsageLogging.scala:350)
	at com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:13)
	at com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:38)
	at com.databricks.rpc.JettyServer$RequestManager$$anonfun$11.apply(JettyServer.scala:399)
	at scala.util.Try$.apply(Try.scala:192)
	at com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:399)
	at com.databricks.rpc.JettyServer$RequestManager.com$databricks$rpc$JettyServer$RequestManager$$handleRequestAndRespond(JettyServer.scala:302)
	at com.databricks.rpc.JettyServer$RequestManager$$anonfun$handleHttp$1.apply$mcV$sp(JettyServer.scala:190)
	at com.databricks.rpc.JettyServer$RequestManager$$anonfun$handleHttp$1.apply(JettyServer.scala:181)
	at com.databricks.rpc.JettyServer$RequestManager$$anonfun$handleHttp$1.apply(JettyServer.scala:181)
	at com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)
	at com.databricks.rpc.JettyServer$.withAttributionContext(JettyServer.scala:116)
	at com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:271)
	at com.databricks.rpc.JettyServer$.withAttributionTags(JettyServer.scala:116)
	at com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:180)
	at com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:143)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:584)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:514)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)
	at org.eclipse.jetty.server.Server.handle(Server.java:534)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)
	at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)
	at java.lang.Thread.run(Thread.java:748)
19/08/12 22:55:52 WARN UntrustedUtils$: Uncaught exception in thread PrometheusMetricsExporter
java.lang.NullPointerException
	at org.apache.spark.storage.BlockManagerMaster.getStorageStatus(BlockManagerMaster.scala:169)
	at org.apache.spark.storage.BlockManagerSource$$anon$2.get(BlockManagerSource.scala:41)
	at org.apache.spark.storage.BlockManagerSource$$anon$2.get(BlockManagerSource.scala:39)
	at com.google.common.base.Suppliers$ExpiringMemoizingSupplier.get(Suppliers.java:192)
	at org.apache.spark.storage.BlockManagerSource$$anonfun$1$$anon$1.getValue(BlockManagerSource.scala:72)
	at org.apache.spark.storage.BlockManagerSource$$anonfun$1$$anon$1.getValue(BlockManagerSource.scala:71)
	at com.databricks.backend.daemon.driver.SparkMetricsExporter$$anonfun$fromGauge$1.apply(SparkMetricsExporter.scala:131)
	at com.databricks.backend.daemon.driver.SparkMetricsExporter$$anonfun$fromGauge$1.apply(SparkMetricsExporter.scala:129)
	at scala.Option.map(Option.scala:146)
	at com.databricks.backend.daemon.driver.SparkMetricsExporter$.fromGauge(SparkMetricsExporter.scala:129)
	at com.databricks.backend.daemon.driver.SparkMetricsExporter$$anonfun$collect$1.apply(SparkMetricsExporter.scala:51)
	at com.databricks.backend.daemon.driver.SparkMetricsExporter$$anonfun$collect$1.apply(SparkMetricsExporter.scala:47)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:104)
	at com.databricks.backend.daemon.driver.SparkMetricsExporter.collect(SparkMetricsExporter.scala:47)
	at io.prometheus.client.CollectorRegistry$MetricFamilySamplesEnumeration.findNextElement(CollectorRegistry.java:199)
	at io.prometheus.client.CollectorRegistry$MetricFamilySamplesEnumeration.nextElement(CollectorRegistry.java:232)
	at io.prometheus.client.CollectorRegistry$MetricFamilySamplesEnumeration.nextElement(CollectorRegistry.java:153)
	at io.prometheus.client.exporter.common.TextFormat.write004(TextFormat.java:22)
	at com.databricks.DatabricksMain.encodedMetricsBlob(DatabricksMain.scala:363)
	at com.databricks.DatabricksMain$$anonfun$startPrometheusMetricsExport$1.apply$mcV$sp(DatabricksMain.scala:341)
	at com.databricks.DatabricksMain$$anonfun$startPrometheusMetricsExport$1.apply(DatabricksMain.scala:340)
	at com.databricks.DatabricksMain$$anonfun$startPrometheusMetricsExport$1.apply(DatabricksMain.scala:340)
	at com.databricks.util.UntrustedUtils$.tryLog(UntrustedUtils.scala:98)
	at com.databricks.threading.NamedTimer$$anon$1$$anonfun$run$1.apply(NamedTimer.scala:54)
	at com.databricks.threading.NamedTimer$$anon$1$$anonfun$run$1.apply(NamedTimer.scala:54)
	at com.databricks.logging.UsageLogging$$anonfun$recordOperation$1.apply(UsageLogging.scala:369)
	at com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)
	at com.databricks.threading.NamedTimer$$anon$1.withAttributionContext(NamedTimer.scala:51)
	at com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:271)
	at com.databricks.threading.NamedTimer$$anon$1.withAttributionTags(NamedTimer.scala:51)
	at com.databricks.logging.UsageLogging$class.recordOperation(UsageLogging.scala:350)
	at com.databricks.threading.NamedTimer$$anon$1.recordOperation(NamedTimer.scala:51)
	at com.databricks.threading.NamedTimer$$anon$1.run(NamedTimer.scala:53)
	at java.util.TimerThread.mainLoop(Timer.java:555)
	at java.util.TimerThread.run(Timer.java:505)
19/08/12 22:55:52 INFO DriverCorral: DBFS health check ok
19/08/12 22:55:53 INFO HikariDataSource: metastore-monitor - Starting...
19/08/12 22:55:53 INFO HikariDataSource: metastore-monitor - Start completed.
19/08/12 22:55:53 INFO HikariDataSource: metastore-monitor - Shutdown initiated...
19/08/12 22:55:53 INFO HikariDataSource: metastore-monitor - Shutdown completed.
19/08/12 22:55:53 INFO MetastoreMonitor: Metastore healthcheck successful (connection duration = 35 milliseconds)
19/08/12 22:56:05 INFO HiveMetaStore: 1: get_database: default
19/08/12 22:56:05 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_database: default	
19/08/12 22:56:05 INFO HiveMetaStore: 1: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
19/08/12 22:56:05 INFO ObjectStore: ObjectStore, initialize called
19/08/12 22:56:05 INFO Query: Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing
19/08/12 22:56:05 INFO ObjectStore: Initialized ObjectStore
19/08/12 22:56:05 INFO DriverCorral: Metastore health check ok
19/08/12 22:56:43 ERROR DriverCorral: Failure when launching REPL ReplId-15ece-3266c-e650c-d
java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SparkContext.<init>(SparkContext.scala:90)
com.databricks.backend.daemon.driver.DatabricksILoop$$anonfun$4.apply(DatabricksILoop.scala:345)
com.databricks.backend.daemon.driver.DatabricksILoop$$anonfun$4.apply(DatabricksILoop.scala:345)
com.databricks.backend.daemon.driver.ClassLoaders$.withContextClassLoader(ClassLoaders.scala:29)
com.databricks.backend.daemon.driver.DatabricksILoop$.initializeSharedDriverContext(DatabricksILoop.scala:344)
com.databricks.backend.daemon.driver.DatabricksILoop$.getOrCreateSharedDriverContext(DatabricksILoop.scala:273)
com.databricks.backend.daemon.driver.DriverCorral.com$databricks$backend$daemon$driver$DriverCorral$$driverContext(DriverCorral.scala:175)
com.databricks.backend.daemon.driver.DriverCorral.<init>(DriverCorral.scala:211)
com.databricks.backend.daemon.driver.DriverDaemon.<init>(DriverDaemon.scala:34)
com.databricks.backend.daemon.driver.DriverDaemon$.create(DriverDaemon.scala:150)
com.databricks.backend.daemon.driver.DriverDaemon$.wrappedMain(DriverDaemon.scala:155)
com.databricks.DatabricksMain$$anonfun$main$1.apply$mcV$sp(DatabricksMain.scala:88)
com.databricks.DatabricksMain$$anonfun$main$1.apply(DatabricksMain.scala:87)
com.databricks.DatabricksMain$$anonfun$main$1.apply(DatabricksMain.scala:87)
com.databricks.DatabricksMain$$anonfun$1.apply(DatabricksMain.scala:245)
com.databricks.logging.UsageLogging$$anonfun$recordOperation$1.apply(UsageLogging.scala:369)
com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)
scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)
com.databricks.DatabricksMain.withAttributionContext(DatabricksMain.scala:57)

The currently active SparkContext was created at:

(No active SparkContext.)
         
	at org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:114)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:114)
	at org.apache.spark.sql.SparkSession.newSession(SparkSession.scala:270)
	at org.apache.spark.sql.hive.HiveContext.newSession(HiveContext.scala:49)
	at org.apache.spark.sql.hive.HiveContext.newSession(HiveContext.scala:32)
	at com.databricks.backend.daemon.driver.DriverSparkHooks.sessionizedDriverHooks(DriverSparkHooks.scala:164)
	at com.databricks.backend.daemon.driver.DriverCorral.com$databricks$backend$daemon$driver$DriverCorral$$createDriverSparkHooks(DriverCorral.scala:286)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$6.apply(DriverCorral.scala:298)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$6.apply(DriverCorral.scala:298)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:79)
	at com.databricks.backend.daemon.driver.DriverCorral.addReplToExecutionContext(DriverCorral.scala:296)
	at com.databricks.backend.daemon.driver.DriverCorral.com$databricks$backend$daemon$driver$DriverCorral$$handleRequest(DriverCorral.scala:390)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:695)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:691)
	at com.databricks.rpc.ServerBackend$$anonfun$internalReceive$1$$anonfun$apply$1.apply(ServerBackend.scala:44)
	at com.databricks.rpc.ServerBackend$$anonfun$internalReceive$1$$anonfun$apply$1.apply(ServerBackend.scala:40)
	at com.databricks.rpc.ServerBackend$$anonfun$com$databricks$rpc$ServerBackend$$commonReceive$1.applyOrElse(ServerBackend.scala:61)
	at com.databricks.rpc.ServerBackend$$anonfun$com$databricks$rpc$ServerBackend$$commonReceive$1.applyOrElse(ServerBackend.scala:61)
	at com.databricks.rpc.ServerBackend$$anonfun$internalReceive$1.apply(ServerBackend.scala:39)
	at com.databricks.logging.UsageLogging$$anonfun$recordOperation$1.apply(UsageLogging.scala:369)
	at com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)
	at com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:13)
	at com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:271)
	at com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:13)
	at com.databricks.logging.UsageLogging$class.recordOperation(UsageLogging.scala:350)
	at com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:13)
	at com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:38)
	at com.databricks.rpc.JettyServer$RequestManager$$anonfun$11.apply(JettyServer.scala:399)
	at scala.util.Try$.apply(Try.scala:192)
	at com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:399)
	at com.databricks.rpc.JettyServer$RequestManager.com$databricks$rpc$JettyServer$RequestManager$$handleRequestAndRespond(JettyServer.scala:302)
	at com.databricks.rpc.JettyServer$RequestManager$$anonfun$handleHttp$1.apply$mcV$sp(JettyServer.scala:190)
	at com.databricks.rpc.JettyServer$RequestManager$$anonfun$handleHttp$1.apply(JettyServer.scala:181)
	at com.databricks.rpc.JettyServer$RequestManager$$anonfun$handleHttp$1.apply(JettyServer.scala:181)
	at com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)
	at com.databricks.rpc.JettyServer$.withAttributionContext(JettyServer.scala:116)
	at com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:271)
	at com.databricks.rpc.JettyServer$.withAttributionTags(JettyServer.scala:116)
	at com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:180)
	at com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:143)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:584)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:514)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)
	at org.eclipse.jetty.server.Server.handle(Server.java:534)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)
	at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)
	at java.lang.Thread.run(Thread.java:748)
19/08/12 22:56:49 ERROR DriverCorral: Failure when launching REPL ReplId-7bd8d-1f4fa-4ba13-5
java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SparkContext.<init>(SparkContext.scala:90)
com.databricks.backend.daemon.driver.DatabricksILoop$$anonfun$4.apply(DatabricksILoop.scala:345)
com.databricks.backend.daemon.driver.DatabricksILoop$$anonfun$4.apply(DatabricksILoop.scala:345)
com.databricks.backend.daemon.driver.ClassLoaders$.withContextClassLoader(ClassLoaders.scala:29)
com.databricks.backend.daemon.driver.DatabricksILoop$.initializeSharedDriverContext(DatabricksILoop.scala:344)
com.databricks.backend.daemon.driver.DatabricksILoop$.getOrCreateSharedDriverContext(DatabricksILoop.scala:273)
com.databricks.backend.daemon.driver.DriverCorral.com$databricks$backend$daemon$driver$DriverCorral$$driverContext(DriverCorral.scala:175)
com.databricks.backend.daemon.driver.DriverCorral.<init>(DriverCorral.scala:211)
com.databricks.backend.daemon.driver.DriverDaemon.<init>(DriverDaemon.scala:34)
com.databricks.backend.daemon.driver.DriverDaemon$.create(DriverDaemon.scala:150)
com.databricks.backend.daemon.driver.DriverDaemon$.wrappedMain(DriverDaemon.scala:155)
com.databricks.DatabricksMain$$anonfun$main$1.apply$mcV$sp(DatabricksMain.scala:88)
com.databricks.DatabricksMain$$anonfun$main$1.apply(DatabricksMain.scala:87)
com.databricks.DatabricksMain$$anonfun$main$1.apply(DatabricksMain.scala:87)
com.databricks.DatabricksMain$$anonfun$1.apply(DatabricksMain.scala:245)
com.databricks.logging.UsageLogging$$anonfun$recordOperation$1.apply(UsageLogging.scala:369)
com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)
scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)
com.databricks.DatabricksMain.withAttributionContext(DatabricksMain.scala:57)

The currently active SparkContext was created at:

(No active SparkContext.)
         
	at org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:114)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:114)
	at org.apache.spark.sql.SparkSession.newSession(SparkSession.scala:270)
	at org.apache.spark.sql.hive.HiveContext.newSession(HiveContext.scala:49)
	at org.apache.spark.sql.hive.HiveContext.newSession(HiveContext.scala:32)
	at com.databricks.backend.daemon.driver.DriverSparkHooks.sessionizedDriverHooks(DriverSparkHooks.scala:164)
	at com.databricks.backend.daemon.driver.DriverCorral.com$databricks$backend$daemon$driver$DriverCorral$$createDriverSparkHooks(DriverCorral.scala:286)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$6.apply(DriverCorral.scala:298)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$6.apply(DriverCorral.scala:298)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:79)
	at com.databricks.backend.daemon.driver.DriverCorral.addReplToExecutionContext(DriverCorral.scala:296)
	at com.databricks.backend.daemon.driver.DriverCorral.com$databricks$backend$daemon$driver$DriverCorral$$handleRequest(DriverCorral.scala:390)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:695)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:691)
	at com.databricks.rpc.ServerBackend$$anonfun$internalReceive$1$$anonfun$apply$1.apply(ServerBackend.scala:44)
	at com.databricks.rpc.ServerBackend$$anonfun$internalReceive$1$$anonfun$apply$1.apply(ServerBackend.scala:40)
	at com.databricks.rpc.ServerBackend$$anonfun$com$databricks$rpc$ServerBackend$$commonReceive$1.applyOrElse(ServerBackend.scala:61)
	at com.databricks.rpc.ServerBackend$$anonfun$com$databricks$rpc$ServerBackend$$commonReceive$1.applyOrElse(ServerBackend.scala:61)
	at com.databricks.rpc.ServerBackend$$anonfun$internalReceive$1.apply(ServerBackend.scala:39)
	at com.databricks.logging.UsageLogging$$anonfun$recordOperation$1.apply(UsageLogging.scala:369)
	at com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)
	at com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:13)
	at com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:271)
	at com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:13)
	at com.databricks.logging.UsageLogging$class.recordOperation(UsageLogging.scala:350)
	at com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:13)
	at com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:38)
	at com.databricks.rpc.JettyServer$RequestManager$$anonfun$11.apply(JettyServer.scala:399)
	at scala.util.Try$.apply(Try.scala:192)
	at com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:399)
	at com.databricks.rpc.JettyServer$RequestManager.com$databricks$rpc$JettyServer$RequestManager$$handleRequestAndRespond(JettyServer.scala:302)
	at com.databricks.rpc.JettyServer$RequestManager$$anonfun$handleHttp$1.apply$mcV$sp(JettyServer.scala:190)
	at com.databricks.rpc.JettyServer$RequestManager$$anonfun$handleHttp$1.apply(JettyServer.scala:181)
	at com.databricks.rpc.JettyServer$RequestManager$$anonfun$handleHttp$1.apply(JettyServer.scala:181)
	at com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)
	at com.databricks.rpc.JettyServer$.withAttributionContext(JettyServer.scala:116)
	at com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:271)
	at com.databricks.rpc.JettyServer$.withAttributionTags(JettyServer.scala:116)
	at com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:180)
	at com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:143)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:584)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:514)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)
	at org.eclipse.jetty.server.Server.handle(Server.java:534)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)
	at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)
	at java.lang.Thread.run(Thread.java:748)
19/08/12 22:56:52 WARN UntrustedUtils$: Uncaught exception in thread PrometheusMetricsExporter
java.lang.NullPointerException
	at org.apache.spark.storage.BlockManagerMaster.getStorageStatus(BlockManagerMaster.scala:169)
	at org.apache.spark.storage.BlockManagerSource$$anon$2.get(BlockManagerSource.scala:41)
	at org.apache.spark.storage.BlockManagerSource$$anon$2.get(BlockManagerSource.scala:39)
	at com.google.common.base.Suppliers$ExpiringMemoizingSupplier.get(Suppliers.java:192)
	at org.apache.spark.storage.BlockManagerSource$$anonfun$1$$anon$1.getValue(BlockManagerSource.scala:72)
	at org.apache.spark.storage.BlockManagerSource$$anonfun$1$$anon$1.getValue(BlockManagerSource.scala:71)
	at com.databricks.backend.daemon.driver.SparkMetricsExporter$$anonfun$fromGauge$1.apply(SparkMetricsExporter.scala:131)
	at com.databricks.backend.daemon.driver.SparkMetricsExporter$$anonfun$fromGauge$1.apply(SparkMetricsExporter.scala:129)
	at scala.Option.map(Option.scala:146)
	at com.databricks.backend.daemon.driver.SparkMetricsExporter$.fromGauge(SparkMetricsExporter.scala:129)
	at com.databricks.backend.daemon.driver.SparkMetricsExporter$$anonfun$collect$1.apply(SparkMetricsExporter.scala:51)
	at com.databricks.backend.daemon.driver.SparkMetricsExporter$$anonfun$collect$1.apply(SparkMetricsExporter.scala:47)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:104)
	at com.databricks.backend.daemon.driver.SparkMetricsExporter.collect(SparkMetricsExporter.scala:47)
	at io.prometheus.client.CollectorRegistry$MetricFamilySamplesEnumeration.findNextElement(CollectorRegistry.java:199)
	at io.prometheus.client.CollectorRegistry$MetricFamilySamplesEnumeration.nextElement(CollectorRegistry.java:232)
	at io.prometheus.client.CollectorRegistry$MetricFamilySamplesEnumeration.nextElement(CollectorRegistry.java:153)
	at io.prometheus.client.exporter.common.TextFormat.write004(TextFormat.java:22)
	at com.databricks.DatabricksMain.encodedMetricsBlob(DatabricksMain.scala:363)
	at com.databricks.DatabricksMain$$anonfun$startPrometheusMetricsExport$1.apply$mcV$sp(DatabricksMain.scala:341)
	at com.databricks.DatabricksMain$$anonfun$startPrometheusMetricsExport$1.apply(DatabricksMain.scala:340)
	at com.databricks.DatabricksMain$$anonfun$startPrometheusMetricsExport$1.apply(DatabricksMain.scala:340)
	at com.databricks.util.UntrustedUtils$.tryLog(UntrustedUtils.scala:98)
	at com.databricks.threading.NamedTimer$$anon$1$$anonfun$run$1.apply(NamedTimer.scala:54)
	at com.databricks.threading.NamedTimer$$anon$1$$anonfun$run$1.apply(NamedTimer.scala:54)
	at com.databricks.logging.UsageLogging$$anonfun$recordOperation$1.apply(UsageLogging.scala:369)
	at com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)
	at com.databricks.threading.NamedTimer$$anon$1.withAttributionContext(NamedTimer.scala:51)
	at com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:271)
	at com.databricks.threading.NamedTimer$$anon$1.withAttributionTags(NamedTimer.scala:51)
	at com.databricks.logging.UsageLogging$class.recordOperation(UsageLogging.scala:350)
	at com.databricks.threading.NamedTimer$$anon$1.recordOperation(NamedTimer.scala:51)
	at com.databricks.threading.NamedTimer$$anon$1.run(NamedTimer.scala:53)
	at java.util.TimerThread.mainLoop(Timer.java:555)
	at java.util.TimerThread.run(Timer.java:505)
19/08/12 22:56:52 ERROR DriverCorral: Failure when launching REPL ReplId-7eb58-d6802-af865-6
java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.SparkContext.<init>(SparkContext.scala:90)
com.databricks.backend.daemon.driver.DatabricksILoop$$anonfun$4.apply(DatabricksILoop.scala:345)
com.databricks.backend.daemon.driver.DatabricksILoop$$anonfun$4.apply(DatabricksILoop.scala:345)
com.databricks.backend.daemon.driver.ClassLoaders$.withContextClassLoader(ClassLoaders.scala:29)
com.databricks.backend.daemon.driver.DatabricksILoop$.initializeSharedDriverContext(DatabricksILoop.scala:344)
com.databricks.backend.daemon.driver.DatabricksILoop$.getOrCreateSharedDriverContext(DatabricksILoop.scala:273)
com.databricks.backend.daemon.driver.DriverCorral.com$databricks$backend$daemon$driver$DriverCorral$$driverContext(DriverCorral.scala:175)
com.databricks.backend.daemon.driver.DriverCorral.<init>(DriverCorral.scala:211)
com.databricks.backend.daemon.driver.DriverDaemon.<init>(DriverDaemon.scala:34)
com.databricks.backend.daemon.driver.DriverDaemon$.create(DriverDaemon.scala:150)
com.databricks.backend.daemon.driver.DriverDaemon$.wrappedMain(DriverDaemon.scala:155)
com.databricks.DatabricksMain$$anonfun$main$1.apply$mcV$sp(DatabricksMain.scala:88)
com.databricks.DatabricksMain$$anonfun$main$1.apply(DatabricksMain.scala:87)
com.databricks.DatabricksMain$$anonfun$main$1.apply(DatabricksMain.scala:87)
com.databricks.DatabricksMain$$anonfun$1.apply(DatabricksMain.scala:245)
com.databricks.logging.UsageLogging$$anonfun$recordOperation$1.apply(UsageLogging.scala:369)
com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)
scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)
com.databricks.DatabricksMain.withAttributionContext(DatabricksMain.scala:57)

The currently active SparkContext was created at:

(No active SparkContext.)
         
	at org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:114)
	at org.apache.spark.sql.SparkSession.<init>(SparkSession.scala:114)
	at org.apache.spark.sql.SparkSession.newSession(SparkSession.scala:270)
	at org.apache.spark.sql.hive.HiveContext.newSession(HiveContext.scala:49)
	at org.apache.spark.sql.hive.HiveContext.newSession(HiveContext.scala:32)
	at com.databricks.backend.daemon.driver.DriverSparkHooks.sessionizedDriverHooks(DriverSparkHooks.scala:164)
	at com.databricks.backend.daemon.driver.DriverCorral.com$databricks$backend$daemon$driver$DriverCorral$$createDriverSparkHooks(DriverCorral.scala:286)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$6.apply(DriverCorral.scala:298)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$6.apply(DriverCorral.scala:298)
	at scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:79)
	at com.databricks.backend.daemon.driver.DriverCorral.addReplToExecutionContext(DriverCorral.scala:296)
	at com.databricks.backend.daemon.driver.DriverCorral.com$databricks$backend$daemon$driver$DriverCorral$$handleRequest(DriverCorral.scala:390)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:695)
	at com.databricks.backend.daemon.driver.DriverCorral$$anonfun$receive$1.applyOrElse(DriverCorral.scala:691)
	at com.databricks.rpc.ServerBackend$$anonfun$internalReceive$1$$anonfun$apply$1.apply(ServerBackend.scala:44)
	at com.databricks.rpc.ServerBackend$$anonfun$internalReceive$1$$anonfun$apply$1.apply(ServerBackend.scala:40)
	at com.databricks.rpc.ServerBackend$$anonfun$com$databricks$rpc$ServerBackend$$commonReceive$1.applyOrElse(ServerBackend.scala:61)
	at com.databricks.rpc.ServerBackend$$anonfun$com$databricks$rpc$ServerBackend$$commonReceive$1.applyOrElse(ServerBackend.scala:61)
	at com.databricks.rpc.ServerBackend$$anonfun$internalReceive$1.apply(ServerBackend.scala:39)
	at com.databricks.logging.UsageLogging$$anonfun$recordOperation$1.apply(UsageLogging.scala:369)
	at com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)
	at com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:13)
	at com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:271)
	at com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:13)
	at com.databricks.logging.UsageLogging$class.recordOperation(UsageLogging.scala:350)
	at com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:13)
	at com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:38)
	at com.databricks.rpc.JettyServer$RequestManager$$anonfun$11.apply(JettyServer.scala:399)
	at scala.util.Try$.apply(Try.scala:192)
	at com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:399)
	at com.databricks.rpc.JettyServer$RequestManager.com$databricks$rpc$JettyServer$RequestManager$$handleRequestAndRespond(JettyServer.scala:302)
	at com.databricks.rpc.JettyServer$RequestManager$$anonfun$handleHttp$1.apply$mcV$sp(JettyServer.scala:190)
	at com.databricks.rpc.JettyServer$RequestManager$$anonfun$handleHttp$1.apply(JettyServer.scala:181)
	at com.databricks.rpc.JettyServer$RequestManager$$anonfun$handleHttp$1.apply(JettyServer.scala:181)
	at com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)
	at com.databricks.rpc.JettyServer$.withAttributionContext(JettyServer.scala:116)
	at com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:271)
	at com.databricks.rpc.JettyServer$.withAttributionTags(JettyServer.scala:116)
	at com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:180)
	at com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:143)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:584)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:514)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)
	at org.eclipse.jetty.server.Server.handle(Server.java:534)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108)
	at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)
	at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)
	at java.lang.Thread.run(Thread.java:748)
19/08/12 22:57:52 WARN UntrustedUtils$: Uncaught exception in thread PrometheusMetricsExporter
java.lang.NullPointerException
	at org.apache.spark.storage.BlockManagerMaster.getStorageStatus(BlockManagerMaster.scala:169)
	at org.apache.spark.storage.BlockManagerSource$$anon$2.get(BlockManagerSource.scala:41)
	at org.apache.spark.storage.BlockManagerSource$$anon$2.get(BlockManagerSource.scala:39)
	at com.google.common.base.Suppliers$ExpiringMemoizingSupplier.get(Suppliers.java:192)
	at org.apache.spark.storage.BlockManagerSource$$anonfun$1$$anon$1.getValue(BlockManagerSource.scala:72)
	at org.apache.spark.storage.BlockManagerSource$$anonfun$1$$anon$1.getValue(BlockManagerSource.scala:71)
	at com.databricks.backend.daemon.driver.SparkMetricsExporter$$anonfun$fromGauge$1.apply(SparkMetricsExporter.scala:131)
	at com.databricks.backend.daemon.driver.SparkMetricsExporter$$anonfun$fromGauge$1.apply(SparkMetricsExporter.scala:129)
	at scala.Option.map(Option.scala:146)
	at com.databricks.backend.daemon.driver.SparkMetricsExporter$.fromGauge(SparkMetricsExporter.scala:129)
	at com.databricks.backend.daemon.driver.SparkMetricsExporter$$anonfun$collect$1.apply(SparkMetricsExporter.scala:51)
	at com.databricks.backend.daemon.driver.SparkMetricsExporter$$anonfun$collect$1.apply(SparkMetricsExporter.scala:47)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:104)
	at com.databricks.backend.daemon.driver.SparkMetricsExporter.collect(SparkMetricsExporter.scala:47)
	at io.prometheus.client.CollectorRegistry$MetricFamilySamplesEnumeration.findNextElement(CollectorRegistry.java:199)
	at io.prometheus.client.CollectorRegistry$MetricFamilySamplesEnumeration.nextElement(CollectorRegistry.java:232)
	at io.prometheus.client.CollectorRegistry$MetricFamilySamplesEnumeration.nextElement(CollectorRegistry.java:153)
	at io.prometheus.client.exporter.common.TextFormat.write004(TextFormat.java:22)
	at com.databricks.DatabricksMain.encodedMetricsBlob(DatabricksMain.scala:363)
	at com.databricks.DatabricksMain$$anonfun$startPrometheusMetricsExport$1.apply$mcV$sp(DatabricksMain.scala:341)
	at com.databricks.DatabricksMain$$anonfun$startPrometheusMetricsExport$1.apply(DatabricksMain.scala:340)
	at com.databricks.DatabricksMain$$anonfun$startPrometheusMetricsExport$1.apply(DatabricksMain.scala:340)
	at com.databricks.util.UntrustedUtils$.tryLog(UntrustedUtils.scala:98)
	at com.databricks.threading.NamedTimer$$anon$1$$anonfun$run$1.apply(NamedTimer.scala:54)
	at com.databricks.threading.NamedTimer$$anon$1$$anonfun$run$1.apply(NamedTimer.scala:54)
	at com.databricks.logging.UsageLogging$$anonfun$recordOperation$1.apply(UsageLogging.scala:369)
	at com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)
	at com.databricks.threading.NamedTimer$$anon$1.withAttributionContext(NamedTimer.scala:51)
	at com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:271)
	at com.databricks.threading.NamedTimer$$anon$1.withAttributionTags(NamedTimer.scala:51)
	at com.databricks.logging.UsageLogging$class.recordOperation(UsageLogging.scala:350)
	at com.databricks.threading.NamedTimer$$anon$1.recordOperation(NamedTimer.scala:51)
	at com.databricks.threading.NamedTimer$$anon$1.run(NamedTimer.scala:53)
	at java.util.TimerThread.mainLoop(Timer.java:555)
	at java.util.TimerThread.run(Timer.java:505)
