{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# https://github.com/rahuldusadrd/parquet-combine-file-format\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# C:\\SPARK\\examples\\src\\main\\python\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# avro data:  https://github.com/Teradata/kylo/tree/master/samples/sample-data/avro\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "# data:  books1.json\n",
    "       \n",
    "# {\"id\":\"978-0641723445\",\"cat\":\"book\",\"name\":\"The Lightning Thief\",\"author\":\"Rick Riordan\",\"series_t\":\"Percy Jackson and the Olympians\",\"sequence_i\":1,\"genre_s\":\"fantasy\",\"inStock\":true,\"price\":12.50,\"pages_i\":384}\n",
    "# {\"id\":\"978-1423103349\",\"cat\":\"paperback\",\"name\":\"The Sea of Monsters\",\"author\":\"Rick Riordan\",\"series_t\":\"Percy Jackson and the Olympians\",\"sequence_i\":2,\"genre_s\":\"fantasy\",\"inStock\":true,\"price\":6.49,\"pages_i\":304}\n",
    "# {\"id\":\"978-1857995879\",\"cat\":\"paperback\",\"name\":\"Sophie's World : The Greek Philosophers\",\"author\":\"Jostein Gaarder\",\"sequence_i\":1,\"genre_s\":\"fantasy\",\"inStock\":true,\"price\":3.07,\"pages_i\":64}\n",
    "# {\"id\":\"978-1923988177\",\"cat\":\"paperback\",\"name\":\"Lucene in Action, Second Edition\",\"author\":\"Michael McCandless\",\"sequence_i\":1,\"genre_s\":\"IT\",\"inStock\":true,\"price\":30.50,\"pages_i\":475}\n",
    "# {\"id\":\"978-1933988077\",\"cat\":\"paperback\",\"name\":\"Lucene in Action, Second Edition\",\"author\":\"Michael McCandless\",\"sequence_i\":1,\"genre_s\":\"IT\",\"inStock\":true,\"price\":30.50,\"pages_i\":475}\n",
    "# {\"id\":\"968-1933988177\",\"cat\":\"paperback\",\"name\":\"Lucene in Action, Second Edition\",\"author\":\"Michael McCandless\",\"sequence_i\":1,\"genre_s\":\"IT\",\"inStock\":true,\"price\":30.50,\"pages_i\":300}\n",
    "# {\"id\":\"978-2933988077\",\"cat\":\"paperback\",\"name\":\"Lucene in Action, Second Edition\",\"author\":\"Michael McCandless\",\"sequence_i\":1,\"genre_s\":\"IT\",\"inStock\":true,\"price\":30.50,\"pages_i\":475}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkSQL - 101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkSQL on Windows 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tom Bresee\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lock and Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load basic libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import random\n",
    "from operator import add\n",
    "from operator import add\n",
    "from time import time\n",
    "import os\n",
    "import time\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more libraries\n",
    "import pyspark\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "# ALS\n",
    "from pyspark.mllib.recommendation import ALS\n",
    "# Logistic Regression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "# Kmeans\n",
    "from pyspark.mllib.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the spark \n",
    "import findspark\n",
    "#findspark.find()\n",
    "findspark.init()\n",
    "# i dont know if you HAVE to have this, but it seems to help alot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x2a0f1c700f0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "conf = SparkConf()\n",
    "# conf.setMaster('spark://HEAD_NODE_HOSTNAME:7077') <- example \n",
    "conf.setAppName('spark-basic')\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "\n",
    "\n",
    "#################################################################\n",
    "# this is how they seem to prefer to turn it up:\n",
    "\n",
    "#    spark = SparkSession.builder.appName(\"PythonPi\").getOrCreate()\n",
    "#################################################################\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the command you start with on Windows 10 to get things going"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# now we are going into SparksQL arena !\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "# spark = SparkSession \\\n",
    "#     .builder \\\n",
    "#     .appName(\"Python Spark SQL basic example\") \\\n",
    "#     .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "#     .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://TM0493322.gsm1900.org:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>spark-basic</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=spark-basic>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.SQLContext at 0x2a0ef114240>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &nbsp;  A. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inferring the Schema\n",
    "With a SQLContext, we are ready to create a DataFrame from our existing RDD. But first we need to tell Spark SQL the schema in our data.\n",
    "\n",
    "Spark SQL can convert an RDD of Row objects to a DataFrame. Rows are constructed by passing a list of key/value pairs as kwargs to the Row class. The keys define the column names, and the types are inferred by looking at the first row. Therefore, it is important that there is no missing data in the first row of the RDD in order to properly infer the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-dc63a27e9690>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# spark is an existing SparkSession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"examples/src/main/resources/people.json\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# spark is an existing SparkSession\n",
    "df = spark.read.json(\"examples/src/main/resources/people.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.141960\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "from random import random\n",
    "from operator import add\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "n = 100000 * 3\n",
    "\n",
    "\n",
    "def f(_):\n",
    "   x = random() * 2 - 1\n",
    "   y = random() * 2 - 1\n",
    "   return 1 if x ** 2 + y ** 2 <= 1 else 0\n",
    "\n",
    "\n",
    "count = sc.parallelize(range(1, n + 1), 3).map(f).reduce(add)\n",
    "\n",
    "print(\"Pi is roughly %f\" % (4.0 * count / n))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SparkContext' object has no attribute 'read'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-e6d29ba3bc4b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"e://README_spark.md\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0msortedCount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msortByKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SparkContext' object has no attribute 'read'"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "raw_data_RDD = sc.textFile(\"e://README_spark.md\")  \n",
    "\n",
    "lines = sc.read.text(\"e://README_spark.md\").rdd.map(lambda r: r[0])\n",
    "\n",
    "sortedCount = lines.flatMap(lambda x: x.split(' ')).map(lambda x: (int(x), 1)).sortByKey()\n",
    "\n",
    "# This is just a demo on how to bring all the sorted data back to a single node.\n",
    "\n",
    "# In reality, we wouldn't want to collect all the data to the driver node.\n",
    "\n",
    "output = sortedCount.collect()\n",
    "\n",
    "for (num, unitcount) in output:\n",
    "        print(num)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#\n",
    "# Licensed to the Apache Software Foundation (ASF) under one or more\n",
    "# contributor license agreements.  See the NOTICE file distributed with\n",
    "# this work for additional information regarding copyright ownership.\n",
    "# The ASF licenses this file to You under the Apache License, Version 2.0\n",
    "# (the \"License\"); you may not use this file except in compliance with\n",
    "# the License.  You may obtain a copy of the License at\n",
    "#\n",
    "#    http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#\n",
    "\n",
    "\"\"\"\n",
    "A simple example demonstrating basic Spark SQL features.\n",
    "Run with:\n",
    "  ./bin/spark-submit examples/src/main/python/sql/basic.py\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "# $example on:init_session$\n",
    "from pyspark.sql import SparkSession\n",
    "# $example off:init_session$\n",
    "\n",
    "# $example on:schema_inferring$\n",
    "from pyspark.sql import Row\n",
    "# $example off:schema_inferring$\n",
    "\n",
    "# $example on:programmatic_schema$\n",
    "# Import data types\n",
    "from pyspark.sql.types import *\n",
    "# $example off:programmatic_schema$\n",
    "\n",
    "\n",
    "def basic_df_example(spark):\n",
    "    # $example on:create_df$\n",
    "    # spark is an existing SparkSession\n",
    "    df = spark.read.json(\"examples/src/main/resources/people.json\")\n",
    "    # Displays the content of the DataFrame to stdout\n",
    "    df.show()\n",
    "    # +----+-------+\n",
    "    # | age|   name|\n",
    "    # +----+-------+\n",
    "    # |null|Michael|\n",
    "    # |  30|   Andy|\n",
    "    # |  19| Justin|\n",
    "    # +----+-------+\n",
    "    # $example off:create_df$\n",
    "\n",
    "    # $example on:untyped_ops$\n",
    "    # spark, df are from the previous example\n",
    "    # Print the schema in a tree format\n",
    "    df.printSchema()\n",
    "    # root\n",
    "    # |-- age: long (nullable = true)\n",
    "    # |-- name: string (nullable = true)\n",
    "\n",
    "    # Select only the \"name\" column\n",
    "    df.select(\"name\").show()\n",
    "    # +-------+\n",
    "    # |   name|\n",
    "    # +-------+\n",
    "    # |Michael|\n",
    "    # |   Andy|\n",
    "    # | Justin|\n",
    "    # +-------+\n",
    "\n",
    "    # Select everybody, but increment the age by 1\n",
    "    df.select(df['name'], df['age'] + 1).show()\n",
    "    # +-------+---------+\n",
    "    # |   name|(age + 1)|\n",
    "    # +-------+---------+\n",
    "    # |Michael|     null|\n",
    "    # |   Andy|       31|\n",
    "    # | Justin|       20|\n",
    "    # +-------+---------+\n",
    "\n",
    "    # Select people older than 21\n",
    "    df.filter(df['age'] > 21).show()\n",
    "    # +---+----+\n",
    "    # |age|name|\n",
    "    # +---+----+\n",
    "    # | 30|Andy|\n",
    "    # +---+----+\n",
    "\n",
    "    # Count people by age\n",
    "    df.groupBy(\"age\").count().show()\n",
    "    # +----+-----+\n",
    "    # | age|count|\n",
    "    # +----+-----+\n",
    "    # |  19|    1|\n",
    "    # |null|    1|\n",
    "    # |  30|    1|\n",
    "    # +----+-----+\n",
    "    # $example off:untyped_ops$\n",
    "\n",
    "    # $example on:run_sql$\n",
    "    # Register the DataFrame as a SQL temporary view\n",
    "    df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "    sqlDF = spark.sql(\"SELECT * FROM people\")\n",
    "    sqlDF.show()\n",
    "    # +----+-------+\n",
    "    # | age|   name|\n",
    "    # +----+-------+\n",
    "    # |null|Michael|\n",
    "    # |  30|   Andy|\n",
    "    # |  19| Justin|\n",
    "    # +----+-------+\n",
    "    # $example off:run_sql$\n",
    "\n",
    "    # $example on:global_temp_view$\n",
    "    # Register the DataFrame as a global temporary view\n",
    "    df.createGlobalTempView(\"people\")\n",
    "\n",
    "    # Global temporary view is tied to a system preserved database `global_temp`\n",
    "    spark.sql(\"SELECT * FROM global_temp.people\").show()\n",
    "    # +----+-------+\n",
    "    # | age|   name|\n",
    "    # +----+-------+\n",
    "    # |null|Michael|\n",
    "    # |  30|   Andy|\n",
    "    # |  19| Justin|\n",
    "    # +----+-------+\n",
    "\n",
    "    # Global temporary view is cross-session\n",
    "    spark.newSession().sql(\"SELECT * FROM global_temp.people\").show()\n",
    "    # +----+-------+\n",
    "    # | age|   name|\n",
    "    # +----+-------+\n",
    "    # |null|Michael|\n",
    "    # |  30|   Andy|\n",
    "    # |  19| Justin|\n",
    "    # +----+-------+\n",
    "    # $example off:global_temp_view$\n",
    "\n",
    "\n",
    "def schema_inference_example(spark):\n",
    "    # $example on:schema_inferring$\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "    # Load a text file and convert each line to a Row.\n",
    "    lines = sc.textFile(\"examples/src/main/resources/people.txt\")\n",
    "    parts = lines.map(lambda l: l.split(\",\"))\n",
    "    people = parts.map(lambda p: Row(name=p[0], age=int(p[1])))\n",
    "\n",
    "    # Infer the schema, and register the DataFrame as a table.\n",
    "    schemaPeople = spark.createDataFrame(people)\n",
    "    schemaPeople.createOrReplaceTempView(\"people\")\n",
    "\n",
    "    # SQL can be run over DataFrames that have been registered as a table.\n",
    "    teenagers = spark.sql(\"SELECT name FROM people WHERE age >= 13 AND age <= 19\")\n",
    "\n",
    "    # The results of SQL queries are Dataframe objects.\n",
    "    # rdd returns the content as an :class:`pyspark.RDD` of :class:`Row`.\n",
    "    teenNames = teenagers.rdd.map(lambda p: \"Name: \" + p.name).collect()\n",
    "    for name in teenNames:\n",
    "        print(name)\n",
    "    # Name: Justin\n",
    "    # $example off:schema_inferring$\n",
    "\n",
    "\n",
    "def programmatic_schema_example(spark):\n",
    "    # $example on:programmatic_schema$\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "    # Load a text file and convert each line to a Row.\n",
    "    lines = sc.textFile(\"examples/src/main/resources/people.txt\")\n",
    "    parts = lines.map(lambda l: l.split(\",\"))\n",
    "    # Each line is converted to a tuple.\n",
    "    people = parts.map(lambda p: (p[0], p[1].strip()))\n",
    "\n",
    "    # The schema is encoded in a string.\n",
    "    schemaString = \"name age\"\n",
    "\n",
    "    fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\n",
    "    schema = StructType(fields)\n",
    "\n",
    "    # Apply the schema to the RDD.\n",
    "    schemaPeople = spark.createDataFrame(people, schema)\n",
    "\n",
    "    # Creates a temporary view using the DataFrame\n",
    "    schemaPeople.createOrReplaceTempView(\"people\")\n",
    "\n",
    "    # SQL can be run over DataFrames that have been registered as a table.\n",
    "    results = spark.sql(\"SELECT name FROM people\")\n",
    "\n",
    "    results.show()\n",
    "    # +-------+\n",
    "    # |   name|\n",
    "    # +-------+\n",
    "    # |Michael|\n",
    "    # |   Andy|\n",
    "    # | Justin|\n",
    "    # +-------+\n",
    "    # $example off:programmatic_schema$\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # $example on:init_session$\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Python Spark SQL basic example\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n",
    "    # $example off:init_session$\n",
    "\n",
    "    basic_df_example(spark)\n",
    "    schema_inference_example(spark)\n",
    "    programmatic_schema_example(spark)\n",
    "\n",
    "    spark.stop()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: bigint, name: string]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# $example on:init_session$\n",
    "from pyspark.sql import SparkSession\n",
    "# $example off:init_session$\n",
    "\n",
    "# $example on:schema_inferring$\n",
    "from pyspark.sql import Row\n",
    "# $example off:schema_inferring$\n",
    "\n",
    "# $example on:programmatic_schema$\n",
    "# Import data types\n",
    "from pyspark.sql.types import *\n",
    "# $example off:programmatic_schema$\n",
    "\n",
    "# READ IN JSON FILE ! \n",
    "df = sqlContext.read.json(\"C:/SPARK/examples/src/main/resources/people.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agg\n",
      "alias\n",
      "approxQuantile\n",
      "cache\n",
      "checkpoint\n",
      "coalesce\n",
      "colRegex\n",
      "collect\n",
      "columns\n",
      "corr\n",
      "count\n",
      "cov\n",
      "createGlobalTempView\n",
      "createOrReplaceGlobalTempView\n",
      "createOrReplaceTempView\n",
      "createTempView\n",
      "crossJoin\n",
      "crosstab\n",
      "cube\n",
      "describe\n",
      "distinct\n",
      "drop\n",
      "dropDuplicates\n",
      "drop_duplicates\n",
      "dropna\n",
      "dtypes\n",
      "exceptAll\n",
      "explain\n",
      "fillna\n",
      "filter\n",
      "first\n",
      "foreach\n",
      "foreachPartition\n",
      "freqItems\n",
      "groupBy\n",
      "groupby\n",
      "head\n",
      "hint\n",
      "intersect\n",
      "intersectAll\n",
      "isLocal\n",
      "isStreaming\n",
      "is_cached\n",
      "join\n",
      "limit\n",
      "localCheckpoint\n",
      "na\n",
      "orderBy\n",
      "persist\n",
      "printSchema\n",
      "randomSplit\n",
      "rdd\n",
      "registerTempTable\n",
      "repartition\n",
      "repartitionByRange\n",
      "replace\n",
      "rollup\n",
      "sample\n",
      "sampleBy\n",
      "schema\n",
      "select\n",
      "selectExpr\n",
      "show\n",
      "sort\n",
      "sortWithinPartitions\n",
      "sql_ctx\n",
      "stat\n",
      "storageLevel\n",
      "subtract\n",
      "summary\n",
      "take\n",
      "toDF\n",
      "toJSON\n",
      "toLocalIterator\n",
      "toPandas\n",
      "union\n",
      "unionAll\n",
      "unionByName\n",
      "unpersist\n",
      "where\n",
      "withColumn\n",
      "withColumnRenamed\n",
      "withWatermark\n",
      "write\n",
      "writeStream\n"
     ]
    }
   ],
   "source": [
    "for i in dir(df):\n",
    "    if not i.startswith(\"_\"):\n",
    "        print(i)\n",
    "        \n",
    "# agg\n",
    "# alias\n",
    "# approxQuantile\n",
    "# cache\n",
    "# checkpoint\n",
    "# coalesce\n",
    "# colRegex\n",
    "# collect\n",
    "# columns\n",
    "# corr\n",
    "# count\n",
    "# cov\n",
    "# createGlobalTempView\n",
    "# createOrReplaceGlobalTempView\n",
    "# createOrReplaceTempView\n",
    "# createTempView\n",
    "# crossJoin\n",
    "# crosstab\n",
    "# cube\n",
    "# describe\n",
    "# distinct\n",
    "# drop\n",
    "# dropDuplicates\n",
    "# drop_duplicates\n",
    "# dropna\n",
    "# dtypes\n",
    "# exceptAll\n",
    "# explain\n",
    "# fillna\n",
    "# filter\n",
    "# first\n",
    "# foreach\n",
    "# foreachPartition\n",
    "# freqItems\n",
    "# groupBy\n",
    "# groupby\n",
    "# head\n",
    "# hint\n",
    "# intersect\n",
    "# intersectAll\n",
    "# isLocal\n",
    "# isStreaming\n",
    "# is_cached\n",
    "# join\n",
    "# limit\n",
    "# localCheckpoint\n",
    "# na\n",
    "# orderBy\n",
    "# persist\n",
    "# printSchema\n",
    "# randomSplit\n",
    "# rdd\n",
    "# registerTempTable\n",
    "# repartition\n",
    "# repartitionByRange\n",
    "# replace\n",
    "# rollup\n",
    "# sample\n",
    "# sampleBy\n",
    "# schema\n",
    "# select\n",
    "# selectExpr\n",
    "# show\n",
    "# sort\n",
    "# sortWithinPartitions\n",
    "# sql_ctx\n",
    "# stat\n",
    "# storageLevel\n",
    "# subtract\n",
    "# summary\n",
    "# take\n",
    "# toDF\n",
    "# toJSON\n",
    "# toLocalIterator\n",
    "# toPandas\n",
    "# union\n",
    "# unionAll\n",
    "# unionByName\n",
    "# unpersist\n",
    "# where\n",
    "# withColumn\n",
    "# withColumnRenamed\n",
    "# withWatermark\n",
    "# write\n",
    "# writeStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#  people.json\n",
    "\n",
    "# {\"name\":\"Michael\"}\n",
    "# {\"name\":\"Andy\", \"age\":30}\n",
    "# {\"name\":\"Justin\", \"age\":19}\n",
    "\n",
    "# Displays the content of the DataFrame to stdout\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print the schema in a tree format\n",
    "df.printSchema()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Select only the \"name\" column\n",
    "df.select(\"name\").show()  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|   name|(age + 1)|\n",
      "+-------+---------+\n",
      "|Michael|     null|\n",
      "|   Andy|       31|\n",
      "| Justin|       20|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Select everybody, but increment the age by 1\n",
    "df.select(df['name'], df['age'] + 1).show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Select people older than 21\n",
    "df.filter(df['age'] > 21).show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| age|count|\n",
      "+----+-----+\n",
      "|  19|    1|\n",
      "|null|    1|\n",
      "|  30|    1|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Count people by age\n",
    "df.groupBy(\"age\").count().show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# $example on:run_sql$\n",
    "# Register the DataFrame as a SQL temporary view\n",
    "\n",
    "# Register the DataFrame as a SQL temporary view\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "sqlDF = sqlContext.sql(\"SELECT * FROM people\")\n",
    "sqlDF.show()  \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"Temporary view 'people' already exists;\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o74.createGlobalTempView.\n: org.apache.spark.sql.catalyst.analysis.TempTableAlreadyExistsException: Temporary view 'people' already exists;\r\n\tat org.apache.spark.sql.catalyst.catalog.GlobalTempViewManager.create(GlobalTempViewManager.scala:61)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createGlobalTempView(SessionCatalog.scala:507)\r\n\tat org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:149)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\r\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:194)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:79)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3406)\r\n\tat org.apache.spark.sql.Dataset.createGlobalTempView(Dataset.scala:3114)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-91171b303457>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# $example on:global_temp_view$\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Register the DataFrame as a global temporary view\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateGlobalTempView\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"people\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Global temporary view is tied to a system preserved database `global_temp`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mcreateGlobalTempView\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m         \"\"\"\n\u001b[1;32m--> 200\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateGlobalTempView\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     69\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.catalyst.parser.ParseException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mParseException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: \"Temporary view 'people' already exists;\""
     ]
    }
   ],
   "source": [
    "\n",
    "# $example on:global_temp_view$\n",
    "# Register the DataFrame as a global temporary view\n",
    "df.createGlobalTempView(\"people\")\n",
    "\n",
    "# Global temporary view is tied to a system preserved database `global_temp`\n",
    "sqlContext.sql(\"SELECT * FROM global_temp.people\").show()    \n",
    "\n",
    "# error =  AnalysisException: \"Temporary view 'people' already exists;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Global temporary view is cross-session\n",
    "\n",
    "sqlContext.newSession().sql(\"SELECT * FROM global_temp.people\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://TM0493322.gsm1900.org:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>spark-basic</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=spark-basic>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.SQLContext at 0x2a0ef114240>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## datasource.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> PARQUET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "# $example on:schema_merging$\n",
    "from pyspark.sql import Row\n",
    "# $example off:schema_merging$\n",
    "\n",
    "# $example on:generic_load_save_functions$\n",
    "df = sqlContext.read.load(\"C:/SPARK/examples/src/main/resources/users.parquet\")\n",
    "# df.select(\"name\", \"favorite_color\").write.save(\"namesAndFavColors.parquet\")\n",
    "# $example off:generic_load_save_functions$\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, favorite_color: string, favorite_numbers: array<int>]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# < i n s r t   -   image as you do it >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------------+\n",
      "|  name|favorite_color|favorite_numbers|\n",
      "+------+--------------+----------------+\n",
      "|Alyssa|          null|  [3, 9, 15, 20]|\n",
      "|   Ben|           red|              []|\n",
      "+------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parquet is a columnar format, supported by many data processing systems. The advantages of having a columnar storage are as follows −\n",
    "\n",
    "Columnar storage limits IO operations.\n",
    "\n",
    "Columnar storage can fetch specific columns that you need to access.\n",
    "\n",
    "Columnar storage consumes less space.\n",
    "\n",
    "Columnar storage gives better-summarized data and follows type-specific encoding.\n",
    "\n",
    "Spark SQL provides support for both reading and writing parquet files that automatically capture the schema of the original data. Like JSON datasets, parquet files follow the same procedure.\n",
    "\n",
    "Let’s take another look at the same example of employee record data named employee.parquet placed in the same directory where spark-shell is running.\n",
    "\n",
    "Given data − Do not bother about converting the input data of employee records into parquet format. We use the following commands that convert the RDD data into Parquet file. Place the employee.json document, which we have used as the input file in our previous examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# $example on:generic_load_save_functions$\n",
    "df2 = sqlContext.read.load(\"E:/userdata1.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+----------+---------+--------------------+------+---------------+-------------------+--------------------+----------+---------+--------------------+--------------------+\n",
      "|  registration_dttm| id|first_name|last_name|               email|gender|     ip_address|                 cc|             country| birthdate|   salary|               title|            comments|\n",
      "+-------------------+---+----------+---------+--------------------+------+---------------+-------------------+--------------------+----------+---------+--------------------+--------------------+\n",
      "|2016-02-03 01:55:29|  1|    Amanda|   Jordan|    ajordan0@com.com|Female|    1.197.201.2|   6759521864920116|           Indonesia|  3/8/1971| 49756.53|    Internal Auditor|               1E+02|\n",
      "|2016-02-03 11:04:03|  2|    Albert|  Freeman|     afreeman1@is.gd|  Male| 218.111.175.34|                   |              Canada| 1/16/1968|150280.17|       Accountant IV|                    |\n",
      "|2016-02-02 19:09:31|  3|    Evelyn|   Morgan|emorgan2@altervis...|Female|   7.161.136.94|   6767119071901597|              Russia|  2/1/1960|144972.51| Structural Engineer|                    |\n",
      "|2016-02-02 18:36:21|  4|    Denise|    Riley|    driley3@gmpg.org|Female|  140.35.109.83|   3576031598965625|               China|  4/8/1997| 90263.05|Senior Cost Accou...|                    |\n",
      "|2016-02-02 23:05:31|  5|    Carlos|    Burns|cburns4@miitbeian...|      | 169.113.235.40|   5602256255204850|        South Africa|          |     null|                    |                    |\n",
      "|2016-02-03 01:22:34|  6|   Kathryn|    White|  kwhite5@google.com|Female| 195.131.81.179|   3583136326049310|           Indonesia| 2/25/1983| 69227.11|   Account Executive|                    |\n",
      "|2016-02-03 02:33:08|  7|    Samuel|   Holmes|sholmes6@foxnews.com|  Male| 232.234.81.197|   3582641366974690|            Portugal|12/18/1987| 14247.62|Senior Financial ...|                    |\n",
      "|2016-02-03 00:47:06|  8|     Harry|   Howell| hhowell7@eepurl.com|  Male|   91.235.51.73|                   |Bosnia and Herzeg...|  3/1/1962|186469.43|    Web Developer IV|                    |\n",
      "|2016-02-02 21:52:53|  9|      Jose|   Foster|   jfoster8@yelp.com|  Male|   132.31.53.61|                   |         South Korea| 3/27/1992|231067.84|Software Test Eng...|               1E+02|\n",
      "|2016-02-03 12:29:47| 10|     Emily|  Stewart|estewart9@opensou...|Female| 143.28.251.245|   3574254110301671|             Nigeria| 1/28/1997| 27234.28|     Health Coach IV|                    |\n",
      "|2016-02-02 18:10:42| 11|     Susan|  Perkins| sperkinsa@patch.com|Female|    180.85.0.62|   3573823609854134|              Russia|          |210001.95|                    |                    |\n",
      "|2016-02-03 12:04:34| 12|     Alice|    Berry|aberryb@wikipedia...|Female| 246.225.12.189|   4917830851454417|               China| 8/12/1968| 22944.53|    Quality Engineer|                    |\n",
      "|2016-02-03 12:48:17| 13|    Justin|    Berry|jberryc@usatoday.com|  Male|   157.7.146.43|6331109912871813274|              Zambia| 8/15/1975| 44165.46|Structural Analys...|                    |\n",
      "|2016-02-03 15:46:52| 14|     Kathy| Reynolds|kreynoldsd@redcro...|Female|  81.254.172.13|   5537178462965976|Bosnia and Herzeg...| 6/27/1970|286592.99|           Librarian|                    |\n",
      "|2016-02-03 02:53:23| 15|   Dorothy|   Hudson|dhudsone@blogger.com|Female|       8.59.7.0|   3542586858224170|               Japan|12/20/1989|157099.71|  Nurse Practicioner|<script>alert('hi...|\n",
      "|2016-02-02 18:44:01| 16|     Bruce|   Willis|bwillisf@bluehost...|  Male|239.182.219.189|   3573030625927601|              Brazil|          |239100.65|                    |                    |\n",
      "|2016-02-02 18:57:45| 17|     Emily|  Andrews|eandrewsg@cornell...|Female| 29.231.180.172|     30271790537626|              Russia| 4/13/1990|116800.65|        Food Chemist|                    |\n",
      "|2016-02-03 10:44:24| 18|   Stephen|  Wallace|swallaceh@netvibe...|  Male|  152.49.213.62|   5433943468526428|             Ukraine| 1/15/1978|248877.99|Account Represent...|                    |\n",
      "|2016-02-03 05:45:54| 19|  Clarence|   Lawson|clawsoni@vkontakt...|  Male| 107.175.15.152|   3544052814080964|              Russia|          |177122.99|                    |                    |\n",
      "|2016-02-03 04:30:36| 20|   Rebecca|     Bell| rbellj@bandcamp.com|Female|172.215.104.127|                   |               China|          |137251.19|                    |                    |\n",
      "+-------------------+---+----------+---------+--------------------+------+---------------+-------------------+--------------------+----------+---------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "userdata[1-5].parquet: These are sample files containing data in PARQUET format.\n",
    "\n",
    "-> Number of rows in each file: 1000\n",
    "-> Column details:\n",
    "column#\t\tcolumn_name\t\thive_datatype\n",
    "=====================================================\n",
    "1\t\tregistration_dttm \ttimestamp\n",
    "2\t\tid \t\t\tint\n",
    "3\t\tfirst_name \t\tstring\n",
    "4\t\tlast_name \t\tstring\n",
    "5\t\temail \t\t\tstring\n",
    "6\t\tgender \t\t\tstring\n",
    "7\t\tip_address \t\tstring\n",
    "8\t\tcc \t\t\tstring\n",
    "9\t\tcountry \t\tstring\n",
    "10\t\tbirthdate \t\tstring\n",
    "11\t\tsalary \t\t\tdouble\n",
    "12\t\ttitle \t\t\tstring\n",
    "13\t\tcomments \t\tstring\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "| 10|\n",
      "| 11|\n",
      "| 12|\n",
      "| 13|\n",
      "| 14|\n",
      "| 15|\n",
      "| 16|\n",
      "| 17|\n",
      "| 18|\n",
      "| 19|\n",
      "| 20|\n",
      "+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# sqlDF = sqlContext.sql(\"SELECT * FROM people\")\n",
    "\n",
    "all_info_in_id_column = df2.select(\"id\").show()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|first_name|\n",
      "+----------+\n",
      "|    Amanda|\n",
      "|    Albert|\n",
      "|    Evelyn|\n",
      "|    Denise|\n",
      "|    Carlos|\n",
      "|   Kathryn|\n",
      "|    Samuel|\n",
      "|     Harry|\n",
      "|      Jose|\n",
      "|     Emily|\n",
      "|     Susan|\n",
      "|     Alice|\n",
      "|    Justin|\n",
      "|     Kathy|\n",
      "|   Dorothy|\n",
      "|     Bruce|\n",
      "|     Emily|\n",
      "|   Stephen|\n",
      "|  Clarence|\n",
      "|   Rebecca|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "first_name = df2.select(\"first_name\").show()   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
