{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8d3513b7698b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "#  iff  sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# good reference: \n",
    "# https://www.datacamp.com/community/tutorials/apache-spark-python\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# could be dangerous, use later if you feel helps \n",
    "# import os\n",
    "# import sys\n",
    "# spark_path = r\"C:\\Users\\tbresee\\Spark\" # spark installed folder\n",
    "# os.environ['SPARK_HOME'] = spark_path\n",
    "# sys.path.insert(0, spark_path + \"/bin\")\n",
    "# sys.path.insert(0, spark_path + \"/python/pyspark/\")\n",
    "# sys.path.insert(0, spark_path + \"/python/lib/pyspark.zip\")\n",
    "# sys.path.insert(0, spark_path + \"/python/lib/py4j-0.10.7-src.zip\")\n",
    "\n",
    "\n",
    "\n",
    "# import findspark\n",
    "# findspark.init()\n",
    "# import pyspark\n",
    "\n",
    "\n",
    "\n",
    "# from pyspark import SparkContext\n",
    "# sc = SparkContext()\n",
    "# data = range(1,1000)\n",
    "# rdd = sc.parallelize(data)\n",
    "# rdd.collect()\n",
    "\n",
    "\n",
    "\n",
    "# import pyspark \n",
    "# only run after findspark.init()\n",
    "# from pyspark.sql import SparkSession\n",
    "# spark = SparkSession.builder.getOrCreate()\n",
    "# this appeared to work ! ! ! ! ! \n",
    "\n",
    "\n",
    "\n",
    "# def mod(x):\n",
    "#     import numpy as np\n",
    "#     return (x, np.mod(x, 2))\n",
    "# rdd = sc.parallelize(range(1000)).map(mod).take(10)\n",
    "# print rdd\n",
    "\n",
    "\n",
    "\n",
    "# sc = pyspark.SparkContext()\n",
    "\n",
    "\n",
    "\n",
    "# from pyspark import SparkConf, SparkContext\n",
    "# conf = SparkConf().setAppName(\"PySpark App\").setMaster(\"spark://master:7077\")\n",
    "# sc = SparkContext(conf=conf)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  data_RDD.toDF\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "# import findspark\n",
    "# findspark.init()\n",
    "# import pyspark\n",
    "# import random\n",
    "# sc = pyspark.SparkContext(appName=\"Pi\")\n",
    "# num_samples = 100000000\n",
    "# def inside(p):     \n",
    "#   x, y = random.random(), random.random()\n",
    "#   return x*x + y*y < 1\n",
    "# count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "# pi = 4 * count / num_samples\n",
    "# print(pi)\n",
    "# sc.stop()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# import findspark\n",
    "# findspark.init(\"/opt/spark\")\n",
    "# import random\n",
    "# from pyspark import SparkContext\n",
    "# sc = SparkContext(appName=\"EstimatePi\")\n",
    "# def inside(p):\n",
    "#     x, y = random.random(), random.random()\n",
    "#     return x*x + y*y < 1\n",
    "# NUM_SAMPLES = 1000000\n",
    "# count = sc.parallelize(range(0, NUM_SAMPLES)) \\\n",
    "#              .filter(inside).count()\n",
    "# print(\"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES))\n",
    "# sc.stop()\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PYSPARK 101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark on Windows 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tom Bresee\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some notes: \n",
    "    \n",
    "    If you ever ever ever want to run Spark on Windows 10, you must have your environment variables perfect "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big Picture View"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.tutorialspoint.com/pyspark/images/sparkcontext.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lock and Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load basic libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import random\n",
    "from operator import add\n",
    "from operator import add\n",
    "from time import time\n",
    "import os\n",
    "import time\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more libraries\n",
    "import pyspark\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "# ALS\n",
    "from pyspark.mllib.recommendation import ALS\n",
    "# Logistic Regression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "# Kmeans\n",
    "from pyspark.mllib.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the spark \n",
    "import findspark\n",
    "#findspark.find()\n",
    "findspark.init()\n",
    "# i dont know if you HAVE to have this, but it seems to help alot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the command you start with on Windows 10 to get things going"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x2881b8b1400>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=spark-basic, master=local[*]) created by __init__ at <ipython-input-135-db3aa604c00f>:7 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-140-d2146eb5e2df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# conf.setMaster('spark://HEAD_NODE_HOSTNAME:7077') <- example\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetAppName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'spark-basic'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    131\u001b[0m                     \" note this option will be removed in Spark 3.0\")\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    330\u001b[0m                         \u001b[1;34m\" created by %s at %s:%s \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[1;32m--> 332\u001b[1;33m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[0;32m    333\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=spark-basic, master=local[*]) created by __init__ at <ipython-input-135-db3aa604c00f>:7 "
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "conf = SparkConf()\n",
    "# conf.setMaster('spark://HEAD_NODE_HOSTNAME:7077') <- example \n",
    "conf.setAppName('spark-basic')\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# note:  if you try to rerun things, you will error out, you can't have two spark contexts, so kill one:\n",
    "# ValueError: Cannot run multiple SparkContexts at once; existing SparkContext(app=spark-basic, master=local[*]) \n",
    "#     created by __init__ at <ipython-input-135-db3aa604c00f>:7 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets look around "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.3\n"
     ]
    }
   ],
   "source": [
    "print(sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you have spark context running and its Apache Spark version 2.4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### housekeeping but important to keep up to date with to avoid future issues with installs etc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\JAVA'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\SPARK'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\HADOOP\\\\hadoop-2.7.1'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\MAVEN'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\tbresee\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\python'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'jupyter'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\"'notebook'\""
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\tbresee\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\python'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ.get('JAVA_HOME')\n",
    "os.environ.get('SPARK_HOME')\n",
    "os.environ.get('HADOOP_HOME')\n",
    "os.environ.get('MAVEN_HOME')\n",
    "os.environ.get('PYSPARK_PYTHON')\n",
    "os.environ.get('PYSPARK_DRIVER_PYTHON')\n",
    "\n",
    "os.environ.get('PYSPARK_DRIVER_PYTHON_OPTS')  # wrong ? \n",
    "#  maybe this is right ? ? ? worth changing ? PYSPARK_DRIVER_PYTHON_OPTS='notebook' pyspark\n",
    "\n",
    "os.environ.get('PYSPARK_PYTHON')\n",
    "# ? PYSPARK_PYTHON=python3 ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ.get('PATH')   < - - good to enter\n",
    "# os.environ.get('Path')   < - same thing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### &nbsp; A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://TM0493322.gsm1900.org:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>spark-basic</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=spark-basic>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc\n",
    "# the local[*] means use all cores you can \n",
    "\n",
    "# click the URL, its cool "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STOP AND READ THIS:\n",
    "\n",
    "# print(sc.uiWebUrl) \n",
    "\n",
    "# this will show you the URL you can connect to \n",
    "# you will be able to see all the behind the scenes work going on,\n",
    "# you need to do this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Base options can be expanded: \n",
    "\n",
    "class pyspark.SparkContext (\n",
    "   master = None,\n",
    "   appName = None, \n",
    "   sparkHome = None, \n",
    "   pyFiles = None, \n",
    "   environment = None, \n",
    "   batchSize = 0, \n",
    "   serializer = PickleSerializer(), \n",
    "   conf = None, \n",
    "   gateway = None, \n",
    "   jsc = None, \n",
    "   profiler_cls = <class 'pyspark.profiler.BasicProfiler'>)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are running Apache Spark version:   2.4.3\n"
     ]
    }
   ],
   "source": [
    "print(\"You are running Apache Spark version:  \",sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your application name is:   spark-basic\n"
     ]
    }
   ],
   "source": [
    "print(\"Your application name is:  \", sc.appName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "These are all the methods available for the sc context:\n",
      "\n",
      "PACKAGE_EXTENSIONS\n",
      "accumulator\n",
      "addFile\n",
      "addPyFile\n",
      "appName\n",
      "applicationId\n",
      "binaryFiles\n",
      "binaryRecords\n",
      "broadcast\n",
      "cancelAllJobs\n",
      "cancelJobGroup\n",
      "defaultMinPartitions\n",
      "defaultParallelism\n",
      "dump_profiles\n",
      "emptyRDD\n",
      "environment\n",
      "getConf\n",
      "getLocalProperty\n",
      "getOrCreate\n",
      "hadoopFile\n",
      "hadoopRDD\n",
      "master\n",
      "newAPIHadoopFile\n",
      "newAPIHadoopRDD\n",
      "parallelize\n",
      "pickleFile\n",
      "profiler_collector\n",
      "pythonExec\n",
      "pythonVer\n",
      "range\n",
      "runJob\n",
      "sequenceFile\n",
      "serializer\n",
      "setCheckpointDir\n",
      "setJobDescription\n",
      "setJobGroup\n",
      "setLocalProperty\n",
      "setLogLevel\n",
      "setSystemProperty\n",
      "show_profiles\n",
      "sparkHome\n",
      "sparkUser\n",
      "startTime\n",
      "statusTracker\n",
      "stop\n",
      "textFile\n",
      "uiWebUrl\n",
      "union\n",
      "version\n",
      "wholeTextFiles\n"
     ]
    }
   ],
   "source": [
    "# all commands for sc. (i.e. methods)\n",
    "\n",
    "print(\"\\nThese are all the methods available for the sc context:\\n\")\n",
    "for i in dir(sc): \n",
    "    if not i.startswith(\"_\"):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'spark-basic'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'2.4.3'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1559583461540"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'local-1559583461641'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'PYTHONHASHSEED': '0'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.pythonVer              # your python version \n",
    "sc.appName                # your app name\n",
    "sc.version                # your apache spark version \n",
    "sc.startTime              # start time for context\n",
    "sc.applicationId          # appliation ID\n",
    "sc.environment            # environment infor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1559583461540\n"
     ]
    }
   ],
   "source": [
    "print(sc.startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.name', 'spark-basic'),\n",
       " ('spark.app.id', 'local-1559583461641'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.driver.host', 'TM0493322.gsm1900.org'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.driver.port', '1103')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# very helpful command, information quicker\n",
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Lets get some data into the basic Spark data structure:  the Resilient Distributed Dataset (RDD)\n",
    "\n",
    "#  An RDD is a distributed collection of elements. All work in Spark is expressed as either creating new RDDs, \n",
    "#  transforming existing RDDs, or calling actions on RDDs to compute a result.  Thats it. \n",
    "\n",
    "#  Spark automatically distributes the data contained in RDDs across your cluster and parallelizes \n",
    "#  the operations you perform on them.  Obviously assuming you have a cluster. \n",
    "\n",
    "\n",
    "#  Creating a new RDD using 'parallelize' command \n",
    "\n",
    "# range of 0 to 100 values, simple python\n",
    "a = range(100)\n",
    "\n",
    "# creates list of values 0 - > 100 into RDD \n",
    "data_RDD = sc.parallelize(a)  # build the RDD manually, not importing a file approach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregate\n",
      "aggregateByKey\n",
      "barrier\n",
      "cache\n",
      "cartesian\n",
      "checkpoint\n",
      "coalesce\n",
      "cogroup\n",
      "collect\n",
      "collectAsMap\n",
      "combineByKey\n",
      "context\n",
      "count\n",
      "countApprox\n",
      "countApproxDistinct\n",
      "countByKey\n",
      "countByValue\n",
      "ctx\n",
      "distinct\n",
      "filter\n",
      "first\n",
      "flatMap\n",
      "flatMapValues\n",
      "fold\n",
      "foldByKey\n",
      "foreach\n",
      "foreachPartition\n",
      "fullOuterJoin\n",
      "func\n",
      "getCheckpointFile\n",
      "getNumPartitions\n",
      "getStorageLevel\n",
      "glom\n",
      "groupBy\n",
      "groupByKey\n",
      "groupWith\n",
      "histogram\n",
      "id\n",
      "intersection\n",
      "isCheckpointed\n",
      "isEmpty\n",
      "isLocallyCheckpointed\n",
      "is_barrier\n",
      "is_cached\n",
      "is_checkpointed\n",
      "join\n",
      "keyBy\n",
      "keys\n",
      "leftOuterJoin\n",
      "localCheckpoint\n",
      "lookup\n",
      "map\n",
      "mapPartitions\n",
      "mapPartitionsWithIndex\n",
      "mapPartitionsWithSplit\n",
      "mapValues\n",
      "max\n",
      "mean\n",
      "meanApprox\n",
      "min\n",
      "name\n",
      "partitionBy\n",
      "partitioner\n",
      "persist\n",
      "pipe\n",
      "preservesPartitioning\n",
      "prev\n",
      "randomSplit\n",
      "reduce\n",
      "reduceByKey\n",
      "reduceByKeyLocally\n",
      "repartition\n",
      "repartitionAndSortWithinPartitions\n",
      "rightOuterJoin\n",
      "sample\n",
      "sampleByKey\n",
      "sampleStdev\n",
      "sampleVariance\n",
      "saveAsHadoopDataset\n",
      "saveAsHadoopFile\n",
      "saveAsNewAPIHadoopDataset\n",
      "saveAsNewAPIHadoopFile\n",
      "saveAsPickleFile\n",
      "saveAsSequenceFile\n",
      "saveAsTextFile\n",
      "setName\n",
      "sortBy\n",
      "sortByKey\n",
      "stats\n",
      "stdev\n",
      "subtract\n",
      "subtractByKey\n",
      "sum\n",
      "sumApprox\n",
      "take\n",
      "takeOrdered\n",
      "takeSample\n",
      "toDF\n",
      "toDebugString\n",
      "toLocalIterator\n",
      "top\n",
      "treeAggregate\n",
      "treeReduce\n",
      "union\n",
      "unpersist\n",
      "values\n",
      "variance\n",
      "zip\n",
      "zipWithIndex\n",
      "zipWithUniqueId\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# so now that we have build an RDD, here is a good way of showing you the .methods possible to use\n",
    "for i in dir(data_RDD):\n",
    "    if not i.startswith(\"_\"):\n",
    "        print(i)\n",
    "        \n",
    "# these are all the 'commands' you can run on your RDD  ! ! ! \n",
    "#  very helpful for seeing the big picture \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seconds since epoch = 1559588341.9662297\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "seconds = time.time()\n",
    "print(\"Seconds since epoch =\", seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.7821595668792725\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "# please return the contents of my RDD ! \n",
    "data_RDD.collect()\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "collect(): will get all the elements in the RDD into memory for us to work with them. For this reason it has to be used with care, specially when working with large RDDs!\n",
    "```    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data collected in 5.726 seconds\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "test = data_RDD.collect()\n",
    "tt = time() - t0\n",
    "print (\"Data collected in {} seconds\".format(round(tt,3)))\n",
    "\n",
    "#  Every Spark worker node that has a fragment of the RDD has to be coordinated in order to \n",
    "#  retrieve its part, and then reduce everything together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# how many elements are in my data RDD set ? \n",
    "\n",
    "data_RDD.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# return the first five entries of my data RDD please\n",
    "\n",
    "data_RDD.take(5)\n",
    "\n",
    "# kinda like .head(n=5)\n",
    "# we used the range command, and thus you see start with 0 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# i want to see if my data_RDD has the value 10 in it \n",
    "# then i want to print out this filtered RDD \n",
    "\n",
    "filtered_RDD = data_RDD.filter(lambda x: x == 10)\n",
    "filtered_RDD.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# is the value 10 in my data_RDD ? \n",
    "\n",
    "bool = data_RDD.filter(lambda x: x == 10).count()>0\n",
    "print(bool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# tell me how many times you see the number 22 in my data_RDD \n",
    "#  obviously i should only have 1, im going from 0 -> 99\n",
    "data_RDD.filter(lambda x: x == 22).count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[99]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data_RDD.top(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# return the min value in the RDD \n",
    "\n",
    "data_RDD.min()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your RDD:\n",
      "  It is the following python type:\n",
      "\t <class 'pyspark.rdd.PipelinedRDD'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Your RDD:\")\n",
    "print(\"  It is the following python type:\")\n",
    "print(\"\\t\",type(data_RDD))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of entries in this RDD . . . . . . .  100\n",
      "Min value in your RDD . . . . . . . . . . .  0\n",
      "Max value in your RDD . . . . . . . . . . .  99\n",
      "Mean of your RDD values . . . . . . . . . .  49.5\n",
      "Variance of your RDD  . . . . . . . . . . .  833.25\n",
      "Std Dev of your RDD . . . . . . . . . . . .  28.86607004772212\n",
      "Sum of your RDD values . . . . . . . . . .   4950\n",
      "Is your RDD empty ? . . . . . . . . . . . .  False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# maybe get a touch smarter here:\n",
    "\n",
    "def toms_RDD_numeric_analyzer(your_RDD):\n",
    "    print(\"\\nNumber of entries in this RDD . . . . . . . \", your_RDD.count())\n",
    "    print(\"Min value in your RDD . . . . . . . . . . . \", your_RDD.min())\n",
    "    print(\"Max value in your RDD . . . . . . . . . . . \", your_RDD.max())\n",
    "    print(\"Mean of your RDD values . . . . . . . . . . \", your_RDD.mean())\n",
    "    print(\"Variance of your RDD  . . . . . . . . . . . \", your_RDD.variance())\n",
    "    print(\"Std Dev of your RDD . . . . . . . . . . . . \", your_RDD.stdev())\n",
    "    print(\"Sum of your RDD values . . . . . . . . . .  \", your_RDD.sum())\n",
    "    print(\"Is your RDD empty ? . . . . . . . . . . . . \", data_RDD.isEmpty())\n",
    "\n",
    "\n",
    "toms_RDD_numeric_analyzer(data_RDD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.0,\n",
       "  4.95,\n",
       "  9.9,\n",
       "  14.850000000000001,\n",
       "  19.8,\n",
       "  24.75,\n",
       "  29.700000000000003,\n",
       "  34.65,\n",
       "  39.6,\n",
       "  44.550000000000004,\n",
       "  49.5,\n",
       "  54.45,\n",
       "  59.400000000000006,\n",
       "  64.35000000000001,\n",
       "  69.3,\n",
       "  74.25,\n",
       "  79.2,\n",
       "  84.15,\n",
       "  89.10000000000001,\n",
       "  94.05,\n",
       "  99],\n",
       " [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# terrible example with this dataset, but pretend you want a histogram - \n",
    "# set your bin count to lets say 20, tell me how many values in this \n",
    "# histogram and how distributed (my case is like perfectly uniform but whatever)\n",
    "\n",
    "data_RDD.histogram(buckets=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81, 100]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# lets get a bit fancier:\n",
    "\n",
    "new_RDD = data_RDD.map(lambda i: i * i)\n",
    "new_RDD.take(11)  # take 11 \n",
    "\n",
    "# map your existing values to a new value (i squared)\n",
    "# pretty simple stuff (im returning the first 11 entries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "new_RDD = data_RDD.map(lambda i: i + 100)\n",
    "new_RDD.take(15)  # take first 15 values \n",
    "\n",
    "# map each value to a number 100 higher, simple example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding all the elements -> 15\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# create new RDD called nums_RDD\n",
    "nums_RDD = sc.parallelize([1, 2, 3, 4, 5])\n",
    "\n",
    "adding = nums_RDD.reduce(add)\n",
    "\n",
    "print (\"Adding all the elements -> %i\" % (adding))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size is 11 out of 100\n",
      "\n",
      " Sample is: \n",
      " [0, 8, 11, 22, 30, 33, 36, 66, 81, 82, 83]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# we will *sample* data now from the original data_RDD\n",
    "\n",
    "#  -  Sample with replacement:  False\n",
    "#  -  Sample size as a fraction:  0.1\n",
    "#  -  Random Seed:  1234 \n",
    "\n",
    "raw_data_sample_RDD = data_RDD.sample(False, 0.1, 1234)\n",
    "\n",
    "sample_size = raw_data_sample_RDD.count()\n",
    "total_size = data_RDD.count()\n",
    "print (\"Sample size is {} out of {}\".format(sample_size, total_size))\n",
    "\n",
    "print(\"\\n Sample is: \\n\", raw_data_sample_RDD.collect())\n",
    "\n",
    "# you can also use the .takeSample command instead and just tell it how many samples to take\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# shifting gears, lets create a new RDD out of words (parallelize...)\n",
    "\n",
    "words_RDD = sc.parallelize (\n",
    "   [\"Tom\", \n",
    "    \"is\", \n",
    "    \"very\",\n",
    "    \"very\", \n",
    "    \"cool\", \n",
    "    \"and\", \n",
    "    \"never\", \n",
    "    \"ever\", \n",
    "    \"sleeps\", \n",
    "    \",\", \n",
    "    \"just\", \n",
    "    \"like\", \n",
    "    \"money\",\n",
    "    \".\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tom',\n",
       " 'is',\n",
       " 'very',\n",
       " 'very',\n",
       " 'cool',\n",
       " 'and',\n",
       " 'never',\n",
       " 'ever',\n",
       " 'sleeps',\n",
       " ',',\n",
       " 'just',\n",
       " 'like',\n",
       " 'money',\n",
       " '.']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "This interaction was completed in 0.046 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# improve this...\n",
    "\n",
    "# lets get fancy and start timing some of this stuff \n",
    "#  you may want to keep this function, it helps \n",
    "\n",
    "t0 = time.time()  # what time is it prior to your command \n",
    "\n",
    "# all the elements in the RDD are returned here \n",
    "words_RDD.collect()\n",
    "print(type(words_RDD.collect()))\n",
    "\n",
    "tt = time.time() - t0   # calculated delta of time it took to execute the command\n",
    "\n",
    "print (\"This interaction was completed in {} seconds\".format(round(tt,3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements in words_RDD -> ['Tom', 'is', 'very', 'very', 'cool', 'and', 'never', 'ever', 'sleeps', ',', 'just', 'like', 'money', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "coll = words_RDD.collect()\n",
    "\n",
    "print (\"Elements in words_RDD -> %s\" % (coll))\n",
    "\n",
    "# remember, maybe not a great idea to collect() alot...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# how many words in my RDD ?\n",
    "words_RDD.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements in words_RDD -> 14\n"
     ]
    }
   ],
   "source": [
    "\n",
    "counts = words_RDD.count()\n",
    "print (\"Number of elements in words_RDD -> %i\" % (counts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def f(x): \n",
    "    print(x)\n",
    "    \n",
    "# apply function     \n",
    "fore = words_RDD.foreach(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitered RDD -> ['Tom']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# lets use filter to see if there is match for 'Tom' in my words_RDD\n",
    "\n",
    "words_filter_RDD = words_RDD.filter(lambda x: 'Tom' in x)\n",
    "\n",
    "filtered = words_filter_RDD.collect()\n",
    "\n",
    "print (\"Fitered RDD -> %s\" % (filtered))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(False, False, False, False, 1)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# find out storage levels \n",
    "\n",
    "words_RDD.getStorageLevel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines with a: 1, lines with b: 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "numAs = words_RDD.filter(lambda s: 'Tom' in s).count()\n",
    "\n",
    "numBs = words_RDD.filter(lambda s: 'very' in s).count()\n",
    "\n",
    "print (\"Lines with a: %i, lines with b: %i\" % (numAs, numBs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Tom', 1),\n",
       " ('is', 1),\n",
       " ('very', 1),\n",
       " ('very', 1),\n",
       " ('cool', 1),\n",
       " ('and', 1),\n",
       " ('never', 1),\n",
       " ('ever', 1),\n",
       " ('sleeps', 1),\n",
       " (',', 1),\n",
       " ('just', 1),\n",
       " ('like', 1),\n",
       " ('money', 1),\n",
       " ('.', 1)]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# this appears to be odd but go with it \n",
    "\n",
    "# A new RDD is returned by applying a function to each element in the RDD. \n",
    "# In the following example, we form a key value pair and map every string with \n",
    "# a value of 1.\n",
    "\n",
    "# fyi, you do have this option: preservesPartitioning = True (or False)\n",
    "\n",
    "words_map = words_RDD.map(lambda x: (x, 1))\n",
    "\n",
    "mapping = words_map.collect()\n",
    "\n",
    "mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[80] at parallelize at PythonRDD.scala:195"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# lets 'persist' this RDD with the default storage level (MEMORY_ONLY). \n",
    "# You can also check if the RDD is cached or not.\n",
    "\n",
    "words_RDD.cache() \n",
    "\n",
    "caching = words_RDD.persist().is_cached \n",
    "\n",
    "# did your RDD get cached ? \n",
    "caching\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class pyspark.StorageLevel(useDisk, useMemory, useOffHeap, deserialized, replication = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# read in a random README.md file i have locally on my laptop\n",
    "\n",
    "# creating a RDD from this base input txt \n",
    "\n",
    "raw_data_RDD = sc.textFile(\"e://README_spark.md\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e://README_spark.md MapPartitionsRDD[89] at textFile at NativeMethodAccessorImpl.java:0\n"
     ]
    }
   ],
   "source": [
    "print(raw_data_RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(type(raw_data_RDD))\n",
    "\n",
    "# interesting, its a pyspark RDD, you can see it here \n",
    "# good command to use to confirm in general \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# Apache Spark',\n",
       " '',\n",
       " 'Spark is a unified analytics engine for large-scale data processing. It provides',\n",
       " 'high-level APIs in Scala, Java, Python, and R, and an optimized engine that',\n",
       " 'supports general computation graphs for data analysis. It also supports a',\n",
       " 'rich set of higher-level tools including Spark SQL for SQL and DataFrames,',\n",
       " 'MLlib for machine learning, GraphX for graph processing,',\n",
       " 'and Structured Streaming for stream processing.',\n",
       " '',\n",
       " '<https://spark.apache.org/>',\n",
       " '',\n",
       " '[![Jenkins Build](https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-sbt-hadoop-2.7/badge/icon)](https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-sbt-hadoop-2.7)',\n",
       " '[![AppVeyor Build](https://img.shields.io/appveyor/ci/ApacheSoftwareFoundation/spark/master.svg?style=plastic&logo=appveyor)](https://ci.appveyor.com/project/ApacheSoftwareFoundation/spark)',\n",
       " '[![PySpark Coverage](https://img.shields.io/badge/dynamic/xml.svg?label=pyspark%20coverage&url=https%3A%2F%2Fspark-test.github.io%2Fpyspark-coverage-site&query=%2Fhtml%2Fbody%2Fdiv%5B1%5D%2Fdiv%2Fh1%2Fspan&colorB=brightgreen&style=plastic)](https://spark-test.github.io/pyspark-coverage-site)',\n",
       " '',\n",
       " '',\n",
       " '## Online Documentation',\n",
       " '',\n",
       " 'You can find the latest Spark documentation, including a programming',\n",
       " 'guide, on the [project web page](https://spark.apache.org/documentation.html).']"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# return the first 20 line entries of my README file\n",
    "\n",
    "raw_data_RDD.take(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# if you wanted to return the entire file lines, just .collect() command\n",
    "\n",
    "# raw_data_RDD.collect()\n",
    "# you can uncomment, takes up a big space \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.141553696\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# i'm going to start with something simple like a billion samples\n",
    "# if you are running this on your laptop, you will hear a silent scream, check out your CPU\n",
    "num_samples = 1000000000\n",
    "\n",
    "def inside(p):     \n",
    "  x, y = random.random(), random.random()\n",
    "  return x*x + y*y < 1\n",
    "\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "\n",
    "pi = 4 * count / num_samples\n",
    "\n",
    "print(pi)\n",
    "\n",
    "# sc.stop()  < - im going to keep sc up here \n",
    "# you will see a number you recognize... \n",
    "# i did stop the sc context, ill have to re-enable to keep going...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark import SparkConf\n",
    "# from pyspark import SparkContext\n",
    "# conf = SparkConf()\n",
    "# # conf.setMaster('spark://HEAD_NODE_HOSTNAME:7077') <- example \n",
    "# conf.setAppName('spark-basic')\n",
    "# sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating(user=1, product=2, rating=5.0)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.recommendation import Rating \n",
    "r = Rating(user = 1, product = 2, rating = 5.0)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print the tables in the catalog\n",
    "print(my_spark.catalog.listTables())\n",
    "\n",
    "#  example:\n",
    "#  [Table(name='flights', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pd_temp\n",
    "pd_temp = pd.DataFrame(np.random.random(10))\n",
    "\n",
    "# Create spark_temp from pd_temp\n",
    "spark_temp = my_spark.createDataFrame(pd_temp)\n",
    "\n",
    "# Examine the tables in the catalog\n",
    "print(my_spark.catalog.listTables())\n",
    "\n",
    "# Add spark_temp to the catalog\n",
    "spark_temp.createOrReplaceTempView(\"temp\")\n",
    "\n",
    "# Examine the tables in the catalog again\n",
    "print(my_spark.catalog.listTables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default configurations\n",
    "my_spark.sparkContext._conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####  complete overview of your Spark environment and configuration\n",
    "\n",
    "# for item in sorted(sc._conf.getAll()): print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####  to create a spark session\n",
    "\n",
    "#    spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# for item in sorted(os.environ.items()): print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  logger.info(ssc.sparkContext.getConf.getAll.mkString(\"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-d250789423b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrdd2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"e://iris.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "rdd2 = sc.textFile(\"e://iris.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rdd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rdd2.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the version of SparkContext\n",
    "print(\"The version of Spark Context in the PySpark shell is\", sc.version)\n",
    "\n",
    "# Print the Python version of SparkContext\n",
    "print(\"The Python version of Spark Context in the PySpark shell is\", sc.pythonVer)\n",
    "\n",
    "# Print the master of SparkContext\n",
    "print(\"The master of Spark Context in the PySpark shell is\", sc.master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a python list named numb \n",
    "numb = range(1, 101)\n",
    "\n",
    "# Load the list into PySpark\n",
    "spark_data = sc.parallelize(numb)\n",
    "print(spark_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_data.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in sorted(sc._conf.getAll()): print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in sorted(sc._conf.getAll()): print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distData = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from numpy import array\n",
    "from math import sqrt\n",
    "# $example off$\n",
    "\n",
    "from pyspark import SparkContext\n",
    "# $example on$\n",
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "# $example off$\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sc = SparkContext(appName=\"KMeansExample\")  # SparkContext\n",
    "\n",
    "    # $example on$\n",
    "    # Load and parse the data\n",
    "    data = sc.textFile(\"e://kmeans_data.txt\")\n",
    "    parsedData = data.map(lambda line: array([float(x) for x in line.split(' ')]))\n",
    "\n",
    "    # Build the model (cluster the data)\n",
    "    clusters = KMeans.train(parsedData, 2, maxIterations=10, initializationMode=\"random\")\n",
    "\n",
    "    # Evaluate clustering by computing Within Set Sum of Squared Errors\n",
    "    def error(point):\n",
    "        center = clusters.centers[clusters.predict(point)]\n",
    "        return sqrt(sum([x**2 for x in (point - center)]))\n",
    "\n",
    "    WSSSE = parsedData.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
    "    print(\"Within Set Sum of Squared Error = \" + str(WSSSE))\n",
    "\n",
    "    # Save and load model\n",
    "    # clusters.save(sc, \"e://KMeansModel\")\n",
    "    # sameModel = KMeansModel.load(sc, \"e://KMeansModel\")\n",
    "    # $example off$\n",
    "\n",
    "    # sc.stop()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Spark's core data structure is the Resilient Distributed Dataset (RDD). This is a low level object that lets Spark work its magic by splitting data across multiple nodes in the cluster. However, RDDs are hard to work with directly, so in this course you'll be using the Spark DataFrame abstraction built on top of RDDs.\n",
    "\n",
    "The Spark DataFrame was designed to behave a lot like a SQL table (a table with variables in the columns and observations in the rows).  Not only are they easier to understand, DataFrames are also more optimized for complicated operations than RDDs.\n",
    "\n",
    "When you start modifying and combining columns and rows of data, there are many ways to arrive at the same result, but some often take much longer than others. When using RDDs, it's up to the data scientist to figure out the right way to optimize the query, but the DataFrame implementation has much of this optimization built in!\n",
    "\n",
    "To start working with Spark DataFrames, you first have to create a SparkSession object from your SparkContext. You can think of the SparkContext as your connection to the cluster and the SparkSession as your interface with that connection.\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "We've already created a SparkSession for you called spark, but what if you're not sure there already is one? Creating multiple SparkSessions and SparkContexts can cause issues, so it's best practice to use the SparkSession.builder.getOrCreate() method. This returns an existing SparkSession if there's already one in the environment, or creates a new one if necessary!\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkSession from pyspark.sql.\n",
    "# Make a new SparkSession called my_spark using SparkSession.builder.getOrCreate().\n",
    "# Print my_spark to the console to verify it's a SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Import SparkSession from pyspark.sql\n",
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# # Create my_spark\n",
    "# my_spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# # Print my_spark\n",
    "# print(my_spark)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# OUTPUT:\n",
    "# <script.py> output:\n",
    "#     <pyspark.sql.session.SparkSession object at 0x7f337ecf9198>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Once you've created a SparkSession, you can start poking around to see what data is in your cluster!\n",
    "\n",
    "# Your SparkSession has an attribute called catalog which lists all the data inside the cluster. This attribute has a few methods for extracting different pieces of information.\n",
    "\n",
    "# One of the most useful is the .listTables() method, which returns the names of all the tables in your cluster as a list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  what tables are in my cluster ? \n",
    "#  Print the tables in the catalog\n",
    "#   print(spark.catalog.listTables())\n",
    "\n",
    "\n",
    "# OUTPUT:\n",
    "# <script.py> output:\n",
    "#    [Table(name='flights', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# help(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  MLlib Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.recommendation import Rating \n",
    "r = Rating(user = 1, product = 2, rating = 5.0)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(r[0],r[1],r[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "\n",
    "training, test=data.randomSplit([0.6, 0.4])\n",
    "\n",
    "training.collect()\n",
    "\n",
    "test.collect()\n",
    "\n",
    "#  [1,7,8,10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "r1 = Rating(1, 1, 1.0)\n",
    "r2 = Rating(1, 2, 2.0)\n",
    "r3 = Rating(2, 1, 2.0)\n",
    "\n",
    "ratings = sc.parallelize([r1, r2, r3])\n",
    "ratings.collect()\n",
    "model = ALS.train(ratings, rank=10, iterations=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "unrated_RDD = sc.parallelize([(1, 2), (1, 1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "r1 = Rating(1, 1, 1.0)\n",
    "r2 = Rating(1, 2, 2.0)\n",
    "r3 = Rating(2, 1, 2.0)\n",
    "\n",
    "ratings = sc.parallelize([r1, r2, r3])\n",
    "\n",
    "ratings.collect()\n",
    "\n",
    "model = ALS.train(ratings, rank=10, iterations=10)\n",
    "\n",
    "unrated_RDD = sc.parallelize([(1, 2), (1, 1)])\n",
    "\n",
    "predictions = model.predictAll(unrated_RDD)\n",
    "\n",
    "predictions.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rates = ratings.map(lambda x: ((x[0], x[1]), x[2]))\n",
    "rates.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Load the data into RDD\n",
    "# data = sc.textFile(file_path)\n",
    "\n",
    "# # Split the RDD \n",
    "# ratings = data.map(lambda l: l.split(','))\n",
    "\n",
    "# # Transform the ratings RDD\n",
    "# ratings_final = ratings.map(lambda line: Rating(int(line[0]), int(line[1]), float(line[2])))\n",
    "\n",
    "# # Split the data into training and test\n",
    "# training_data, test_data = ratings_final.randomSplit([0.8, 0.2])\n",
    "\n",
    "# # Create the ALS model on the training data\n",
    "# model = ALS.train(training_data, rank=10, iterations=10)\n",
    "\n",
    "# # Drop the ratings column \n",
    "# testdata_no_rating = test_data.map(lambda p: (p[0], p[1]))\n",
    "\n",
    "# # Predict the model  \n",
    "# predictions = model.predictAll(testdata_no_rating)\n",
    "\n",
    "# # Print the first rows of the RDD\n",
    "# predictions.take(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning file download with py requests ...... [ STARTING ]\n",
      "\n",
      "200 is good if you see it here ........ [ 200 ]\n",
      "Content-Type:   image/jpeg\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "print('Beginning file download with py requests ...... [ STARTING ]')\n",
    "\n",
    "my_url = 'http://i3.ytimg.com/vi/J---aiyznGQ/mqdefault.jpg'  \n",
    "\n",
    "r = requests.get(my_url)\n",
    "\n",
    "with open('e://cat3.jpg', 'wb') as f:  \n",
    "    f.write(r.content)\n",
    "\n",
    "# Retrieving HTTP meta-data\n",
    "print(\"\\n200 is good if you see it here ........ [\",r.status_code,\"]\")  \n",
    "print(\"Content-Type:  \", r.headers['content-type'])  \n",
    "print(r.encoding)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
