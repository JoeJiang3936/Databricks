{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading VCF files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Generation Genome Sequencing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Tom Bresee*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.  &nbsp;  Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running pyspark etc on windows 10 terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load basic libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import random\n",
    "from operator import add\n",
    "from operator import add\n",
    "from time import time\n",
    "import os\n",
    "import time\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import sys\n",
    "from random import random\n",
    "from operator import add\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import more libraries\n",
    "import pyspark\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "# ALS\n",
    "from pyspark.mllib.recommendation import ALS\n",
    "# Logistic Regression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "# Kmeans\n",
    "from pyspark.mllib.clustering import KMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# find the spark \n",
    "import findspark\n",
    "#findspark.find()\n",
    "findspark.init()\n",
    "# i dont know if you HAVE to have this, but it seems to help alot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.engine import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x231f64d8fd0>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=reading-vcf-application, master=local[*]) created by __init__ at <ipython-input-5-97ede4356bd5>:7 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-97-bdd600e1e2b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# conf.setMaster('spark://HEAD_NODE_HOSTNAME:7077') <- example\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetAppName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'reading-vcf-application'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    131\u001b[0m                     \" note this option will be removed in Spark 3.0\")\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    330\u001b[0m                         \u001b[1;34m\" created by %s at %s:%s \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[1;32m--> 332\u001b[1;33m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[0;32m    333\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=reading-vcf-application, master=local[*]) created by __init__ at <ipython-input-5-97ede4356bd5>:7 "
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import Row\n",
    "\n",
    "conf = SparkConf()\n",
    "# conf.setMaster('spark://HEAD_NODE_HOSTNAME:7077') <- example \n",
    "conf.setAppName('reading-vcf-application')\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "\n",
    "#################################################################\n",
    "# this is how they seem to prefer to turn it up:\n",
    "\n",
    "#    spark = SparkSession.builder.appName(\"PythonPi\").getOrCreate()\n",
    "#################################################################\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.  &nbsp; This is the command you start with on Windows 10 to get things going"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# now we are going into SparksQL arena !\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sqlSparkContext = SQLContext(sc)\n",
    "\n",
    "# confirm if this is the preferred way of turning things up\n",
    "# spark = SparkSession \\\n",
    "#     .builder \\\n",
    "#     .appName(\"Python Spark SQL basic example\") \\\n",
    "#     .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "#     .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.75.1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>reading-vcf-application</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=reading-vcf-application>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# main sc context (main)\n",
    "sc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.SQLContext at 0x231f63055f8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# this is my guy\n",
    "sqlSparkContext\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. &nbsp;  Begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inferring the Schema\n",
    "With a SQLContext, we are ready to create a DataFrame from our existing RDD. But first we need to tell Spark SQL the schema in our data.  Spark SQL can convert an RDD of Row objects to a DataFrame. Rows are constructed by passing a list of key/value pairs as kwargs to the Row class. The keys define the column names, and the types are inferred by looking at the first row. Therefore, it is important that there is no missing data in the first row of the RDD in order to properly infer the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# spark is an existing SparkSession\n",
    "\n",
    "path = 'C:/SPARK/examples/src/main/resources/people.json'\n",
    "\n",
    "df = sqlSparkContext.read.json(path) \n",
    "\n",
    "# A JSON dataset is pointed to by path.\n",
    "# The path can be either a single text file or a directory storing text files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  https://spark.apache.org/docs/latest/sql-data-sources-json.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# The inferred schema can be visualized using the printSchema() method\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.context.SQLContext'>\n",
      "<class 'pyspark.context.SparkContext'>\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(type(sqlSparkContext))\n",
    "print(type(sc))\n",
    "print(type(df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D.  &nbsp;  SQL go "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|name|\n",
      "+----+\n",
      "|Andy|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Creates a temporary view using the DataFrame\n",
    "df.createOrReplaceTempView(\"people\")  # in my case would be df.createOrReplace\n",
    "\n",
    "# SQL statements can be run by using the sql methods provided by spark\n",
    "df_andy = sqlSparkContext.sql(\"SELECT name FROM people WHERE age BETWEEN 20 and 40\")\n",
    "df_andy.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_michael = sqlSparkContext.sql(\"SELECT name FROM people WHERE name = 'Michael'\")\n",
    "df_michael.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_conf',\n",
       " '_inferSchema',\n",
       " '_instantiatedContext',\n",
       " '_jsc',\n",
       " '_jsqlContext',\n",
       " '_jvm',\n",
       " '_sc',\n",
       " '_ssql_ctx',\n",
       " 'cacheTable',\n",
       " 'clearCache',\n",
       " 'createDataFrame',\n",
       " 'createExternalTable',\n",
       " 'dropTempTable',\n",
       " 'getConf',\n",
       " 'getOrCreate',\n",
       " 'newSession',\n",
       " 'range',\n",
       " 'read',\n",
       " 'readStream',\n",
       " 'registerDataFrameAsTable',\n",
       " 'registerFunction',\n",
       " 'registerJavaFunction',\n",
       " 'setConf',\n",
       " 'sparkSession',\n",
       " 'sql',\n",
       " 'streams',\n",
       " 'table',\n",
       " 'tableNames',\n",
       " 'tables',\n",
       " 'udf',\n",
       " 'uncacheTable']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dir(sqlSparkContext)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#---  REFERENCE - READING OPTIONS  ---\n",
    "\n",
    "# dir(sqlSparkContext.read)\n",
    "\n",
    "#  'csv',\n",
    "#  'format',\n",
    "#  'jdbc',\n",
    "#  'json',\n",
    "#  'load',\n",
    "#  'option',\n",
    "#  'options',\n",
    "#  'orc',\n",
    "#  'parquet',\n",
    "#  'schema',\n",
    "#  'table',\n",
    "#  'text']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[database: string, tableName: string, isTemporary: boolean]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sqlSparkContext.tables()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method SQLContext.tables of <pyspark.sql.context.SQLContext object at 0x00000231F63055F8>>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sqlSparkContext.tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__func__',\n",
       " '__ge__',\n",
       " '__get__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__self__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dir(sqlSparkContext.sql)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E.  &nbsp; Go Off "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_t = sqlSparkContext.read.load(\"C:/SPARK/examples/src/main/resources/users.parquet\")\n",
    "\n",
    "print(type(df_t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agg\n",
      "alias\n",
      "approxQuantile\n",
      "cache\n",
      "checkpoint\n",
      "coalesce\n",
      "colRegex\n",
      "collect\n",
      "columns\n",
      "corr\n",
      "count\n",
      "cov\n",
      "createGlobalTempView\n",
      "createOrReplaceGlobalTempView\n",
      "createOrReplaceTempView\n",
      "createTempView\n",
      "crossJoin\n",
      "crosstab\n",
      "cube\n",
      "describe\n",
      "distinct\n",
      "drop\n",
      "dropDuplicates\n",
      "drop_duplicates\n",
      "dropna\n",
      "dtypes\n",
      "exceptAll\n",
      "explain\n",
      "fillna\n",
      "filter\n",
      "first\n",
      "foreach\n",
      "foreachPartition\n",
      "freqItems\n",
      "groupBy\n",
      "groupby\n",
      "head\n",
      "hint\n",
      "intersect\n",
      "intersectAll\n",
      "isLocal\n",
      "isStreaming\n",
      "is_cached\n",
      "join\n",
      "limit\n",
      "localCheckpoint\n",
      "na\n",
      "orderBy\n",
      "persist\n",
      "printSchema\n",
      "randomSplit\n",
      "rdd\n",
      "registerTempTable\n",
      "repartition\n",
      "repartitionByRange\n",
      "replace\n",
      "rollup\n",
      "sample\n",
      "sampleBy\n",
      "schema\n",
      "select\n",
      "selectExpr\n",
      "show\n",
      "sort\n",
      "sortWithinPartitions\n",
      "sql_ctx\n",
      "stat\n",
      "storageLevel\n",
      "subtract\n",
      "summary\n",
      "take\n",
      "toDF\n",
      "toJSON\n",
      "toLocalIterator\n",
      "toPandas\n",
      "union\n",
      "unionAll\n",
      "unionByName\n",
      "unpersist\n",
      "where\n",
      "withColumn\n",
      "withColumnRenamed\n",
      "withWatermark\n",
      "write\n",
      "writeStream\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#---  REFERENCE - your option methods for this DF concept  ---\n",
    "\n",
    "for i in dir(df_t):\n",
    "    if not i.startswith(\"_\"):\n",
    "        print(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A DataFrame is a distributed collection of data, which is organized into named columns. Conceptually, it is equivalent to relational tables with good optimization techniques.\n",
    "\n",
    "A DataFrame can be constructed from an array of different sources such as Hive tables, Structured Data files, external databases, or existing RDDs. This API was designed for modern Big Data and data science applications taking inspiration from DataFrame in R Programming and Pandas in Python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "Features of DataFrame\n",
    "\n",
    "Here is a set of few characteristic features of DataFrame\n",
    "\n",
    "Ability to process the data in the size of Kilobytes to Petabytes on a single node cluster to large cluster.\n",
    "\n",
    "Supports different data formats (Avro, csv, elastic search, and Cassandra) and storage systems (HDFS, HIVE tables, mysql, etc).\n",
    "\n",
    "State of art optimization and code generation through the Spark SQL Catalyst optimizer (tree transformation framework).\n",
    "\n",
    "Can be easily integrated with all Big Data tools and frameworks via Spark-Core.\n",
    "\n",
    "Provides API for Python, Java, Scala, and R Programming.\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Spark introduces a programming module for structured data processing called Spark SQL. It provides a programming abstraction called DataFrame and can act as distributed SQL query engine.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "The following command is used for initializing the SparkContext through spark-shell.\n",
    "\n",
    "$ spark-shell\n",
    "By default, the SparkContext object is initialized with the name sc when the spark-shell starts.\n",
    "\n",
    "Use the following command to create SQLContext.\n",
    "\n",
    "scala> val sqlcontext = new org.apache.spark.sql.SQLContext(sc)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* some data files \n",
    "   *  https://github.com/apache/spark/tree/master/examples/src/main/resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "michael = sqlSparkContext.sql(\"SELECT name FROM people WHERE name = 'Michael'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.75.1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>reading-vcf-application</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=reading-vcf-application>"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['df_t', 'df_michael', 'df_andy', 'df', '_45']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# ---  REFERENCE - list out the dataframes currently created ! --- \n",
    "\n",
    "def list_dataframes():\n",
    "    from pyspark.sql import DataFrame\n",
    "    return [k for (k, v) in globals().items() if isinstance(v, DataFrame)]\n",
    "\n",
    "list_dataframes()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, salary: bigint]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|   name|salary|\n",
      "+-------+------+\n",
      "|Michael|  3000|\n",
      "|   Andy|  4500|\n",
      "| Justin|  3500|\n",
      "|  Berta|  4000|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "path = 'C:\\SPARK\\examples\\src\\main\\python\\employees.json'\n",
    "\n",
    "df_emp = sqlSparkContext.read.json(path)    \n",
    "\n",
    "df_emp\n",
    "df_emp.show()\n",
    "\n",
    "# scala> val dfs = sqlContext.read.json(\"employee.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['df_t', '___', 'df_michael', 'df_andy', 'df', 'df_emp', '_60', '__', '_45', '_58']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# how many DF do i have right now ? \n",
    "print(list_dataframes())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_emp.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "|  Berta|\n",
      "+-------+\n",
      "\n",
      "+------+\n",
      "|salary|\n",
      "+------+\n",
      "|  3000|\n",
      "|  4500|\n",
      "|  3500|\n",
      "|  4000|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# direct call ! ! ! \n",
    "\n",
    "df_emp.select(\"name\").show()\n",
    "\n",
    "df_emp.select(\"salary\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|salary|count|\n",
      "+------+-----+\n",
      "|  4500|    1|\n",
      "|  4000|    1|\n",
      "|  3500|    1|\n",
      "|  3000|    1|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# group by salary \n",
    "df_emp.groupBy(\"salary\").count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|  name|salary|\n",
      "+------+------+\n",
      "|  Andy|  4500|\n",
      "|Justin|  3500|\n",
      "| Berta|  4000|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_emp.filter(df_emp[\"salary\"] > 3200).show()\n",
    "# interesting:  scala uses () but python uses []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# peopleDF = spark.read.json(\"examples/src/main/resources/people.json\")\n",
    "\n",
    "# # DataFrames can be saved as Parquet files, maintaining the schema information.\n",
    "# peopleDF.write.parquet(\"people.parquet\")\n",
    "\n",
    "# # Read in the Parquet file created above.\n",
    "# # Parquet files are self-describing so the schema is preserved.\n",
    "# # The result of loading a parquet file is also a DataFrame.\n",
    "# parquetFile = spark.read.parquet(\"people.parquet\")\n",
    "\n",
    "# # Parquet files can also be used to create a temporary view and then used in SQL statements.\n",
    "# parquetFile.createOrReplaceTempView(\"parquetFile\")\n",
    "# teenagers = spark.sql(\"SELECT name FROM parquetFile WHERE age >= 13 AND age <= 19\")\n",
    "# teenagers.show()\n",
    "# # +------+\n",
    "# # |  name|\n",
    "# # +------+\n",
    "# # |Justin|\n",
    "# # +------+\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_emp.write.parquet(\"C:/SPARK/examples/src/main/resources/tom_emp.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F.  &nbsp;  Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Parquet is a columnar format, supported by many data processing systems. The advantages of having a columnar storage are as follows −\n",
    "\n",
    "Columnar storage limits IO operations.\n",
    "\n",
    "Columnar storage can fetch specific columns that you need to access.\n",
    "\n",
    "Columnar storage consumes less space.\n",
    "\n",
    "Columnar storage gives better-summarized data and follows type-specific encoding.\n",
    "\n",
    "Spark SQL provides support for both reading and writing parquet files that automatically capture the schema of the original data. Like JSON datasets, parquet files follow the same procedure.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "# $example on:schema_merging$\n",
    "from pyspark.sql import Row\n",
    "# $example off:schema_merging$\n",
    "\n",
    "# $example on:generic_load_save_functions$\n",
    "df_users = sqlSparkContext.read.load(\"C:/SPARK/examples/src/main/resources/users.parquet\")\n",
    "# df.select(\"name\", \"favorite_color\").write.save(\"namesAndFavColors.parquet\")\n",
    "# $example off:generic_load_save_functions$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "peopleDF = spark.read.json(\"examples/src/main/resources/people.json\")\n",
    "\n",
    "# DataFrames can be saved as Parquet files, maintaining the schema information.\n",
    "peopleDF.write.parquet(\"people.parquet\")\n",
    "\n",
    "# Read in the Parquet file created above.\n",
    "# Parquet files are self-describing so the schema is preserved.\n",
    "# The result of loading a parquet file is also a DataFrame.\n",
    "parquetFile = spark.read.parquet(\"people.parquet\")\n",
    "\n",
    "# Parquet files can also be used to create a temporary view and then used in SQL statements.\n",
    "parquetFile.createOrReplaceTempView(\"parquetFile\")\n",
    "teenagers = spark.sql(\"SELECT name FROM parquetFile WHERE age >= 13 AND age <= 19\")\n",
    "teenagers.show()\n",
    "# +------+\n",
    "# |  name|\n",
    "# +------+\n",
    "# |Justin|\n",
    "# +------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from os import walk\n",
    "# from pyspark.sql import SQLContext\n",
    "\n",
    "# sc = SparkContext.getOrCreate()\n",
    "# sqlContext = SQLContext(sc)\n",
    "\n",
    "# parquetdir = r'C:\\PATH\\TO\\YOUR\\PARQUET\\FILES'\n",
    "\n",
    "# # Getting all parquet files in a dir as spark contexts.\n",
    "# # There might be more easy ways to access single parquets, but I had nested dirs\n",
    "# dirpath, dirnames, filenames = next(walk(parquetdir), (None, [], []))\n",
    "\n",
    "# # for each parquet file, i.e. table in our database, spark creates a tempview with\n",
    "# # the respective table name equal the parquet filename\n",
    "# print('New tables available: \\n')\n",
    "\n",
    "# for parquet in filenames:\n",
    "#     print(parquet[:-8])\n",
    "#     spark.read.parquet(parquetdir+'\\\\'+parquet).createOrReplaceTempView(parquet[:-8])\n",
    "    \n",
    "# my_test_query = spark.sql(\"\"\"\n",
    "# select\n",
    "#   field1,\n",
    "#   field2\n",
    "# from parquetfilename1\n",
    "# where\n",
    "#   field1 = 'something'\n",
    "# \"\"\")\n",
    "\n",
    "# my_test_query.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "squaresDF = sqlSparkContext.createDataFrame(sc.parallelize(range(1, 6)).map(lambda i: Row(single=i, double=i ** 2)))\n",
    "\n",
    "squaresDF.write.parquet(\"data/test_table/key=1\")\n",
    "\n",
    "# Create another DataFrame in a new partition directory,\n",
    "# adding a new column and dropping an existing column\n",
    "cubesDF = sqlSparkContext.createDataFrame(sc.parallelize(range(6, 11)).map(lambda i: Row(single=i, triple=i ** 3)))\n",
    "\n",
    "cubesDF.write.parquet(\"C:/SPARK/key=2\")\n",
    "\n",
    "# Read the partitioned table\n",
    "mergedDF = sqlSparkContext.read.option(\"mergeSchema\", \"true\").parquet(\"C:/SPARKata/test_table\")\n",
    "mergedDF.printSchema()\n",
    "\n",
    "# The final schema consists of all 3 columns in the Parquet files together\n",
    "# with the partitioning column appeared in the partition directory paths.\n",
    "# root\n",
    "#  |-- double: long (nullable = true)\n",
    "#  |-- single: long (nullable = true)\n",
    "#  |-- triple: long (nullable = true)\n",
    "#  |-- key: integer (nullable = true)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# .catalog.listTables()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # raw_data_RDD = sc.textFile(\"e://README_spark.md\")  \n",
    "\n",
    "# lines = sc.read.text(\"e://README_spark.md\").rdd.map(lambda r: r[0])\n",
    "\n",
    "# sortedCount = lines.flatMap(lambda x: x.split(' ')).map(lambda x: (int(x), 1)).sortByKey()\n",
    "\n",
    "# # This is just a demo on how to bring all the sorted data back to a single node.\n",
    "\n",
    "# # In reality, we wouldn't want to collect all the data to the driver node.\n",
    "\n",
    "# output = sortedCount.collect()\n",
    "\n",
    "# for (num, unitcount) in output:\n",
    "#         print(num)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# scala \n",
    "\n",
    "# spark: SparkSession = // create the Spark Session\n",
    "# val df = spark.read.csv(\"file.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#\n",
    "# Licensed to the Apache Software Foundation (ASF) under one or more\n",
    "# contributor license agreements.  See the NOTICE file distributed with\n",
    "# this work for additional information regarding copyright ownership.\n",
    "# The ASF licenses this file to You under the Apache License, Version 2.0\n",
    "# (the \"License\"); you may not use this file except in compliance with\n",
    "# the License.  You may obtain a copy of the License at\n",
    "#\n",
    "#    http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#\n",
    "\n",
    "\"\"\"\n",
    "A simple example demonstrating basic Spark SQL features.\n",
    "Run with:\n",
    "  ./bin/spark-submit examples/src/main/python/sql/basic.py\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "# $example on:init_session$\n",
    "from pyspark.sql import SparkSession\n",
    "# $example off:init_session$\n",
    "\n",
    "# $example on:schema_inferring$\n",
    "from pyspark.sql import Row\n",
    "# $example off:schema_inferring$\n",
    "\n",
    "# $example on:programmatic_schema$\n",
    "# Import data types\n",
    "from pyspark.sql.types import *\n",
    "# $example off:programmatic_schema$\n",
    "\n",
    "\n",
    "def basic_df_example(spark):\n",
    "    # $example on:create_df$\n",
    "    # spark is an existing SparkSession\n",
    "    df = spark.read.json(\"examples/src/main/resources/people.json\")\n",
    "    # Displays the content of the DataFrame to stdout\n",
    "    df.show()\n",
    "    # +----+-------+\n",
    "    # | age|   name|\n",
    "    # +----+-------+\n",
    "    # |null|Michael|\n",
    "    # |  30|   Andy|\n",
    "    # |  19| Justin|\n",
    "    # +----+-------+\n",
    "    # $example off:create_df$\n",
    "\n",
    "    # $example on:untyped_ops$\n",
    "    # spark, df are from the previous example\n",
    "    # Print the schema in a tree format\n",
    "    df.printSchema()\n",
    "    # root\n",
    "    # |-- age: long (nullable = true)\n",
    "    # |-- name: string (nullable = true)\n",
    "\n",
    "    # Select only the \"name\" column\n",
    "    df.select(\"name\").show()\n",
    "    # +-------+\n",
    "    # |   name|\n",
    "    # +-------+\n",
    "    # |Michael|\n",
    "    # |   Andy|\n",
    "    # | Justin|\n",
    "    # +-------+\n",
    "\n",
    "    # Select everybody, but increment the age by 1\n",
    "    df.select(df['name'], df['age'] + 1).show()\n",
    "    # +-------+---------+\n",
    "    # |   name|(age + 1)|\n",
    "    # +-------+---------+\n",
    "    # |Michael|     null|\n",
    "    # |   Andy|       31|\n",
    "    # | Justin|       20|\n",
    "    # +-------+---------+\n",
    "\n",
    "    # Select people older than 21\n",
    "    df.filter(df['age'] > 21).show()\n",
    "    # +---+----+\n",
    "    # |age|name|\n",
    "    # +---+----+\n",
    "    # | 30|Andy|\n",
    "    # +---+----+\n",
    "\n",
    "    # Count people by age\n",
    "    df.groupBy(\"age\").count().show()\n",
    "    # +----+-----+\n",
    "    # | age|count|\n",
    "    # +----+-----+\n",
    "    # |  19|    1|\n",
    "    # |null|    1|\n",
    "    # |  30|    1|\n",
    "    # +----+-----+\n",
    "    # $example off:untyped_ops$\n",
    "\n",
    "    # $example on:run_sql$\n",
    "    # Register the DataFrame as a SQL temporary view\n",
    "    df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "    sqlDF = spark.sql(\"SELECT * FROM people\")\n",
    "    sqlDF.show()\n",
    "    # +----+-------+\n",
    "    # | age|   name|\n",
    "    # +----+-------+\n",
    "    # |null|Michael|\n",
    "    # |  30|   Andy|\n",
    "    # |  19| Justin|\n",
    "    # +----+-------+\n",
    "    # $example off:run_sql$\n",
    "\n",
    "    # $example on:global_temp_view$\n",
    "    # Register the DataFrame as a global temporary view\n",
    "    df.createGlobalTempView(\"people\")\n",
    "\n",
    "    # Global temporary view is tied to a system preserved database `global_temp`\n",
    "    spark.sql(\"SELECT * FROM global_temp.people\").show()\n",
    "    # +----+-------+\n",
    "    # | age|   name|\n",
    "    # +----+-------+\n",
    "    # |null|Michael|\n",
    "    # |  30|   Andy|\n",
    "    # |  19| Justin|\n",
    "    # +----+-------+\n",
    "\n",
    "    # Global temporary view is cross-session\n",
    "    spark.newSession().sql(\"SELECT * FROM global_temp.people\").show()\n",
    "    # +----+-------+\n",
    "    # | age|   name|\n",
    "    # +----+-------+\n",
    "    # |null|Michael|\n",
    "    # |  30|   Andy|\n",
    "    # |  19| Justin|\n",
    "    # +----+-------+\n",
    "    # $example off:global_temp_view$\n",
    "\n",
    "\n",
    "def schema_inference_example(spark):\n",
    "    # $example on:schema_inferring$\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "    # Load a text file and convert each line to a Row.\n",
    "    lines = sc.textFile(\"examples/src/main/resources/people.txt\")\n",
    "    parts = lines.map(lambda l: l.split(\",\"))\n",
    "    people = parts.map(lambda p: Row(name=p[0], age=int(p[1])))\n",
    "\n",
    "    # Infer the schema, and register the DataFrame as a table.\n",
    "    schemaPeople = spark.createDataFrame(people)\n",
    "    schemaPeople.createOrReplaceTempView(\"people\")\n",
    "\n",
    "    # SQL can be run over DataFrames that have been registered as a table.\n",
    "    teenagers = spark.sql(\"SELECT name FROM people WHERE age >= 13 AND age <= 19\")\n",
    "\n",
    "    # The results of SQL queries are Dataframe objects.\n",
    "    # rdd returns the content as an :class:`pyspark.RDD` of :class:`Row`.\n",
    "    teenNames = teenagers.rdd.map(lambda p: \"Name: \" + p.name).collect()\n",
    "    for name in teenNames:\n",
    "        print(name)\n",
    "    # Name: Justin\n",
    "    # $example off:schema_inferring$\n",
    "\n",
    "\n",
    "def programmatic_schema_example(spark):\n",
    "    # $example on:programmatic_schema$\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "    # Load a text file and convert each line to a Row.\n",
    "    lines = sc.textFile(\"examples/src/main/resources/people.txt\")\n",
    "    parts = lines.map(lambda l: l.split(\",\"))\n",
    "    # Each line is converted to a tuple.\n",
    "    people = parts.map(lambda p: (p[0], p[1].strip()))\n",
    "\n",
    "    # The schema is encoded in a string.\n",
    "    schemaString = \"name age\"\n",
    "\n",
    "    fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\n",
    "    schema = StructType(fields)\n",
    "\n",
    "    # Apply the schema to the RDD.\n",
    "    schemaPeople = spark.createDataFrame(people, schema)\n",
    "\n",
    "    # Creates a temporary view using the DataFrame\n",
    "    schemaPeople.createOrReplaceTempView(\"people\")\n",
    "\n",
    "    # SQL can be run over DataFrames that have been registered as a table.\n",
    "    results = spark.sql(\"SELECT name FROM people\")\n",
    "\n",
    "    results.show()\n",
    "    # +-------+\n",
    "    # |   name|\n",
    "    # +-------+\n",
    "    # |Michael|\n",
    "    # |   Andy|\n",
    "    # | Justin|\n",
    "    # +-------+\n",
    "    # $example off:programmatic_schema$\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # $example on:init_session$\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Python Spark SQL basic example\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n",
    "    # $example off:init_session$\n",
    "\n",
    "    basic_df_example(spark)\n",
    "    schema_inference_example(spark)\n",
    "    programmatic_schema_example(spark)\n",
    "\n",
    "    spark.stop()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: bigint, name: string]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# $example on:init_session$\n",
    "from pyspark.sql import SparkSession\n",
    "# $example off:init_session$\n",
    "\n",
    "# $example on:schema_inferring$\n",
    "from pyspark.sql import Row\n",
    "# $example off:schema_inferring$\n",
    "\n",
    "# $example on:programmatic_schema$\n",
    "# Import data types\n",
    "from pyspark.sql.types import *\n",
    "# $example off:programmatic_schema$\n",
    "\n",
    "# READ IN JSON FILE ! \n",
    "df = sqlContext.read.json(\"C:/SPARK/examples/src/main/resources/people.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agg\n",
      "alias\n",
      "approxQuantile\n",
      "cache\n",
      "checkpoint\n",
      "coalesce\n",
      "colRegex\n",
      "collect\n",
      "columns\n",
      "corr\n",
      "count\n",
      "cov\n",
      "createGlobalTempView\n",
      "createOrReplaceGlobalTempView\n",
      "createOrReplaceTempView\n",
      "createTempView\n",
      "crossJoin\n",
      "crosstab\n",
      "cube\n",
      "describe\n",
      "distinct\n",
      "drop\n",
      "dropDuplicates\n",
      "drop_duplicates\n",
      "dropna\n",
      "dtypes\n",
      "exceptAll\n",
      "explain\n",
      "fillna\n",
      "filter\n",
      "first\n",
      "foreach\n",
      "foreachPartition\n",
      "freqItems\n",
      "groupBy\n",
      "groupby\n",
      "head\n",
      "hint\n",
      "intersect\n",
      "intersectAll\n",
      "isLocal\n",
      "isStreaming\n",
      "is_cached\n",
      "join\n",
      "limit\n",
      "localCheckpoint\n",
      "na\n",
      "orderBy\n",
      "persist\n",
      "printSchema\n",
      "randomSplit\n",
      "rdd\n",
      "registerTempTable\n",
      "repartition\n",
      "repartitionByRange\n",
      "replace\n",
      "rollup\n",
      "sample\n",
      "sampleBy\n",
      "schema\n",
      "select\n",
      "selectExpr\n",
      "show\n",
      "sort\n",
      "sortWithinPartitions\n",
      "sql_ctx\n",
      "stat\n",
      "storageLevel\n",
      "subtract\n",
      "summary\n",
      "take\n",
      "toDF\n",
      "toJSON\n",
      "toLocalIterator\n",
      "toPandas\n",
      "union\n",
      "unionAll\n",
      "unionByName\n",
      "unpersist\n",
      "where\n",
      "withColumn\n",
      "withColumnRenamed\n",
      "withWatermark\n",
      "write\n",
      "writeStream\n"
     ]
    }
   ],
   "source": [
    "for i in dir(df):\n",
    "    if not i.startswith(\"_\"):\n",
    "        print(i)\n",
    "        \n",
    "# agg\n",
    "# alias\n",
    "# approxQuantile\n",
    "# cache\n",
    "# checkpoint\n",
    "# coalesce\n",
    "# colRegex\n",
    "# collect\n",
    "# columns\n",
    "# corr\n",
    "# count\n",
    "# cov\n",
    "# createGlobalTempView\n",
    "# createOrReplaceGlobalTempView\n",
    "# createOrReplaceTempView\n",
    "# createTempView\n",
    "# crossJoin\n",
    "# crosstab\n",
    "# cube\n",
    "# describe\n",
    "# distinct\n",
    "# drop\n",
    "# dropDuplicates\n",
    "# drop_duplicates\n",
    "# dropna\n",
    "# dtypes\n",
    "# exceptAll\n",
    "# explain\n",
    "# fillna\n",
    "# filter\n",
    "# first\n",
    "# foreach\n",
    "# foreachPartition\n",
    "# freqItems\n",
    "# groupBy\n",
    "# groupby\n",
    "# head\n",
    "# hint\n",
    "# intersect\n",
    "# intersectAll\n",
    "# isLocal\n",
    "# isStreaming\n",
    "# is_cached\n",
    "# join\n",
    "# limit\n",
    "# localCheckpoint\n",
    "# na\n",
    "# orderBy\n",
    "# persist\n",
    "# printSchema\n",
    "# randomSplit\n",
    "# rdd\n",
    "# registerTempTable\n",
    "# repartition\n",
    "# repartitionByRange\n",
    "# replace\n",
    "# rollup\n",
    "# sample\n",
    "# sampleBy\n",
    "# schema\n",
    "# select\n",
    "# selectExpr\n",
    "# show\n",
    "# sort\n",
    "# sortWithinPartitions\n",
    "# sql_ctx\n",
    "# stat\n",
    "# storageLevel\n",
    "# subtract\n",
    "# summary\n",
    "# take\n",
    "# toDF\n",
    "# toJSON\n",
    "# toLocalIterator\n",
    "# toPandas\n",
    "# union\n",
    "# unionAll\n",
    "# unionByName\n",
    "# unpersist\n",
    "# where\n",
    "# withColumn\n",
    "# withColumnRenamed\n",
    "# withWatermark\n",
    "# write\n",
    "# writeStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#  people.json\n",
    "\n",
    "# {\"name\":\"Michael\"}\n",
    "# {\"name\":\"Andy\", \"age\":30}\n",
    "# {\"name\":\"Justin\", \"age\":19}\n",
    "\n",
    "# Displays the content of the DataFrame to stdout\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print the schema in a tree format\n",
    "df.printSchema()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Select only the \"name\" column\n",
    "df.select(\"name\").show()  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|   name|(age + 1)|\n",
      "+-------+---------+\n",
      "|Michael|     null|\n",
      "|   Andy|       31|\n",
      "| Justin|       20|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Select everybody, but increment the age by 1\n",
    "df.select(df['name'], df['age'] + 1).show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Select people older than 21\n",
    "\n",
    "df.filter(df['age'] > 21).show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| age|count|\n",
      "+----+-----+\n",
      "|  19|    1|\n",
      "|null|    1|\n",
      "|  30|    1|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Count people by age\n",
    "df.groupBy(\"age\").count().show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# $example on:run_sql$\n",
    "# Register the DataFrame as a SQL temporary view\n",
    "\n",
    "# Register the DataFrame as a SQL temporary view\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "sqlDF = sqlContext.sql(\"SELECT * FROM people\")\n",
    "sqlDF.show()  \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"Temporary view 'people' already exists;\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o74.createGlobalTempView.\n: org.apache.spark.sql.catalyst.analysis.TempTableAlreadyExistsException: Temporary view 'people' already exists;\r\n\tat org.apache.spark.sql.catalyst.catalog.GlobalTempViewManager.create(GlobalTempViewManager.scala:61)\r\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createGlobalTempView(SessionCatalog.scala:507)\r\n\tat org.apache.spark.sql.execution.command.CreateViewCommand.run(views.scala:149)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\r\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:194)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:79)\r\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3406)\r\n\tat org.apache.spark.sql.Dataset.createGlobalTempView(Dataset.scala:3114)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-91171b303457>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# $example on:global_temp_view$\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Register the DataFrame as a global temporary view\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateGlobalTempView\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"people\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Global temporary view is tied to a system preserved database `global_temp`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mcreateGlobalTempView\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m         \"\"\"\n\u001b[1;32m--> 200\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateGlobalTempView\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     69\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.catalyst.parser.ParseException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mParseException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: \"Temporary view 'people' already exists;\""
     ]
    }
   ],
   "source": [
    "\n",
    "# $example on:global_temp_view$\n",
    "# Register the DataFrame as a global temporary view\n",
    "df.createGlobalTempView(\"people\")\n",
    "\n",
    "# Global temporary view is tied to a system preserved database `global_temp`\n",
    "sqlContext.sql(\"SELECT * FROM global_temp.people\").show()    \n",
    "\n",
    "# error =  AnalysisException: \"Temporary view 'people' already exists;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Global temporary view is cross-session\n",
    "\n",
    "sqlContext.newSession().sql(\"SELECT * FROM global_temp.people\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## datasource.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> PARQUET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "# $example on:schema_merging$\n",
    "from pyspark.sql import Row\n",
    "# $example off:schema_merging$\n",
    "\n",
    "# $example on:generic_load_save_functions$\n",
    "df = sqlContext.read.load(\"C:/SPARK/examples/src/main/resources/users.parquet\")\n",
    "# df.select(\"name\", \"favorite_color\").write.save(\"namesAndFavColors.parquet\")\n",
    "# $example off:generic_load_save_functions$\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, favorite_color: string, favorite_numbers: array<int>]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# < i n s r t   -   image as you do it >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------------+\n",
      "|  name|favorite_color|favorite_numbers|\n",
      "+------+--------------+----------------+\n",
      "|Alyssa|          null|  [3, 9, 15, 20]|\n",
      "|   Ben|           red|              []|\n",
      "+------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parquet is a columnar format, supported by many data processing systems. The advantages of having a columnar storage are as follows −\n",
    "\n",
    "Columnar storage limits IO operations.\n",
    "\n",
    "Columnar storage can fetch specific columns that you need to access.\n",
    "\n",
    "Columnar storage consumes less space.\n",
    "\n",
    "Columnar storage gives better-summarized data and follows type-specific encoding.\n",
    "\n",
    "Spark SQL provides support for both reading and writing parquet files that automatically capture the schema of the original data. Like JSON datasets, parquet files follow the same procedure.\n",
    "\n",
    "Let’s take another look at the same example of employee record data named employee.parquet placed in the same directory where spark-shell is running.\n",
    "\n",
    "Given data − Do not bother about converting the input data of employee records into parquet format. We use the following commands that convert the RDD data into Parquet file. Place the employee.json document, which we have used as the input file in our previous examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# $example on:generic_load_save_functions$\n",
    "df2 = sqlContext.read.load(\"E:/userdata1.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+----------+---------+--------------------+------+---------------+-------------------+--------------------+----------+---------+--------------------+--------------------+\n",
      "|  registration_dttm| id|first_name|last_name|               email|gender|     ip_address|                 cc|             country| birthdate|   salary|               title|            comments|\n",
      "+-------------------+---+----------+---------+--------------------+------+---------------+-------------------+--------------------+----------+---------+--------------------+--------------------+\n",
      "|2016-02-03 01:55:29|  1|    Amanda|   Jordan|    ajordan0@com.com|Female|    1.197.201.2|   6759521864920116|           Indonesia|  3/8/1971| 49756.53|    Internal Auditor|               1E+02|\n",
      "|2016-02-03 11:04:03|  2|    Albert|  Freeman|     afreeman1@is.gd|  Male| 218.111.175.34|                   |              Canada| 1/16/1968|150280.17|       Accountant IV|                    |\n",
      "|2016-02-02 19:09:31|  3|    Evelyn|   Morgan|emorgan2@altervis...|Female|   7.161.136.94|   6767119071901597|              Russia|  2/1/1960|144972.51| Structural Engineer|                    |\n",
      "|2016-02-02 18:36:21|  4|    Denise|    Riley|    driley3@gmpg.org|Female|  140.35.109.83|   3576031598965625|               China|  4/8/1997| 90263.05|Senior Cost Accou...|                    |\n",
      "|2016-02-02 23:05:31|  5|    Carlos|    Burns|cburns4@miitbeian...|      | 169.113.235.40|   5602256255204850|        South Africa|          |     null|                    |                    |\n",
      "|2016-02-03 01:22:34|  6|   Kathryn|    White|  kwhite5@google.com|Female| 195.131.81.179|   3583136326049310|           Indonesia| 2/25/1983| 69227.11|   Account Executive|                    |\n",
      "|2016-02-03 02:33:08|  7|    Samuel|   Holmes|sholmes6@foxnews.com|  Male| 232.234.81.197|   3582641366974690|            Portugal|12/18/1987| 14247.62|Senior Financial ...|                    |\n",
      "|2016-02-03 00:47:06|  8|     Harry|   Howell| hhowell7@eepurl.com|  Male|   91.235.51.73|                   |Bosnia and Herzeg...|  3/1/1962|186469.43|    Web Developer IV|                    |\n",
      "|2016-02-02 21:52:53|  9|      Jose|   Foster|   jfoster8@yelp.com|  Male|   132.31.53.61|                   |         South Korea| 3/27/1992|231067.84|Software Test Eng...|               1E+02|\n",
      "|2016-02-03 12:29:47| 10|     Emily|  Stewart|estewart9@opensou...|Female| 143.28.251.245|   3574254110301671|             Nigeria| 1/28/1997| 27234.28|     Health Coach IV|                    |\n",
      "|2016-02-02 18:10:42| 11|     Susan|  Perkins| sperkinsa@patch.com|Female|    180.85.0.62|   3573823609854134|              Russia|          |210001.95|                    |                    |\n",
      "|2016-02-03 12:04:34| 12|     Alice|    Berry|aberryb@wikipedia...|Female| 246.225.12.189|   4917830851454417|               China| 8/12/1968| 22944.53|    Quality Engineer|                    |\n",
      "|2016-02-03 12:48:17| 13|    Justin|    Berry|jberryc@usatoday.com|  Male|   157.7.146.43|6331109912871813274|              Zambia| 8/15/1975| 44165.46|Structural Analys...|                    |\n",
      "|2016-02-03 15:46:52| 14|     Kathy| Reynolds|kreynoldsd@redcro...|Female|  81.254.172.13|   5537178462965976|Bosnia and Herzeg...| 6/27/1970|286592.99|           Librarian|                    |\n",
      "|2016-02-03 02:53:23| 15|   Dorothy|   Hudson|dhudsone@blogger.com|Female|       8.59.7.0|   3542586858224170|               Japan|12/20/1989|157099.71|  Nurse Practicioner|<script>alert('hi...|\n",
      "|2016-02-02 18:44:01| 16|     Bruce|   Willis|bwillisf@bluehost...|  Male|239.182.219.189|   3573030625927601|              Brazil|          |239100.65|                    |                    |\n",
      "|2016-02-02 18:57:45| 17|     Emily|  Andrews|eandrewsg@cornell...|Female| 29.231.180.172|     30271790537626|              Russia| 4/13/1990|116800.65|        Food Chemist|                    |\n",
      "|2016-02-03 10:44:24| 18|   Stephen|  Wallace|swallaceh@netvibe...|  Male|  152.49.213.62|   5433943468526428|             Ukraine| 1/15/1978|248877.99|Account Represent...|                    |\n",
      "|2016-02-03 05:45:54| 19|  Clarence|   Lawson|clawsoni@vkontakt...|  Male| 107.175.15.152|   3544052814080964|              Russia|          |177122.99|                    |                    |\n",
      "|2016-02-03 04:30:36| 20|   Rebecca|     Bell| rbellj@bandcamp.com|Female|172.215.104.127|                   |               China|          |137251.19|                    |                    |\n",
      "+-------------------+---+----------+---------+--------------------+------+---------------+-------------------+--------------------+----------+---------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "userdata[1-5].parquet: These are sample files containing data in PARQUET format.\n",
    "\n",
    "-> Number of rows in each file: 1000\n",
    "-> Column details:\n",
    "column#\t\tcolumn_name\t\thive_datatype\n",
    "=====================================================\n",
    "1\t\tregistration_dttm \ttimestamp\n",
    "2\t\tid \t\t\tint\n",
    "3\t\tfirst_name \t\tstring\n",
    "4\t\tlast_name \t\tstring\n",
    "5\t\temail \t\t\tstring\n",
    "6\t\tgender \t\t\tstring\n",
    "7\t\tip_address \t\tstring\n",
    "8\t\tcc \t\t\tstring\n",
    "9\t\tcountry \t\tstring\n",
    "10\t\tbirthdate \t\tstring\n",
    "11\t\tsalary \t\t\tdouble\n",
    "12\t\ttitle \t\t\tstring\n",
    "13\t\tcomments \t\tstring\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "| 10|\n",
      "| 11|\n",
      "| 12|\n",
      "| 13|\n",
      "| 14|\n",
      "| 15|\n",
      "| 16|\n",
      "| 17|\n",
      "| 18|\n",
      "| 19|\n",
      "| 20|\n",
      "+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# sqlDF = sqlContext.sql(\"SELECT * FROM people\")\n",
    "\n",
    "all_info_in_id_column = df2.select(\"id\").show()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|first_name|\n",
      "+----------+\n",
      "|    Amanda|\n",
      "|    Albert|\n",
      "|    Evelyn|\n",
      "|    Denise|\n",
      "|    Carlos|\n",
      "|   Kathryn|\n",
      "|    Samuel|\n",
      "|     Harry|\n",
      "|      Jose|\n",
      "|     Emily|\n",
      "|     Susan|\n",
      "|     Alice|\n",
      "|    Justin|\n",
      "|     Kathy|\n",
      "|   Dorothy|\n",
      "|     Bruce|\n",
      "|     Emily|\n",
      "|   Stephen|\n",
      "|  Clarence|\n",
      "|   Rebecca|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "first_name = df2.select(\"first_name\").show()   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Deep Dive into processing .vcf files "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading VCF files in SparkSQL / Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import scikit-allel\n",
    "import allel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(allel.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "### sample - vcf file below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "THIS IS A RANDOM SAMPLE.VCF EXAMPLE: \n",
    "    \n",
    " -  the file begins with meta-info lines with ## \n",
    "    \n",
    "\n",
    "##fileformat=VCFv4.0\n",
    "##fileDate=20090805\n",
    "##source=myImputationProgramV3.1\n",
    "##reference=1000GenomesPilot-NCBI36\n",
    "##phasing=partial\n",
    "##INFO=<ID=NS,Number=1,Type=Integer,Description=\"Number of Samples With Data\">\n",
    "##INFO=<ID=AN,Number=1,Type=Integer,Description=\"Total number of alleles in called genotypes\">\n",
    "##INFO=<ID=AC,Number=.,Type=Integer,Description=\"Allele count in genotypes, for each ALT allele, in the same order as listed\">\n",
    "##INFO=<ID=DP,Number=1,Type=Integer,Description=\"Total Depth\">\n",
    "##INFO=<ID=AF,Number=.,Type=Float,Description=\"Allele Frequency\">\n",
    "##INFO=<ID=AA,Number=1,Type=String,Description=\"Ancestral Allele\">\n",
    "##INFO=<ID=DB,Number=0,Type=Flag,Description=\"dbSNP membership, build 129\">\n",
    "##INFO=<ID=H2,Number=0,Type=Flag,Description=\"HapMap2 membership\">\n",
    "##FILTER=<ID=q10,Description=\"Quality below 10\">\n",
    "##FILTER=<ID=s50,Description=\"Less than 50% of samples have data\">\n",
    "##FORMAT=<ID=GT,Number=1,Type=String,Description=\"Genotype\">\n",
    "##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\"Genotype Quality\">\n",
    "##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\"Read Depth\">\n",
    "##FORMAT=<ID=HQ,Number=2,Type=Integer,Description=\"Haplotype Quality\">\n",
    "##ALT=<ID=DEL:ME:ALU,Description=\"Deletion of ALU element\">\n",
    "##ALT=<ID=CNV,Description=\"Copy number variable region\">       < - - im inserting a space for clarity \n",
    "\n",
    "\n",
    "#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO\tFORMAT\tNA00001\tNA00002\tNA00003\n",
    "\n",
    "19\t111\t.\tA\tC\t9.6\t.\t.\tGT:HQ\t0|0:10,10\t0|0:10,10\t0/1:3,3\n",
    "19\t112\t.\tA\tG\t10\t.\t.\tGT:HQ\t0|0:10,10\t0|0:10,10\t0/1:3,3\n",
    "20\t14370\trs6054257\tG\tA\t29\tPASS\tNS=3;DP=14;AF=0.5;DB;H2\tGT:GQ:DP:HQ\t0|0:48:1:51,51\t1|0:48:8:51,51\t1/1:43:5:.,.\n",
    "20\t17330\t.\tT\tA\t3\tq10\tNS=3;DP=11;AF=0.017\tGT:GQ:DP:HQ\t0|0:49:3:58,50\t0|1:3:5:65,3\t0/0:41:3:.,.\n",
    "20\t1110696\trs6040355\tA\tG,T\t67\tPASS\tNS=2;DP=10;AF=0.333,0.667;AA=T;DB\tGT:GQ:DP:HQ\t1|2:21:6:23,27\t2|1:2:0:18,2\t2/2:35:4:.,.\n",
    "20\t1230237\t.\tT\t.\t47\tPASS\tNS=3;DP=13;AA=T\tGT:GQ:DP:HQ\t0|0:54:.:56,60\t0|0:48:4:51,51\t0/0:61:2:.,.\n",
    "20\t1234567\tmicrosat1\tG\tGA,GAC\t50\tPASS\tNS=3;DP=9;AA=G;AN=6;AC=3,1\tGT:GQ:DP\t0/1:.:4\t0/2:17:2\t1/1:40:3\n",
    "20\t1235237\t.\tT\t.\t.\t.\t.\tGT\t0/0\t0|0\t./.\n",
    "X\t10\trsTest\tAC\tA,ATG\t10\tPASS\t.\tGT\t0\t0/1\t0|2\n",
    "\n",
    "\n",
    "\n",
    "FYI, this is the header:\n",
    "#CHROM    POS    ID    REF    ALT    QUAL    FILTER    INFO    FORMAT    NA00001    NA00002    NA00003\n",
    "\n",
    "\n",
    "So what is this ? \n",
    "After the header, there are DATA LINES, with each data line describing a genetic variant at a particular \n",
    "position relative to the reference genome of whichever species you are studying. \n",
    "\n",
    "In my case:\n",
    "CHROM\t\n",
    "POS\t\n",
    "ID\t\n",
    "REF\t\n",
    "ALT\t\n",
    "QUAL\n",
    "FILTER\n",
    "INFO\n",
    "FORMAT\n",
    "NA00001\n",
    "NA00002\n",
    "NA00003\n",
    "\n",
    "\n",
    "My first line describes a variant on chromosome **19** at position **11** relative to the to ____\n",
    "assembly of the human genome. The reference allele is ‘C’ and the alternate allele is ‘A’, so this etc etc \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##fileformat=VCFv4.0\n",
      "##fileDate=20090805\n",
      "##source=myImputationProgramV3.1\n",
      "##reference=1000GenomesPilot-NCBI36\n",
      "##phasing=partial\n",
      "##INFO=<ID=NS,Number=1,Type=Integer,Description=\"Number of Samples With Data\">\n",
      "##INFO=<ID=AN,Number=1,Type=Integer,Description=\"Total number of alleles in called genotypes\">\n",
      "##INFO=<ID=AC,Number=.,Type=Integer,Description=\"Allele count in genotypes, for each ALT allele, in the same order as listed\">\n",
      "##INFO=<ID=DP,Number=1,Type=Integer,Description=\"Total Depth\">\n",
      "##INFO=<ID=AF,Number=.,Type=Float,Description=\"Allele Frequency\">\n",
      "##INFO=<ID=AA,Number=1,Type=String,Description=\"Ancestral Allele\">\n",
      "##INFO=<ID=DB,Number=0,Type=Flag,Description=\"dbSNP membership, build 129\">\n",
      "##INFO=<ID=H2,Number=0,Type=Flag,Description=\"HapMap2 membership\">\n",
      "##FILTER=<ID=q10,Description=\"Quality below 10\">\n",
      "##FILTER=<ID=s50,Description=\"Less than 50% of samples have data\">\n",
      "##FORMAT=<ID=GT,Number=1,Type=String,Description=\"Genotype\">\n",
      "##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\"Genotype Quality\">\n",
      "##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\"Read Depth\">\n",
      "##FORMAT=<ID=HQ,Number=2,Type=Integer,Description=\"Haplotype Quality\">\n",
      "##ALT=<ID=DEL:ME:ALU,Description=\"Deletion of ALU element\">\n",
      "##ALT=<ID=CNV,Description=\"Copy number variable region\">\n",
      "#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO\tFORMAT\tNA00001\tNA00002\tNA00003\n",
      "19\t111\t.\tA\tC\t9.6\t.\t.\tGT:HQ\t0|0:10,10\t0|0:10,10\t0/1:3,3\n",
      "19\t112\t.\tA\tG\t10\t.\t.\tGT:HQ\t0|0:10,10\t0|0:10,10\t0/1:3,3\n",
      "20\t14370\trs6054257\tG\tA\t29\tPASS\tNS=3;DP=14;AF=0.5;DB;H2\tGT:GQ:DP:HQ\t0|0:48:1:51,51\t1|0:48:8:51,51\t1/1:43:5:.,.\n",
      "20\t17330\t.\tT\tA\t3\tq10\tNS=3;DP=11;AF=0.017\tGT:GQ:DP:HQ\t0|0:49:3:58,50\t0|1:3:5:65,3\t0/0:41:3:.,.\n",
      "20\t1110696\trs6040355\tA\tG,T\t67\tPASS\tNS=2;DP=10;AF=0.333,0.667;AA=T;DB\tGT:GQ:DP:HQ\t1|2:21:6:23,27\t2|1:2:0:18,2\t2/2:35:4:.,.\n",
      "20\t1230237\t.\tT\t.\t47\tPASS\tNS=3;DP=13;AA=T\tGT:GQ:DP:HQ\t0|0:54:.:56,60\t0|0:48:4:51,51\t0/0:61:2:.,.\n",
      "20\t1234567\tmicrosat1\tG\tGA,GAC\t50\tPASS\tNS=3;DP=9;AA=G;AN=6;AC=3,1\tGT:GQ:DP\t0/1:.:4\t0/2:17:2\t1/1:40:3\n",
      "20\t1235237\t.\tT\t.\t.\t.\t.\tGT\t0/0\t0|0\t./.\n",
      "X\t10\trsTest\tAC\tA,ATG\t10\tPASS\t.\tGT\t0\t0/1\t0|2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# very handy way of printing out the actual .vcf file as a way of getting an idea how this stuff works\n",
    "\n",
    "with open('C:/SPARK/sample.vcf', mode='r') as vcf:\n",
    "    print(vcf.read())\n",
    "    \n",
    "# prints out the vcf sample file\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "callset = allel.read_vcf('C:/SPARK/sample.vcf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(type(callset))   # key/value dict ! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'variants/POS': array([    111,     112,   14370,   17330, 1110696, 1230237, 1234567,\n",
      "       1235237,      10]), 'variants/QUAL': array([ 9.6, 10. , 29. ,  3. , 67. , 47. , 50. ,  nan, 10. ],\n",
      "      dtype=float32), 'variants/ALT': array([['C', '', ''],\n",
      "       ['G', '', ''],\n",
      "       ['A', '', ''],\n",
      "       ['A', '', ''],\n",
      "       ['G', 'T', ''],\n",
      "       ['', '', ''],\n",
      "       ['GA', 'GAC', ''],\n",
      "       ['', '', ''],\n",
      "       ['A', 'ATG', '']], dtype=object), 'samples': array(['NA00001', 'NA00002', 'NA00003'], dtype=object), 'calldata/GT': array([[[ 0,  0],\n",
      "        [ 0,  0],\n",
      "        [ 0,  1]],\n",
      "\n",
      "       [[ 0,  0],\n",
      "        [ 0,  0],\n",
      "        [ 0,  1]],\n",
      "\n",
      "       [[ 0,  0],\n",
      "        [ 1,  0],\n",
      "        [ 1,  1]],\n",
      "\n",
      "       [[ 0,  0],\n",
      "        [ 0,  1],\n",
      "        [ 0,  0]],\n",
      "\n",
      "       [[ 1,  2],\n",
      "        [ 2,  1],\n",
      "        [ 2,  2]],\n",
      "\n",
      "       [[ 0,  0],\n",
      "        [ 0,  0],\n",
      "        [ 0,  0]],\n",
      "\n",
      "       [[ 0,  1],\n",
      "        [ 0,  2],\n",
      "        [ 1,  1]],\n",
      "\n",
      "       [[ 0,  0],\n",
      "        [ 0,  0],\n",
      "        [-1, -1]],\n",
      "\n",
      "       [[ 0, -1],\n",
      "        [ 0,  1],\n",
      "        [ 0,  2]]], dtype=int8), 'variants/REF': array(['A', 'A', 'G', 'T', 'A', 'T', 'G', 'T', 'AC'], dtype=object), 'variants/FILTER_PASS': array([False, False,  True, False,  True,  True,  True, False,  True]), 'variants/CHROM': array(['19', '19', '20', '20', '20', '20', '20', '20', 'X'], dtype=object), 'variants/ID': array(['.', '.', 'rs6054257', '.', 'rs6040355', '.', 'microsat1', '.',\n",
      "       'rsTest'], dtype=object)}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(callset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clear\n",
      "copy\n",
      "fromkeys\n",
      "get\n",
      "items\n",
      "keys\n",
      "pop\n",
      "popitem\n",
      "setdefault\n",
      "update\n",
      "values\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Commands (methods) you can use:\n",
    "\n",
    "for i in dir(callset):\n",
    "    if not i.startswith(\"_\"):\n",
    "        print(i)\n",
    "\n",
    "# methods you can call, as a reference...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [    111     112   14370   17330 1110696 1230237 1234567 1235237      10]\n",
      "\n",
      " [ 9.6 10.  29.   3.  67.  47.  50.   nan 10. ]\n",
      "\n",
      " [['C' '' '']\n",
      " ['G' '' '']\n",
      " ['A' '' '']\n",
      " ['A' '' '']\n",
      " ['G' 'T' '']\n",
      " ['' '' '']\n",
      " ['GA' 'GAC' '']\n",
      " ['' '' '']\n",
      " ['A' 'ATG' '']]\n",
      "\n",
      " ['NA00001' 'NA00002' 'NA00003']\n",
      "\n",
      " [[[ 0  0]\n",
      "  [ 0  0]\n",
      "  [ 0  1]]\n",
      "\n",
      " [[ 0  0]\n",
      "  [ 0  0]\n",
      "  [ 0  1]]\n",
      "\n",
      " [[ 0  0]\n",
      "  [ 1  0]\n",
      "  [ 1  1]]\n",
      "\n",
      " [[ 0  0]\n",
      "  [ 0  1]\n",
      "  [ 0  0]]\n",
      "\n",
      " [[ 1  2]\n",
      "  [ 2  1]\n",
      "  [ 2  2]]\n",
      "\n",
      " [[ 0  0]\n",
      "  [ 0  0]\n",
      "  [ 0  0]]\n",
      "\n",
      " [[ 0  1]\n",
      "  [ 0  2]\n",
      "  [ 1  1]]\n",
      "\n",
      " [[ 0  0]\n",
      "  [ 0  0]\n",
      "  [-1 -1]]\n",
      "\n",
      " [[ 0 -1]\n",
      "  [ 0  1]\n",
      "  [ 0  2]]]\n",
      "\n",
      " ['A' 'A' 'G' 'T' 'A' 'T' 'G' 'T' 'AC']\n",
      "\n",
      " [False False  True False  True  True  True False  True]\n",
      "\n",
      " ['19' '19' '20' '20' '20' '20' '20' '20' 'X']\n",
      "\n",
      " ['.' '.' 'rs6054257' '.' 'rs6040355' '.' 'microsat1' '.' 'rsTest']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in callset.values():  print(\"\\n\",i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The callset object returned by read_vcf() is a Python dictionary (dict). It contains several NumPy arrays, each of which can be accessed via a key. Here are the available keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['calldata/GT',\n",
       " 'samples',\n",
       " 'variants/ALT',\n",
       " 'variants/CHROM',\n",
       " 'variants/FILTER_PASS',\n",
       " 'variants/ID',\n",
       " 'variants/POS',\n",
       " 'variants/QUAL',\n",
       " 'variants/REF']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# The callset object returned by read_vcf() is a Python dictionary (dict). \n",
    "# It contains several NumPy arrays, each of which can be accessed via a key. \n",
    "# Here are the available keys:\n",
    "\n",
    "# keys:\n",
    "sorted(callset.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- calldata/GT\n",
      "- samples\n",
      "- variants/ALT\n",
      "- variants/CHROM\n",
      "- variants/FILTER_PASS\n",
      "- variants/ID\n",
      "- variants/POS\n",
      "- variants/QUAL\n",
      "- variants/REF\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in sorted(callset.keys()):  print(\"-\",i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NA00001', 'NA00002', 'NA00003'], dtype=object)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# The ‘samples’ array contains sample identifiers extracted from the header line in the VCF file.\n",
    "\n",
    "callset['samples']\n",
    "\n",
    "# look to the far right:\n",
    "# #CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO\tFORMAT\tNA00001\tNA00002\tNA00003\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['19', '19', '20', '20', '20', '20', '20', '20', 'X'], dtype=object)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# All arrays with keys beginning ‘variants/’ come from the fixed fields in the VCF file.\n",
    "# For example, here is the data from the ‘CHROM’ field:\n",
    "\n",
    "callset['variants/CHROM']\n",
    "\n",
    "# chromosomes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    111,     112,   14370,   17330, 1110696, 1230237, 1234567,\n",
       "       1235237,      10])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Here is the data from the ‘POS’ field:\n",
    "\n",
    "callset['variants/POS']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.6, 10. , 29. ,  3. , 67. , 47. , 50. ,  nan, 10. ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Here is the data from the ‘QUAL’ field:\n",
    "\n",
    "callset['variants/QUAL']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  0],\n",
       "        [ 0,  0],\n",
       "        [ 0,  1]],\n",
       "\n",
       "       [[ 0,  0],\n",
       "        [ 0,  0],\n",
       "        [ 0,  1]],\n",
       "\n",
       "       [[ 0,  0],\n",
       "        [ 1,  0],\n",
       "        [ 1,  1]],\n",
       "\n",
       "       [[ 0,  0],\n",
       "        [ 0,  1],\n",
       "        [ 0,  0]],\n",
       "\n",
       "       [[ 1,  2],\n",
       "        [ 2,  1],\n",
       "        [ 2,  2]],\n",
       "\n",
       "       [[ 0,  0],\n",
       "        [ 0,  0],\n",
       "        [ 0,  0]],\n",
       "\n",
       "       [[ 0,  1],\n",
       "        [ 0,  2],\n",
       "        [ 1,  1]],\n",
       "\n",
       "       [[ 0,  0],\n",
       "        [ 0,  0],\n",
       "        [-1, -1]],\n",
       "\n",
       "       [[ 0, -1],\n",
       "        [ 0,  1],\n",
       "        [ 0,  2]]], dtype=int8)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# All arrays with keys beginning ‘calldata/’ come from the sample fields in the VCF file. \n",
    "# For example, here are the actual genotype calls from the ‘GT’ field:\n",
    "\n",
    "callset['calldata/GT']\n",
    "\n",
    "# Note the -1 values for one of the genotype calls. By default scikit-allel uses \n",
    "# -1 to indicate a missing value for any array with a signed integer data type \n",
    "# d(although you can change this if you want).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"allel allel-DisplayAs2D\"><span>&lt;GenotypeArray shape=(9, 3, 2) dtype=int8&gt;</span><table><thead><tr><th></th><th style=\"text-align: center\">0</th><th style=\"text-align: center\">1</th><th style=\"text-align: center\">2</th></tr></thead><tbody><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">0</th><td style=\"text-align: center\">0/0</td><td style=\"text-align: center\">0/0</td><td style=\"text-align: center\">0/1</td></tr><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">1</th><td style=\"text-align: center\">0/0</td><td style=\"text-align: center\">0/0</td><td style=\"text-align: center\">0/1</td></tr><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">2</th><td style=\"text-align: center\">0/0</td><td style=\"text-align: center\">1/0</td><td style=\"text-align: center\">1/1</td></tr><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">...</th><td style=\"text-align: center\" colspan=\"4\">...</td></tr><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">6</th><td style=\"text-align: center\">0/1</td><td style=\"text-align: center\">0/2</td><td style=\"text-align: center\">1/1</td></tr><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">7</th><td style=\"text-align: center\">0/0</td><td style=\"text-align: center\">0/0</td><td style=\"text-align: center\">./.</td></tr><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">8</th><td style=\"text-align: center\">0/.</td><td style=\"text-align: center\">0/1</td><td style=\"text-align: center\">0/2</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "<GenotypeArray shape=(9, 3, 2) dtype=int8>\n",
       "0/0 0/0 0/1\n",
       "0/0 0/0 0/1\n",
       "0/0 1/0 1/1\n",
       "...\n",
       "0/1 0/2 1/1\n",
       "0/0 0/0 ./.\n",
       "0/. 0/1 0/2"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Aside: genotype arrays\n",
    "# Because working with genotype calls is a very common task, scikit-allel has \n",
    "# a GenotypeArray class which adds some convenient functionality to an array \n",
    "# of genotype calls. To use this class, pass the raw NumPy array into the GenotypeArray \n",
    "# class constructor, e.g.:\n",
    "\n",
    "\n",
    "gt = allel.GenotypeArray(callset['calldata/GT'])\n",
    "\n",
    "gt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the things that the GenotypeArray class does is provide a slightly more visually-appealing representation when used in a Jupyter notebook, as can be seen above. There are also methods for making various computations over the genotype calls. For example, the is_het() method locates all heterozygous genotype calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [ True,  True, False],\n",
       "       [False, False, False],\n",
       "       [ True,  True, False],\n",
       "       [False, False, False],\n",
       "       [False,  True,  True]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt.is_het()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 2, 0, 2, 0, 2])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# To give another example, the count_het() method will count heterozygous calls, summing over \n",
    "# variants (axis=0) or samples (axis=1) if requested.\n",
    "# E.g., to count the number of het calls per variant:\n",
    "    \n",
    "gt.count_het(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"allel allel-DisplayAs2D\"><span>&lt;AlleleCountsArray shape=(9, 3) dtype=int32&gt;</span><table><thead><tr><th></th><th style=\"text-align: center\">0</th><th style=\"text-align: center\">1</th><th style=\"text-align: center\">2</th></tr></thead><tbody><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">0</th><td style=\"text-align: center\">5</td><td style=\"text-align: center\">1</td><td style=\"text-align: center\">0</td></tr><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">1</th><td style=\"text-align: center\">5</td><td style=\"text-align: center\">1</td><td style=\"text-align: center\">0</td></tr><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">2</th><td style=\"text-align: center\">3</td><td style=\"text-align: center\">3</td><td style=\"text-align: center\">0</td></tr><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">...</th><td style=\"text-align: center\" colspan=\"4\">...</td></tr><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">6</th><td style=\"text-align: center\">2</td><td style=\"text-align: center\">3</td><td style=\"text-align: center\">1</td></tr><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">7</th><td style=\"text-align: center\">4</td><td style=\"text-align: center\">0</td><td style=\"text-align: center\">0</td></tr><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">8</th><td style=\"text-align: center\">3</td><td style=\"text-align: center\">1</td><td style=\"text-align: center\">1</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "<AlleleCountsArray shape=(9, 3) dtype=int32>\n",
       "5 1 0\n",
       "5 1 0\n",
       "3 3 0\n",
       "...\n",
       "2 3 1\n",
       "4 0 0\n",
       "3 1 1"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# One more example, here is how to perform an allele count,\n",
    "# i.e., count the number times each allele (0=reference, 1=first alternate, \n",
    "# 2=second alternate, etc.) is observed for each variant:\n",
    "    \n",
    "ac = gt.count_alleles()\n",
    "ac \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fields: \n",
    "    \n",
    "VCF files can often contain many fields of data, and you may only need to extract \n",
    "some of them to perform a particular analysis. You can select which fields to extract by passing a list of strings as the fields parameter. For example, let’s extract the ‘DP’ field from within the ‘INFO’ field, and let’s also extract the ‘DP’ field from the genotype call data:\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['calldata/DP', 'variants/DP']"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "callset = allel.read_vcf('C:/SPARK/sample.vcf', fields=['variants/DP', 'calldata/DP'])\n",
    "sorted(callset.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1, 14, 11, 10, 13,  9, -1, -1])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# here is the data we just extracted:\n",
    "callset['variants/DP']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1, -1, -1],\n",
       "       [-1, -1, -1],\n",
       "       [ 1,  8,  5],\n",
       "       [ 3,  5,  3],\n",
       "       [ 6,  0,  4],\n",
       "       [-1,  4,  2],\n",
       "       [ 4,  2,  3],\n",
       "       [-1, -1, -1],\n",
       "       [-1, -1, -1]], dtype=int16)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "callset['calldata/DP']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "I chose these two fields to illustrate the point that sometimes the same field name (e.g., ‘DP’) can be used both within the INFO field of a VCF and also within the genotype call data. When selecting fields, to make sure there is no ambiguity, you can include a prefix which is either ‘variants/’ or ‘calldata/’. For example, if you provide ‘variants/DP’, then the read_vcf() function will look for an INFO field named ‘DP’. If you provide ‘calldata/DP’ then read_vcf() will look for a FORMAT field named ‘DP’ within the call data.\n",
    "\n",
    "If you are feeling lazy, you can drop the ‘variants/’ and ‘calldata/’ prefixes, in which case read_vcf() will assume you mean ‘variants/’ if there is any ambiguity. E.g.:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['calldata/GT', 'variants/DP']"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "callset = allel.read_vcf('C:/SPARK/sample.vcf', fields=['DP', 'GT'])\n",
    "sorted(callset.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to extract absolutely everything from a VCF file, then you can provide a special value '*' as the fields parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['calldata/DP',\n",
       " 'calldata/GQ',\n",
       " 'calldata/GT',\n",
       " 'calldata/HQ',\n",
       " 'samples',\n",
       " 'variants/AA',\n",
       " 'variants/AC',\n",
       " 'variants/AF',\n",
       " 'variants/ALT',\n",
       " 'variants/AN',\n",
       " 'variants/CHROM',\n",
       " 'variants/DB',\n",
       " 'variants/DP',\n",
       " 'variants/FILTER_PASS',\n",
       " 'variants/FILTER_q10',\n",
       " 'variants/FILTER_s50',\n",
       " 'variants/H2',\n",
       " 'variants/ID',\n",
       " 'variants/NS',\n",
       " 'variants/POS',\n",
       " 'variants/QUAL',\n",
       " 'variants/REF',\n",
       " 'variants/is_snp',\n",
       " 'variants/numalt',\n",
       " 'variants/svlen']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "callset = allel.read_vcf('C:/SPARK/sample.vcf', fields='*')\n",
    "sorted(callset.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also provide the special value 'variants/*' to request all variants fields (including all INFO), and the special value 'calldata/*' to request all call data fields.\n",
    "\n",
    "If you don’t specify the fields parameter, scikit-allel will default to extracting data from the fixed variants fields (but no INFO) and the GT genotype field if present (but no other call data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy arrays can have various data types, including signed integers (‘int8’, ‘int16’, ‘int32’, ‘int64’), unsigned integers (‘uint8’, ‘uint16’, ‘uint32’, ‘uint64’), floating point numbers (‘float32’, ‘float64’), variable length strings (‘object’) and fixed length strings (e.g., ‘S4’ for a 4-character ASCII string). scikit-allel will try to choose a sensible default data type for the fields you want to extract, based on the meta-information in the VCF file, but you can override these via the types parameter.\n",
    "\n",
    "For example, by default the ‘DP’ INFO field is loaded into a 32-bit integer array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1, 14, 11, 10, 13,  9, -1, -1])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "callset = allel.read_vcf('C:/SPARK/sample.vcf', fields=['DP'])\n",
    "callset['variants/DP']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fields containing textual data, there are two choices for data type. By default, scikit-allel will use an ‘object’ data type, which means that values are stored as an array of Python strings. E.g.:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bam !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['A', 'A', 'G', 'T', 'A', 'T', 'G', 'T', 'AC'], dtype=object)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "callset = allel.read_vcf('C:/SPARK/sample.vcf')\n",
    "callset['variants/REF']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'A', b'A', b'G', b'T', b'A', b'T', b'G', b'T', b'AC'], dtype='|S3')"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# The advantage of using ‘object’ dtype is that strings can be of any length. \n",
    "# Alternatively, you can use a fixed-length string dtype, e.g.:\n",
    "\n",
    "callset = allel.read_vcf('C:/SPARK/sample.vcf', types={'REF': 'S3'})\n",
    "callset['variants/REF']\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Some fields like ‘ALT’, ‘AC’ and ‘AF’ can have a variable number of values. I.e., each variant may have a different number of data values for these fields. One trade-off you have to make when loading data into NumPy arrays is that you cannot have arrays with a variable number of items per row. Rather, you have to fix the maximum number of possible items. While you lose some flexibility, you gain speed of access.\n",
    "\n",
    "For fields like ‘ALT’, scikit-allel will choose a default number of expected values, which is set at 3. E.g., here is what you get by default:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['C', '', ''],\n",
       "       ['G', '', ''],\n",
       "       ['A', '', ''],\n",
       "       ['A', '', ''],\n",
       "       ['G', 'T', ''],\n",
       "       ['', '', ''],\n",
       "       ['GA', 'GAC', ''],\n",
       "       ['', '', ''],\n",
       "       ['A', 'ATG', '']], dtype=object)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "callset = allel.read_vcf('C:/SPARK/sample.vcf')\n",
    "callset['variants/ALT']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, 3 is more that we need, because no variant has more than 2 ALT values. However, some VCF files (especially those including INDELs) may have more than 3 ALT values.\n",
    "\n",
    "If you need to increase or decrease the expected number of values for any field, you can do this via the numbers parameter. E.g., increase the number of ALT values to 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['C', '', '', '', ''],\n",
       "       ['G', '', '', '', ''],\n",
       "       ['A', '', '', '', ''],\n",
       "       ['A', '', '', '', ''],\n",
       "       ['G', 'T', '', '', ''],\n",
       "       ['', '', '', '', ''],\n",
       "       ['GA', 'GAC', '', '', ''],\n",
       "       ['', '', '', '', ''],\n",
       "       ['A', 'ATG', '', '', '']], dtype=object)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "callset = allel.read_vcf('C:/SPARK/sample.vcf', numbers={'ALT': 5})\n",
    "callset['variants/ALT']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genotype ploidy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "By default, scikit-allel assumes you are working with a diploid organism, and so expects to parse out 2 alleles for each genotype call. If you are working with an organism with some other ploidy, you can change the expected number of alleles via the numbers parameter.\n",
    "\n",
    "For example, here is an example VCF with tetraploid genotype calls:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can extract data for only a specific chromosome or genome region via the region parameter. The value of the parameter should be a region string of the format ‘{chromosome}:{begin}-{end}’, just like you would give to tabix or samtools. E.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1110696, 1230237])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "callset = allel.read_vcf('C:/SPARK/sample.vcf', region='20:1000000-1231000')\n",
    "callset['variants/POS']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NA00001', 'NA00003'], dtype=object)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# You can extract data for only specific samples via the samples parameter. \n",
    "# e.g. extract data for samples ‘NA00001’ and ‘NA00003’:\n",
    "\n",
    "callset = allel.read_vcf('C:/SPARK/sample.vcf', samples=['NA00001', 'NA00003'])\n",
    "callset['samples']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"allel allel-DisplayAs2D\"><span>&lt;GenotypeArray shape=(9, 2, 2) dtype=int8&gt;</span><table><thead><tr><th></th><th style=\"text-align: center\">0</th><th style=\"text-align: center\">1</th></tr></thead><tbody><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">0</th><td style=\"text-align: center\">0/0</td><td style=\"text-align: center\">0/1</td></tr><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">1</th><td style=\"text-align: center\">0/0</td><td style=\"text-align: center\">0/1</td></tr><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">2</th><td style=\"text-align: center\">0/0</td><td style=\"text-align: center\">1/1</td></tr><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">...</th><td style=\"text-align: center\" colspan=\"3\">...</td></tr><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">6</th><td style=\"text-align: center\">0/1</td><td style=\"text-align: center\">1/1</td></tr><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">7</th><td style=\"text-align: center\">0/0</td><td style=\"text-align: center\">./.</td></tr><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">8</th><td style=\"text-align: center\">0/.</td><td style=\"text-align: center\">0/2</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "<GenotypeArray shape=(9, 2, 2) dtype=int8>\n",
       "0/0 0/1\n",
       "0/0 0/1\n",
       "0/0 1/1\n",
       "...\n",
       "0/1 1/1\n",
       "0/0 ./.\n",
       "0/. 0/2"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "allel.GenotypeArray(callset['calldata/GT'])\n",
    "\n",
    "# Note that the genotype array now only has two columns, corresponding to the two samples requested.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## we can also take the .vcf and store it as hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  but will store extracted data into an HDF5 file stored on disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vcf_to_hdf5()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For large datasets, the vcf_to_hdf5() function is available. This function again takes similar parameters to read_vcf(), but will store extracted data into an HDF5 file stored on disk. The extraction process works through the VCF file in chunks, and so the entire dataset is never loaded entirely into main memory. A bit further below I give worked examples with a large dataset, but for now here is a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# saving the file directly as hdf5 ! ! ! \n",
    "\n",
    "allel.vcf_to_hdf5('C:/SPARK/sample.vcf', 'C:/SPARK/sample_hdf5.h5', fields='*', overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 file \"sample_hdf5.h5\" (mode r)>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 file \"sample_hdf5.h5\" (mode r)>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# now lets assume i just showed up and wanted to review this information (the hdf5 file):\n",
    "\n",
    "import h5py  # conda install this \n",
    "\n",
    "callset = h5py.File('C:/SPARK/sample_hdf5.h5', mode='r')\n",
    "\n",
    "callset\n",
    "\n",
    "print(callset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"CHROM\": shape (9,), type \"|O\">"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "chrom = callset['variants/CHROM']\n",
    "chrom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"POS\": shape (9,), type \"<i4\">"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pos = callset['variants/POS']\n",
    "pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"GT\": shape (9, 3, 2), type \"|i1\">"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "gt = callset['calldata/GT']\n",
    "gt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['19', '20'], dtype=object)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# This dataset object is useful because you can then load all or only part of \n",
    "# the underlying data into main memory via slicing. e.g.\n",
    "\n",
    "# load second to fourth items into NumPy array\n",
    "chrom[1:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"allel allel-DisplayAs2D\"><span>&lt;GenotypeArray shape=(2, 3, 2) dtype=int8&gt;</span><table><thead><tr><th></th><th style=\"text-align: center\">0</th><th style=\"text-align: center\">1</th><th style=\"text-align: center\">2</th></tr></thead><tbody><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">0</th><td style=\"text-align: center\">0/0</td><td style=\"text-align: center\">0/0</td><td style=\"text-align: center\">0/1</td></tr><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">1</th><td style=\"text-align: center\">0/0</td><td style=\"text-align: center\">1/0</td><td style=\"text-align: center\">1/1</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "<GenotypeArray shape=(2, 3, 2) dtype=int8>\n",
       "0/0 0/0 0/1\n",
       "0/0 1/0 1/1"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# load genotype calls into memory for second to fourth variants, all samples\n",
    "allel.GenotypeArray(gt[1:3, :])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assume you want all of this in a dataframe ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tbresee\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\allel\\io\\vcf_read.py:1569: FutureWarning: from_items is deprecated. Please use DataFrame.from_dict(dict(items), ...) instead. DataFrame.from_dict(OrderedDict(items)) may be used to preserve the key order.\n",
      "  df = pandas.DataFrame.from_items(items)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CHROM</th>\n",
       "      <th>POS</th>\n",
       "      <th>ID</th>\n",
       "      <th>REF</th>\n",
       "      <th>ALT_1</th>\n",
       "      <th>ALT_2</th>\n",
       "      <th>ALT_3</th>\n",
       "      <th>QUAL</th>\n",
       "      <th>FILTER_PASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>111</td>\n",
       "      <td>.</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>9.6</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>112</td>\n",
       "      <td>.</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>14370</td>\n",
       "      <td>rs6054257</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>29.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>17330</td>\n",
       "      <td>.</td>\n",
       "      <td>T</td>\n",
       "      <td>A</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>1110696</td>\n",
       "      <td>rs6040355</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>T</td>\n",
       "      <td></td>\n",
       "      <td>67.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>1230237</td>\n",
       "      <td>.</td>\n",
       "      <td>T</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>47.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20</td>\n",
       "      <td>1234567</td>\n",
       "      <td>microsat1</td>\n",
       "      <td>G</td>\n",
       "      <td>GA</td>\n",
       "      <td>GAC</td>\n",
       "      <td></td>\n",
       "      <td>50.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20</td>\n",
       "      <td>1235237</td>\n",
       "      <td>.</td>\n",
       "      <td>T</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>X</td>\n",
       "      <td>10</td>\n",
       "      <td>rsTest</td>\n",
       "      <td>AC</td>\n",
       "      <td>A</td>\n",
       "      <td>ATG</td>\n",
       "      <td></td>\n",
       "      <td>10.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CHROM      POS         ID REF ALT_1 ALT_2 ALT_3  QUAL  FILTER_PASS\n",
       "0    19      111          .   A     C               9.6        False\n",
       "1    19      112          .   A     G              10.0        False\n",
       "2    20    14370  rs6054257   G     A              29.0         True\n",
       "3    20    17330          .   T     A               3.0        False\n",
       "4    20  1110696  rs6040355   A     G     T        67.0         True\n",
       "5    20  1230237          .   T                    47.0         True\n",
       "6    20  1234567  microsat1   G    GA   GAC        50.0         True\n",
       "7    20  1235237          .   T                     NaN        False\n",
       "8     X       10     rsTest  AC     A   ATG        10.0         True"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# For some analyses it can be useful to think of the data in a VCF file as a table or data frame, \n",
    "# especially if you are only analysing data from the fixed fields and don’t need the genotype \n",
    "# calls or any other call data. The vcf_to_dataframe() function extracts data from a VCF and \n",
    "# loads into a pandas DataFrame. E.g.:\n",
    "\n",
    "df = allel.vcf_to_dataframe('C:/SPARK/sample.vcf')\n",
    "\n",
    "df\n",
    "\n",
    "# extract my data from the vcf and put into a dataframe ! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CHROM      POS         ID REF ALT_1 ALT_2 ALT_3  QUAL  FILTER_PASS\n",
      "0    19      111          .   A     C               9.6        False\n",
      "1    19      112          .   A     G              10.0        False\n",
      "2    20    14370  rs6054257   G     A              29.0         True\n",
      "3    20    17330          .   T     A               3.0        False\n",
      "4    20  1110696  rs6040355   A     G     T        67.0         True\n",
      "5    20  1230237          .   T                    47.0         True\n",
      "6    20  1234567  microsat1   G    GA   GAC        50.0         True\n",
      "7    20  1235237          .   T                     NaN        False\n",
      "8     X       10     rsTest  AC     A   ATG        10.0         True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# so its just the original vcf data but put into a dataframe, which is cool:\n",
    "\n",
    "# #CHROM  POS    ID    REF    ALT    QUAL   \n",
    "\n",
    "# 19      111         .    A    C    9.6   \n",
    "# 19      112          .    A    G    10    \n",
    "# 20      14370        rs6054257    G    A    29    \n",
    "# 20      17330    .    T    A    3    q10       \n",
    "# 20      1110696    rs6040355    A    G,T    67   \n",
    "# 20      1230237    .    T    .    47      \n",
    "# 20      1234567    microsat1    G    GA,GAC    50   \n",
    "# 20      1235237    .    T    .    .    .    .   \n",
    "# X       10    rsTest    AC    A,ATG    10    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tbresee\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\allel\\io\\vcf_read.py:1569: FutureWarning: from_items is deprecated. Please use DataFrame.from_dict(dict(items), ...) instead. DataFrame.from_dict(OrderedDict(items)) may be used to preserve the key order.\n",
      "  df = pandas.DataFrame.from_items(items)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CHROM</th>\n",
       "      <th>POS</th>\n",
       "      <th>ID</th>\n",
       "      <th>REF</th>\n",
       "      <th>ALT_1</th>\n",
       "      <th>ALT_2</th>\n",
       "      <th>QUAL</th>\n",
       "      <th>DP</th>\n",
       "      <th>AN</th>\n",
       "      <th>AF_1</th>\n",
       "      <th>...</th>\n",
       "      <th>NS</th>\n",
       "      <th>AC_1</th>\n",
       "      <th>AC_2</th>\n",
       "      <th>FILTER_PASS</th>\n",
       "      <th>FILTER_q10</th>\n",
       "      <th>FILTER_s50</th>\n",
       "      <th>numalt</th>\n",
       "      <th>svlen_1</th>\n",
       "      <th>svlen_2</th>\n",
       "      <th>is_snp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>111</td>\n",
       "      <td>.</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td></td>\n",
       "      <td>9.6</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>112</td>\n",
       "      <td>.</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td></td>\n",
       "      <td>10.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>14370</td>\n",
       "      <td>rs6054257</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td></td>\n",
       "      <td>29.0</td>\n",
       "      <td>14</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.500</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>17330</td>\n",
       "      <td>.</td>\n",
       "      <td>T</td>\n",
       "      <td>A</td>\n",
       "      <td></td>\n",
       "      <td>3.0</td>\n",
       "      <td>11</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.017</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>1110696</td>\n",
       "      <td>rs6040355</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>T</td>\n",
       "      <td>67.0</td>\n",
       "      <td>10</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.333</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>1230237</td>\n",
       "      <td>.</td>\n",
       "      <td>T</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>47.0</td>\n",
       "      <td>13</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20</td>\n",
       "      <td>1234567</td>\n",
       "      <td>microsat1</td>\n",
       "      <td>G</td>\n",
       "      <td>GA</td>\n",
       "      <td>GAC</td>\n",
       "      <td>50.0</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20</td>\n",
       "      <td>1235237</td>\n",
       "      <td>.</td>\n",
       "      <td>T</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>X</td>\n",
       "      <td>10</td>\n",
       "      <td>rsTest</td>\n",
       "      <td>AC</td>\n",
       "      <td>A</td>\n",
       "      <td>ATG</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  CHROM      POS         ID REF ALT_1 ALT_2  QUAL  DP  AN   AF_1   ...    NS  \\\n",
       "0    19      111          .   A     C         9.6  -1  -1    NaN   ...    -1   \n",
       "1    19      112          .   A     G        10.0  -1  -1    NaN   ...    -1   \n",
       "2    20    14370  rs6054257   G     A        29.0  14  -1  0.500   ...     3   \n",
       "3    20    17330          .   T     A         3.0  11  -1  0.017   ...     3   \n",
       "4    20  1110696  rs6040355   A     G     T  67.0  10  -1  0.333   ...     2   \n",
       "5    20  1230237          .   T              47.0  13  -1    NaN   ...     3   \n",
       "6    20  1234567  microsat1   G    GA   GAC  50.0   9   6    NaN   ...     3   \n",
       "7    20  1235237          .   T               NaN  -1  -1    NaN   ...    -1   \n",
       "8     X       10     rsTest  AC     A   ATG  10.0  -1  -1    NaN   ...    -1   \n",
       "\n",
       "  AC_1  AC_2  FILTER_PASS  FILTER_q10  FILTER_s50  numalt  svlen_1  svlen_2  \\\n",
       "0   -1    -1        False       False       False       1        0        0   \n",
       "1   -1    -1        False       False       False       1        0        0   \n",
       "2   -1    -1         True       False       False       1        0        0   \n",
       "3   -1    -1        False        True       False       1        0        0   \n",
       "4   -1    -1         True       False       False       2        0        0   \n",
       "5   -1    -1         True       False       False       0        0        0   \n",
       "6    3     1         True       False       False       2        1        2   \n",
       "7   -1    -1        False       False       False       0        0        0   \n",
       "8   -1    -1         True       False       False       2       -1        1   \n",
       "\n",
       "   is_snp  \n",
       "0    True  \n",
       "1    True  \n",
       "2    True  \n",
       "3    True  \n",
       "4    True  \n",
       "5   False  \n",
       "6   False  \n",
       "7   False  \n",
       "8   False  \n",
       "\n",
       "[9 rows x 24 columns]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# some values (cols) on the right were missing, but if you want ALL of them:\n",
    "\n",
    "df = allel.vcf_to_dataframe('C:/SPARK/sample.vcf', fields='*', alt_number=2)\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CHROM      POS         ID REF ALT_1 ALT_2  QUAL  DP  AN   AF_1   ...    NS  \\\n",
      "0    19      111          .   A     C         9.6  -1  -1    NaN   ...    -1   \n",
      "1    19      112          .   A     G        10.0  -1  -1    NaN   ...    -1   \n",
      "2    20    14370  rs6054257   G     A        29.0  14  -1  0.500   ...     3   \n",
      "3    20    17330          .   T     A         3.0  11  -1  0.017   ...     3   \n",
      "4    20  1110696  rs6040355   A     G     T  67.0  10  -1  0.333   ...     2   \n",
      "5    20  1230237          .   T              47.0  13  -1    NaN   ...     3   \n",
      "6    20  1234567  microsat1   G    GA   GAC  50.0   9   6    NaN   ...     3   \n",
      "7    20  1235237          .   T               NaN  -1  -1    NaN   ...    -1   \n",
      "8     X       10     rsTest  AC     A   ATG  10.0  -1  -1    NaN   ...    -1   \n",
      "\n",
      "  AC_1  AC_2  FILTER_PASS  FILTER_q10  FILTER_s50  numalt  svlen_1  svlen_2  \\\n",
      "0   -1    -1        False       False       False       1        0        0   \n",
      "1   -1    -1        False       False       False       1        0        0   \n",
      "2   -1    -1         True       False       False       1        0        0   \n",
      "3   -1    -1        False        True       False       1        0        0   \n",
      "4   -1    -1         True       False       False       2        0        0   \n",
      "5   -1    -1         True       False       False       0        0        0   \n",
      "6    3     1         True       False       False       2        1        2   \n",
      "7   -1    -1        False       False       False       0        0        0   \n",
      "8   -1    -1         True       False       False       2       -1        1   \n",
      "\n",
      "   is_snp  \n",
      "0    True  \n",
      "1    True  \n",
      "2    True  \n",
      "3    True  \n",
      "4    True  \n",
      "5   False  \n",
      "6   False  \n",
      "7   False  \n",
      "8   False  \n",
      "\n",
      "[9 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(df)\n",
    "\n",
    "# bam ! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CHROM</th>\n",
       "      <th>POS</th>\n",
       "      <th>ID</th>\n",
       "      <th>REF</th>\n",
       "      <th>ALT_1</th>\n",
       "      <th>ALT_2</th>\n",
       "      <th>QUAL</th>\n",
       "      <th>DP</th>\n",
       "      <th>AN</th>\n",
       "      <th>AF_1</th>\n",
       "      <th>...</th>\n",
       "      <th>NS</th>\n",
       "      <th>AC_1</th>\n",
       "      <th>AC_2</th>\n",
       "      <th>FILTER_PASS</th>\n",
       "      <th>FILTER_q10</th>\n",
       "      <th>FILTER_s50</th>\n",
       "      <th>numalt</th>\n",
       "      <th>svlen_1</th>\n",
       "      <th>svlen_2</th>\n",
       "      <th>is_snp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>14370</td>\n",
       "      <td>rs6054257</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td></td>\n",
       "      <td>29.0</td>\n",
       "      <td>14</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>1230237</td>\n",
       "      <td>.</td>\n",
       "      <td>T</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>47.0</td>\n",
       "      <td>13</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  CHROM      POS         ID REF ALT_1 ALT_2  QUAL  DP  AN  AF_1   ...    NS  \\\n",
       "2    20    14370  rs6054257   G     A        29.0  14  -1   0.5   ...     3   \n",
       "5    20  1230237          .   T              47.0  13  -1   NaN   ...     3   \n",
       "\n",
       "  AC_1  AC_2  FILTER_PASS  FILTER_q10  FILTER_s50  numalt  svlen_1  svlen_2  \\\n",
       "2   -1    -1         True       False       False       1        0        0   \n",
       "5   -1    -1         True       False       False       0        0        0   \n",
       "\n",
       "   is_snp  \n",
       "2    True  \n",
       "5   False  \n",
       "\n",
       "[2 rows x 24 columns]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# wow, you can querey ! \n",
    "\n",
    "df.query('DP > 10 and QUAL > 20')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHROM,POS,DP\n",
      "19,111,-1\n",
      "19,112,-1\n",
      "20,14370,14\n",
      "20,17330,11\n",
      "20,1110696,10\n",
      "20,1230237,13\n",
      "20,1234567,9\n",
      "20,1235237,-1\n",
      "X,10,-1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### export as .csv\n",
    "\n",
    "allel.vcf_to_csv('C:/SPARK/sample.vcf', 'C:/SPARK/example.csv', fields=['CHROM', 'POS', 'DP'])\n",
    "\n",
    "\n",
    "with open('C:/SPARK/example.csv', mode='r') as f:\n",
    "    print(f.read())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Now lets pull down a massive .vcf file and process it ! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* http://hgdownload.cse.ucsc.edu/gbdb/hg19/1000Genomes/phase3/?C=S;O=A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# downloaded a 1.2 GB file of VCF raw data\n",
    "\n",
    "vcf_path = 'C:/SPARK/ALL.chr1.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 TBresee mkpasswd 1.2G Jun  4 19:33 C:/SPARK/ALL.chr1.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!ls -lh {vcf_path}\n",
    "\n",
    "# bam, a 1.2G file \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[read_vcf] 65536 rows in 4.13s; chunk in 4.13s (15856 rows/s); 1\u0000:2308933\n",
      "[read_vcf] 131072 rows in 8.07s; chunk in 3.94s (16642 rows/s); 1\u0000:4177969\n",
      "[read_vcf] 196608 rows in 12.29s; chunk in 4.22s (15547 rows/s); 1\u0000:6022445\n",
      "[read_vcf] 262144 rows in 16.39s; chunk in 4.10s (15986 rows/s); 1\u0000:8078286\n",
      "[read_vcf] 327680 rows in 20.36s; chunk in 3.98s (16476 rows/s); 1\u0000:10246876\n",
      "[read_vcf] 393216 rows in 24.18s; chunk in 3.82s (17177 rows/s); 1\u0000:12313599\n",
      "[read_vcf] 458752 rows in 28.10s; chunk in 3.92s (16703 rows/s); 1\u0000:15033300\n",
      "[read_vcf] 524288 rows in 32.23s; chunk in 4.13s (15864 rows/s); 1\u0000:17226235\n",
      "[read_vcf] 589824 rows in 36.05s; chunk in 3.81s (17182 rows/s); 1\u0000:19176875\n",
      "[read_vcf] 655360 rows in 39.81s; chunk in 3.77s (17397 rows/s); 1\u0000:21331176\n",
      "[read_vcf] 720896 rows in 43.80s; chunk in 3.98s (16459 rows/s); 1\u0000:23514706\n",
      "[read_vcf] 786432 rows in 47.81s; chunk in 4.01s (16332 rows/s); 1\u0000:25882976\n",
      "[read_vcf] 851968 rows in 51.68s; chunk in 3.87s (16933 rows/s); 1\u0000:28279507\n",
      "[read_vcf] 917504 rows in 55.37s; chunk in 3.69s (17766 rows/s); 1\u0000:30713629\n",
      "[read_vcf] 983040 rows in 59.22s; chunk in 3.85s (17030 rows/s); 1\u0000:32885037\n",
      "[read_vcf] 1048576 rows in 63.37s; chunk in 4.15s (15785 rows/s); 1\u0000:35172507\n",
      "[read_vcf] 1114112 rows in 67.24s; chunk in 3.87s (16937 rows/s); 1\u0000:37478330\n",
      "[read_vcf] 1179648 rows in 71.10s; chunk in 3.87s (16941 rows/s); 1\u0000:39748103\n",
      "[read_vcf] 1245184 rows in 74.99s; chunk in 3.89s (16859 rows/s); 1\u0000:42086852\n",
      "[read_vcf] 1310720 rows in 78.99s; chunk in 4.00s (16393 rows/s); 1\u0000:44445975\n",
      "[read_vcf] 1376256 rows in 83.09s; chunk in 4.10s (15973 rows/s); 1\u0000:46867070\n",
      "[read_vcf] 1441792 rows in 87.49s; chunk in 4.40s (14902 rows/s); 1\u0000:49152193\n",
      "[read_vcf] 1507328 rows in 91.52s; chunk in 4.03s (16279 rows/s); 1\u0000:51791580\n",
      "[read_vcf] 1572864 rows in 95.33s; chunk in 3.81s (17191 rows/s); 1\u0000:54133705\n",
      "[read_vcf] 1638400 rows in 99.06s; chunk in 3.73s (17580 rows/s); 1\u0000:56272802\n",
      "[read_vcf] 1703936 rows in 103.12s; chunk in 4.06s (16134 rows/s); 1\u0000:58498999\n",
      "[read_vcf] 1769472 rows in 107.34s; chunk in 4.22s (15518 rows/s); 1\u0000:60743161\n",
      "[read_vcf] 1835008 rows in 111.96s; chunk in 4.62s (14200 rows/s); 1\u0000:63047401\n",
      "[read_vcf] 1900544 rows in 116.36s; chunk in 4.41s (14870 rows/s); 1\u0000:65448230\n",
      "[read_vcf] 1966080 rows in 120.30s; chunk in 3.94s (16635 rows/s); 1\u0000:67804819\n",
      "[read_vcf] 2031616 rows in 124.25s; chunk in 3.95s (16580 rows/s); 1\u0000:70086719\n",
      "[read_vcf] 2097152 rows in 128.28s; chunk in 4.02s (16283 rows/s); 1\u0000:72517936\n",
      "[read_vcf] 2162688 rows in 132.10s; chunk in 3.82s (17164 rows/s); 1\u0000:74833059\n",
      "[read_vcf] 2228224 rows in 135.88s; chunk in 3.78s (17328 rows/s); 1\u0000:77144733\n",
      "[read_vcf] 2293760 rows in 139.94s; chunk in 4.06s (16154 rows/s); 1\u0000:79495341\n",
      "[read_vcf] 2359296 rows in 143.93s; chunk in 3.99s (16422 rows/s); 1\u0000:81601478\n",
      "[read_vcf] 2424832 rows in 147.88s; chunk in 3.95s (16592 rows/s); 1\u0000:83811587\n",
      "[read_vcf] 2490368 rows in 152.35s; chunk in 4.48s (14633 rows/s); 1\u0000:86217241\n",
      "[read_vcf] 2555904 rows in 156.59s; chunk in 4.24s (15473 rows/s); 1\u0000:88536030\n",
      "[read_vcf] 2621440 rows in 160.57s; chunk in 3.98s (16468 rows/s); 1\u0000:90958154\n",
      "[read_vcf] 2686976 rows in 164.32s; chunk in 3.75s (17471 rows/s); 1\u0000:93388079\n",
      "[read_vcf] 2752512 rows in 168.17s; chunk in 3.85s (17008 rows/s); 1\u0000:95785935\n",
      "[read_vcf] 2818048 rows in 172.07s; chunk in 3.90s (16802 rows/s); 1\u0000:98123341\n",
      "[read_vcf] 2883584 rows in 175.76s; chunk in 3.68s (17785 rows/s); 1\u0000:100426497\n",
      "[read_vcf] 2949120 rows in 179.43s; chunk in 3.67s (17873 rows/s); 1\u0000:102706814\n",
      "[read_vcf] 3014656 rows in 183.30s; chunk in 3.87s (16915 rows/s); 1\u0000:105066606\n",
      "[read_vcf] 3080192 rows in 187.15s; chunk in 3.85s (17012 rows/s); 1\u0000:107035468\n",
      "[read_vcf] 3145728 rows in 191.26s; chunk in 4.11s (15949 rows/s); 1\u0000:109497713\n",
      "[read_vcf] 3211264 rows in 195.18s; chunk in 3.92s (16739 rows/s); 1\u0000:111823987\n",
      "[read_vcf] 3276800 rows in 198.99s; chunk in 3.81s (17191 rows/s); 1\u0000:114123013\n",
      "[read_vcf] 3342336 rows in 202.82s; chunk in 3.83s (17103 rows/s); 1\u0000:116449117\n",
      "[read_vcf] 3407872 rows in 206.76s; chunk in 3.94s (16631 rows/s); 1\u0000:118807176\n",
      "[read_vcf] 3473408 rows in 210.61s; chunk in 3.85s (17020 rows/s); 1\u0000:121361045\n",
      "[read_vcf] 3538944 rows in 214.67s; chunk in 4.06s (16134 rows/s); 1\u0000:146730296\n",
      "[read_vcf] 3604480 rows in 218.40s; chunk in 3.72s (17603 rows/s); 1\u0000:150430552\n",
      "[read_vcf] 3670016 rows in 222.20s; chunk in 3.81s (17209 rows/s); 1\u0000:152825060\n",
      "[read_vcf] 3735552 rows in 225.91s; chunk in 3.70s (17703 rows/s); 1\u0000:155138015\n",
      "[read_vcf] 3801088 rows in 229.79s; chunk in 3.88s (16872 rows/s); 1\u0000:157518826\n",
      "[read_vcf] 3866624 rows in 233.56s; chunk in 3.77s (17401 rows/s); 1\u0000:159647947\n",
      "[read_vcf] 3932160 rows in 237.73s; chunk in 4.17s (15708 rows/s); 1\u0000:161935207\n",
      "[read_vcf] 3997696 rows in 242.04s; chunk in 4.31s (15211 rows/s); 1\u0000:164144117\n",
      "[read_vcf] 4063232 rows in 246.15s; chunk in 4.12s (15914 rows/s); 1\u0000:166317740\n",
      "[read_vcf] 4128768 rows in 250.59s; chunk in 4.44s (14761 rows/s); 1\u0000:168527049\n",
      "[read_vcf] 4194304 rows in 254.47s; chunk in 3.87s (16915 rows/s); 1\u0000:170801668\n",
      "[read_vcf] 4259840 rows in 258.30s; chunk in 3.83s (17124 rows/s); 1\u0000:173165318\n",
      "[read_vcf] 4325376 rows in 262.18s; chunk in 3.89s (16850 rows/s); 1\u0000:175588900\n",
      "[read_vcf] 4390912 rows in 266.51s; chunk in 4.32s (15159 rows/s); 1\u0000:177907681\n",
      "[read_vcf] 4456448 rows in 270.66s; chunk in 4.15s (15784 rows/s); 1\u0000:180243702\n",
      "[read_vcf] 4521984 rows in 274.40s; chunk in 3.74s (17499 rows/s); 1\u0000:182517611\n",
      "[read_vcf] 4587520 rows in 278.06s; chunk in 3.66s (17912 rows/s); 1\u0000:184881497\n",
      "[read_vcf] 4653056 rows in 282.03s; chunk in 3.96s (16534 rows/s); 1\u0000:187198495\n",
      "[read_vcf] 4718592 rows in 286.09s; chunk in 4.07s (16112 rows/s); 1\u0000:189298208\n",
      "[read_vcf] 4784128 rows in 290.03s; chunk in 3.93s (16661 rows/s); 1\u0000:191463222\n",
      "[read_vcf] 4849664 rows in 294.20s; chunk in 4.17s (15700 rows/s); 1\u0000:193813991\n",
      "[read_vcf] 4915200 rows in 298.02s; chunk in 3.82s (17164 rows/s); 1\u0000:195952433\n",
      "[read_vcf] 4980736 rows in 301.96s; chunk in 3.94s (16627 rows/s); 1\u0000:198270907\n",
      "[read_vcf] 5046272 rows in 306.01s; chunk in 4.05s (16174 rows/s); 1\u0000:200656420\n",
      "[read_vcf] 5111808 rows in 309.99s; chunk in 3.98s (16472 rows/s); 1\u0000:202897382\n",
      "[read_vcf] 5177344 rows in 313.95s; chunk in 3.96s (16560 rows/s); 1\u0000:205098886\n",
      "[read_vcf] 5242880 rows in 318.19s; chunk in 4.24s (15452 rows/s); 1\u0000:207870968\n",
      "[read_vcf] 5308416 rows in 322.35s; chunk in 4.16s (15749 rows/s); 1\u0000:210058821\n",
      "[read_vcf] 5373952 rows in 326.43s; chunk in 4.07s (16083 rows/s); 1\u0000:212310880\n",
      "[read_vcf] 5439488 rows in 330.45s; chunk in 4.03s (16271 rows/s); 1\u0000:214630869\n",
      "[read_vcf] 5505024 rows in 334.65s; chunk in 4.20s (15618 rows/s); 1\u0000:216886865\n",
      "[read_vcf] 5570560 rows in 338.62s; chunk in 3.97s (16488 rows/s); 1\u0000:219052919\n",
      "[read_vcf] 5636096 rows in 342.41s; chunk in 3.79s (17296 rows/s); 1\u0000:221366760\n",
      "[read_vcf] 5701632 rows in 346.25s; chunk in 3.83s (17097 rows/s); 1\u0000:223680945\n",
      "[read_vcf] 5767168 rows in 350.12s; chunk in 3.87s (16940 rows/s); 1\u0000:226104697\n",
      "[read_vcf] 5832704 rows in 353.88s; chunk in 3.76s (17426 rows/s); 1\u0000:228405212\n",
      "[read_vcf] 5898240 rows in 357.60s; chunk in 3.72s (17613 rows/s); 1\u0000:230542914\n",
      "[read_vcf] 5963776 rows in 361.43s; chunk in 3.83s (17106 rows/s); 1\u0000:232742459\n",
      "[read_vcf] 6029312 rows in 365.90s; chunk in 4.47s (14668 rows/s); 1\u0000:234886959\n",
      "[read_vcf] 6094848 rows in 370.00s; chunk in 4.11s (15957 rows/s); 1\u0000:237093353\n",
      "[read_vcf] 6160384 rows in 373.96s; chunk in 3.96s (16547 rows/s); 1\u0000:239206947\n",
      "[read_vcf] 6225920 rows in 377.96s; chunk in 3.99s (16410 rows/s); 1\u0000:241305636\n",
      "[read_vcf] 6291456 rows in 382.09s; chunk in 4.13s (15864 rows/s); 1\u0000:243454558\n",
      "[read_vcf] 6356992 rows in 386.11s; chunk in 4.02s (16291 rows/s); 1\u0000:245642248\n",
      "[read_vcf] 6422528 rows in 389.98s; chunk in 3.87s (16933 rows/s); 1\u0000:247641903\n",
      "[read_vcf] 6468094 rows in 392.78s; chunk in 2.80s (16288 rows/s)\n",
      "[read_vcf] all done (16467 rows/s)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "callset = allel.read_vcf(vcf_path, fields=['numalt'], log=sys.stdout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "When processing larger VCF files it’s useful to get some feedback on how fast things are going. \n",
    "\n",
    "Ultimately I am going to extract all the data from this VCF file into a Zarr store. \n",
    "However, before I do that, I’m going to check how many alternate alleles I should expect. \n",
    "I’m going to do that by extracting just the ‘numalt’ field, which scikit-allel will compute \n",
    "from the number of values in the ‘ALT’ field:\n",
    "``` \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# let’s see what the largest number of alternate alleles is:\n",
    "\n",
    "numalt = callset['variants/numalt']\n",
    "\n",
    "np.max(numalt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([      0, 6437262,   29538,    1064,     165,      49,      10,\n",
       "             5,       0,       0,       0,       0,       1], dtype=int64)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Out of interest, how many variants are multi-allelic?\n",
    "count_numalt = np.bincount(numalt)\n",
    "count_numalt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30832"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "n_multiallelic = np.sum(count_numalt[2:])\n",
    "n_multiallelic\n",
    "\n",
    "# So there are only a very small number of multi-allelic variants (30,832), the vast majority (6,437,262) \n",
    "# have just one alternate allele.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
