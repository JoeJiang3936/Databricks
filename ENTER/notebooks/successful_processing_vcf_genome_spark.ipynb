{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Generation DNA Genome Sequencing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  *introduction to SparkSQL*   \n",
    ">  *reading DNA .vcf files*    \n",
    ">  *processing .vcf files in bulk to derive insights*  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Tom Bresee*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.  &nbsp;  Load Relevant Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# my setup:\n",
    "\n",
    "#   In my case, this is running Anaconda Windows10 scenario\n",
    "\n",
    "#   Ill do the same version but on Databricks and straight ApachSpark \n",
    "#   in another file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load basic libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import random\n",
    "from random import random\n",
    "from operator import add\n",
    "import os\n",
    "from time import time\n",
    "import sys\n",
    "import time\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import more libraries\n",
    "import pyspark\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "# SQL\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "# Alchemy\n",
    "from sqlalchemy.engine import create_engine\n",
    "# ALS\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.mllib.recommendation import ALS\n",
    "# Logistic Regression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "# Kmeans\n",
    "from pyspark.mllib.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the spark \n",
    "import findspark\n",
    "#findspark.find()\n",
    "findspark.init()\n",
    "# to be safe on windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.  &nbsp; This is the command you start with on Windows 10 to get SparkSQL going"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkSession in Spark 2.0 provides builtin support for Hive features including the ability to write queries using HiveQL, access to Hive UDFs, and the ability to read data from Hive tables. To use these features, you do not need to have an existing Hive setup FYI..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# READ:\n",
    "#   The entry point into **all** functionality in Spark is the *SparkSession* class.\n",
    "#   There are no longer sep instances depending on your application\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# SparkSession in Spark 2.0 provides builtin support for Hive features including \n",
    "# the ability to write queries using HiveQL, access to Hive UDFs, and the ability \n",
    "# to read data from Hive tables. To use these features, you do not need to have an \n",
    "# existing Hive setup.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x000002370BE61588>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(spark)\n",
    "\n",
    "# see how you now have a .sql.session.SparkSession (it is sql specific)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.session.SparkSession'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(type(spark))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://TM0493322.gsm1900.org:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Python Spark SQL basic example</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2370be61588>"
      ]
     },
     "execution_count": 674,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Builder\n",
      "builder\n",
      "catalog\n",
      "conf\n",
      "createDataFrame\n",
      "newSession\n",
      "range\n",
      "read\n",
      "readStream\n",
      "sparkContext\n",
      "sql\n",
      "stop\n",
      "streams\n",
      "table\n",
      "udf\n",
      "version\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in dir(spark):\n",
    "    if not i.startswith(\"_\"):\n",
    "        print(i)\n",
    "        \n",
    "# these are the spark.methods available once you create the context \n",
    "# i.e. spark.read is a method or spark.udf is a method, etc\n",
    "\n",
    "# spark.stop() for instance will stop the sparksession\n",
    "# spark.\n",
    "\n",
    "# you will see me do this a lot, going thru each method is the way to really understand this stuff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOU ARE CURRENTLY RUNNING APACHE SPARK VERSION 2.4.3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"YOU ARE CURRENTLY RUNNING APACHE SPARK VERSION\", spark.version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In earlier versions of Spark, spark context was the entry point for Spark.  As RDD was main API, it was created and manipulated using context API’s, for every other API,we needed to use different contexts. For streaming, we needed StreamingContext, for SQL sqlContext and for hive HiveContext, etc etc.  \n",
    "\n",
    "But as DataSet and Dataframe API’s are becoming new standard API’s we need an entry point build for them. \n",
    "\n",
    "So in Spark 2.0, we have a new entry point for DataSet and Dataframe API’s called a Spark Session.\n",
    "\n",
    "SparkSession is essentially combination of SQLContext, HiveContext and future StreamingContext. \n",
    "\n",
    "All the API’s available on those contexts are available on spark session also. Spark session internally has a spark context for actual computation.\n",
    "\n",
    "So if you get confused, you need to remember there was a major shift in how you created contexts with Spark 2.x..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.  &nbsp;  Begin Diving into SparkSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inferring the Schema\n",
    "\n",
    "With a SQLContext, we are ready to create a DataFrame from our existing RDD. But first we need to tell Spark SQL the schema in our data.  Spark SQL can convert an RDD of Row objects to a DataFrame. Rows are constructed by passing a list of key/value pairs as kwargs to the Row class.  The keys define the column names, and the types are inferred by looking at the first row. Therefore, it is important that there is no missing data in the first row of the RDD in order to properly infer the schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a SparkSession, applications can create DataFrames from an existing RDD, from a Hive table, or from Spark data sources.\n",
    "As an example, the following creates a DataFrame based on the content of a JSON file:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='default', description='default database', locationUri='file:/C:/Users/tbresee/spark-warehouse')]"
      ]
     },
     "execution_count": 677,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# print out all existing databases ! \n",
    "\n",
    "spark.catalog.listDatabases()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'default'"
      ]
     },
     "execution_count": 678,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# current database:\n",
    "\n",
    "spark.catalog.currentDatabase()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='names', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='people', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='products', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 679,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# list all current tables, there are none yet ...\n",
    "\n",
    "spark.catalog.listTables()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cacheTable\n",
      "clearCache\n",
      "createExternalTable\n",
      "createTable\n",
      "currentDatabase\n",
      "dropGlobalTempView\n",
      "dropTempView\n",
      "isCached\n",
      "listColumns\n",
      "listDatabases\n",
      "listFunctions\n",
      "listTables\n",
      "recoverPartitions\n",
      "refreshByPath\n",
      "refreshTable\n",
      "registerFunction\n",
      "setCurrentDatabase\n",
      "uncacheTable\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# list all .methods under catalog:\n",
    "\n",
    "for i in dir(spark.catalog): \n",
    "    if not i.startswith(\"_\"): \n",
    "        print(i)\n",
    "\n",
    "# you will see me do this alot, you want to see the commands that are possible for spark.catalog.X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading a random json file into a dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------+----------------+---------+\n",
      "|                 _id|        product_name|quantity|        supplier|unit_cost|\n",
      "+--------------------+--------------------+--------+----------------+---------+\n",
      "|5968dd23fc13ae04d...|  sildenafil citrate|     261|      Wisozk Inc|   $10.47|\n",
      "|5968dd23fc13ae04d...|Mountain Juniperu...|     292| Keebler-Hilpert|    $8.74|\n",
      "|5968dd23fc13ae04d...|Dextromathorphan HBr|     211|Schmitt-Weissnat|   $20.53|\n",
      "+--------------------+--------------------+--------+----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# spark is the existing sql SparkSession\n",
    "df = spark.read.json(\"C:/SPARK/examples/example.json\")\n",
    "\n",
    "# Displays the content of the DataFrame to stdout\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the df instance is this type ->   <class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"the df instance is this type ->  \", type(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv\n",
      "format\n",
      "jdbc\n",
      "json\n",
      "load\n",
      "option\n",
      "options\n",
      "orc\n",
      "parquet\n",
      "schema\n",
      "table\n",
      "text\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# list out the methods under read, i.e. how can i read stuff in,what are my read options ?  \n",
    "\n",
    "for i in dir(spark.read):\n",
    "    if not i.startswith(\"_\"):\n",
    "        print(i)\n",
    "\n",
    "# i.e. u have spark.read.csv option, spark.read.json option, spark.read.parquet option, etc ! \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: string (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- quantity: long (nullable = true)\n",
      " |-- supplier: string (nullable = true)\n",
      " |-- unit_cost: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# basic examples of structured data processing using Datasets\n",
    "\n",
    "# In Python, it’s possible to access a DataFrame’s columns either by attribute (df.age) or \n",
    "# by indexing (df['age']).   Its recommended  you use the second, but yes, the first is convenient, \n",
    "# we have all done it at some point ...\n",
    "\n",
    "# Print the schema in a tree format\n",
    "df.printSchema()\n",
    "\n",
    "# you are in the databases world now, think like that...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|        supplier|\n",
      "+----------------+\n",
      "|      Wisozk Inc|\n",
      "| Keebler-Hilpert|\n",
      "|Schmitt-Weissnat|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# so if you have ever messed with pandas dataframes, this is similiar, but it does \n",
    "# have a fair amount of differences, so think of this as a new construct...\n",
    "\n",
    "# Select only the \"supplier\" column\n",
    "df.select(\"supplier\").show()\n",
    "\n",
    "# like select statements in sql \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|(quantity + 1000)|\n",
      "+-----------------+\n",
      "|             1261|\n",
      "|             1292|\n",
      "|             1211|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Select the quantity col but increment the 'quantity' by 1000\n",
    "df.select(df['quantity'] + 1000).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 687,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df.select()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+\n",
      "|quantity|        supplier|\n",
      "+--------+----------------+\n",
      "|     261|      Wisozk Inc|\n",
      "|     292| Keebler-Hilpert|\n",
      "|     211|Schmitt-Weissnat|\n",
      "+--------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# create a new df with certain cols only \n",
    "\n",
    "df2 = df.select(\"quantity\", \"supplier\")\n",
    "df2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------+----------------+---------+\n",
      "|                 _id|        product_name|quantity|        supplier|unit_cost|\n",
      "+--------------------+--------------------+--------+----------------+---------+\n",
      "|5968dd23fc13ae04d...|  sildenafil citrate|     261|      Wisozk Inc|   $10.47|\n",
      "|5968dd23fc13ae04d...|Mountain Juniperu...|     292| Keebler-Hilpert|    $8.74|\n",
      "|5968dd23fc13ae04d...|Dextromathorphan HBr|     211|Schmitt-Weissnat|   $20.53|\n",
      "+--------------------+--------------------+--------+----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# when you use SQL, you use commands like select * from <name>, its the same thing...\n",
    "\n",
    "df.select('*').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(type(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# note:  select is a transformation, not an action \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------+---------------+---------+\n",
      "|                 _id|        product_name|quantity|       supplier|unit_cost|\n",
      "+--------------------+--------------------+--------+---------------+---------+\n",
      "|5968dd23fc13ae04d...|  sildenafil citrate|     261|     Wisozk Inc|   $10.47|\n",
      "|5968dd23fc13ae04d...|Mountain Juniperu...|     292|Keebler-Hilpert|    $8.74|\n",
      "+--------------------+--------------------+--------+---------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Select people older than 21\n",
    "df.filter(df['quantity'] > 250).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|quantity|count|\n",
      "+--------+-----+\n",
      "|     292|    1|\n",
      "|     261|    1|\n",
      "|     211|    1|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Count products by quantity\n",
    "df.groupBy(\"quantity\").count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------+----------------+---------+\n",
      "|                 _id|        product_name|quantity|        supplier|unit_cost|\n",
      "+--------------------+--------------------+--------+----------------+---------+\n",
      "|5968dd23fc13ae04d...|  sildenafil citrate|     261|      Wisozk Inc|   $10.47|\n",
      "|5968dd23fc13ae04d...|Mountain Juniperu...|     292| Keebler-Hilpert|    $8.74|\n",
      "|5968dd23fc13ae04d...|Dextromathorphan HBr|     211|Schmitt-Weissnat|   $20.53|\n",
      "+--------------------+--------------------+--------+----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# The sql function on a SparkSession enables applications to run SQL queries \n",
    "# programmatically and returns the result as a DataFrame.\n",
    "# Register the DataFrame as a SQL temporary view\n",
    "df.createOrReplaceTempView(\"products\")\n",
    "\n",
    "sqlDF = spark.sql(\"SELECT * FROM products\")\n",
    "sqlDF.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # $example on:global_temp_view$\n",
    "# # Register the DataFrame as a global temporary view\n",
    "# df.createGlobalTempView(\"people\")\n",
    "\n",
    "# # Global temporary view is tied to a system preserved database `global_temp`\n",
    "# spark.sql(\"SELECT * FROM global_temp.people\").show()\n",
    "\n",
    "# # Global temporary view is cross-session\n",
    "# spark.newSession().sql(\"SELECT * FROM global_temp.people\").show()    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# $example on:schema_inferring$\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Load a text file and convert each line to a Row.\n",
    "lines = sc.textFile(\"C:/SPARK/examples/src/main/resources/people.txt\")\n",
    "parts = lines.map(lambda l: l.split(\",\"))\n",
    "people = parts.map(lambda p: Row(name=p[0], age=int(p[1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Justin\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Infer the schema, and register the DataFrame as a table.\n",
    "schemaPeople = spark.createDataFrame(people)\n",
    "schemaPeople.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# SQL can be run over DataFrames that have been registered as a table\n",
    "teenagers = spark.sql(\"SELECT name FROM people WHERE age >= 13 AND age <= 19\")\n",
    "\n",
    "# The results of SQL queries are Dataframe objects...\n",
    "# rdd returns the content as an :class:`pyspark.RDD` of :class:`Row`.\n",
    "teenNames = teenagers.rdd.map(lambda p: \"Name: \" + p.name).collect()\n",
    "\n",
    "for name in teenNames:\n",
    "      print(name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  name|\n",
      "+------+\n",
      "|Justin|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "teenagers.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://TM0493322.gsm1900.org:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Python Spark SQL basic example</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=Python Spark SQL basic example>"
      ]
     },
     "execution_count": 699,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://TM0493322.gsm1900.org:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Python Spark SQL basic example</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2370be61588>"
      ]
     },
     "execution_count": 700,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### quickref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "A simple example demonstrating basic Spark SQL features.\n",
    "Run with:\n",
    "  ./bin/spark-submit examples/src/main/python/sql/basic.py\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "# $example on:init_session$\n",
    "from pyspark.sql import SparkSession\n",
    "# $example off:init_session$\n",
    "\n",
    "# $example on:schema_inferring$\n",
    "from pyspark.sql import Row\n",
    "# $example off:schema_inferring$\n",
    "\n",
    "# $example on:programmatic_schema$\n",
    "# Import data types\n",
    "from pyspark.sql.types import *\n",
    "# $example off:programmatic_schema$\n",
    "\n",
    "\n",
    "def basic_df_example(spark):\n",
    "    # $example on:create_df$\n",
    "    # spark is an existing SparkSession\n",
    "    df = spark.read.json(\"examples/src/main/resources/people.json\")\n",
    "    # Displays the content of the DataFrame to stdout\n",
    "    df.show()\n",
    "    # +----+-------+\n",
    "    # | age|   name|\n",
    "    # +----+-------+\n",
    "    # |null|Michael|\n",
    "    # |  30|   Andy|\n",
    "    # |  19| Justin|\n",
    "    # +----+-------+\n",
    "    # $example off:create_df$\n",
    "\n",
    "    # $example on:untyped_ops$\n",
    "    # spark, df are from the previous example\n",
    "    # Print the schema in a tree format\n",
    "    df.printSchema()\n",
    "    # root\n",
    "    # |-- age: long (nullable = true)\n",
    "    # |-- name: string (nullable = true)\n",
    "\n",
    "    # Select only the \"name\" column\n",
    "    df.select(\"name\").show()\n",
    "    # +-------+\n",
    "    # |   name|\n",
    "    # +-------+\n",
    "    # |Michael|\n",
    "    # |   Andy|\n",
    "    # | Justin|\n",
    "    # +-------+\n",
    "\n",
    "    # Select everybody, but increment the age by 1\n",
    "    df.select(df['name'], df['age'] + 1).show()\n",
    "    # +-------+---------+\n",
    "    # |   name|(age + 1)|\n",
    "    # +-------+---------+\n",
    "    # |Michael|     null|\n",
    "    # |   Andy|       31|\n",
    "    # | Justin|       20|\n",
    "    # +-------+---------+\n",
    "\n",
    "    # Select people older than 21\n",
    "    df.filter(df['age'] > 21).show()\n",
    "    # +---+----+\n",
    "    # |age|name|\n",
    "    # +---+----+\n",
    "    # | 30|Andy|\n",
    "    # +---+----+\n",
    "\n",
    "    # Count people by age\n",
    "    df.groupBy(\"age\").count().show()\n",
    "    # +----+-----+\n",
    "    # | age|count|\n",
    "    # +----+-----+\n",
    "    # |  19|    1|\n",
    "    # |null|    1|\n",
    "    # |  30|    1|\n",
    "    # +----+-----+\n",
    "    # $example off:untyped_ops$\n",
    "\n",
    "    # $example on:run_sql$\n",
    "    # Register the DataFrame as a SQL temporary view\n",
    "    df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "    sqlDF = spark.sql(\"SELECT * FROM people\")\n",
    "    sqlDF.show()\n",
    "    # +----+-------+\n",
    "    # | age|   name|\n",
    "    # +----+-------+\n",
    "    # |null|Michael|\n",
    "    # |  30|   Andy|\n",
    "    # |  19| Justin|\n",
    "    # +----+-------+\n",
    "    # $example off:run_sql$\n",
    "\n",
    "    # $example on:global_temp_view$\n",
    "    # Register the DataFrame as a global temporary view\n",
    "    df.createGlobalTempView(\"people\")\n",
    "\n",
    "    # Global temporary view is tied to a system preserved database `global_temp`\n",
    "    spark.sql(\"SELECT * FROM global_temp.people\").show()\n",
    "    # +----+-------+\n",
    "    # | age|   name|\n",
    "    # +----+-------+\n",
    "    # |null|Michael|\n",
    "    # |  30|   Andy|\n",
    "    # |  19| Justin|\n",
    "    # +----+-------+\n",
    "\n",
    "    # Global temporary view is cross-session\n",
    "    spark.newSession().sql(\"SELECT * FROM global_temp.people\").show()\n",
    "    # +----+-------+\n",
    "    # | age|   name|\n",
    "    # +----+-------+\n",
    "    # |null|Michael|\n",
    "    # |  30|   Andy|\n",
    "    # |  19| Justin|\n",
    "    # +----+-------+\n",
    "    # $example off:global_temp_view$\n",
    "\n",
    "\n",
    "def schema_inference_example(spark):\n",
    "    # $example on:schema_inferring$\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "    # Load a text file and convert each line to a Row.\n",
    "    lines = sc.textFile(\"examples/src/main/resources/people.txt\")\n",
    "    parts = lines.map(lambda l: l.split(\",\"))\n",
    "    people = parts.map(lambda p: Row(name=p[0], age=int(p[1])))\n",
    "\n",
    "    # Infer the schema, and register the DataFrame as a table.\n",
    "    schemaPeople = spark.createDataFrame(people)\n",
    "    schemaPeople.createOrReplaceTempView(\"people\")\n",
    "\n",
    "    # SQL can be run over DataFrames that have been registered as a table.\n",
    "    teenagers = spark.sql(\"SELECT name FROM people WHERE age >= 13 AND age <= 19\")\n",
    "\n",
    "    # The results of SQL queries are Dataframe objects.\n",
    "    # rdd returns the content as an :class:`pyspark.RDD` of :class:`Row`.\n",
    "    teenNames = teenagers.rdd.map(lambda p: \"Name: \" + p.name).collect()\n",
    "    for name in teenNames:\n",
    "        print(name)\n",
    "    # Name: Justin\n",
    "    # $example off:schema_inferring$\n",
    "\n",
    "\n",
    "def programmatic_schema_example(spark):\n",
    "    # $example on:programmatic_schema$\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "    # Load a text file and convert each line to a Row.\n",
    "    lines = sc.textFile(\"examples/src/main/resources/people.txt\")\n",
    "    parts = lines.map(lambda l: l.split(\",\"))\n",
    "    # Each line is converted to a tuple.\n",
    "    people = parts.map(lambda p: (p[0], p[1].strip()))\n",
    "\n",
    "    # The schema is encoded in a string.\n",
    "    schemaString = \"name age\"\n",
    "\n",
    "    fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\n",
    "    schema = StructType(fields)\n",
    "\n",
    "    # Apply the schema to the RDD.\n",
    "    schemaPeople = spark.createDataFrame(people, schema)\n",
    "\n",
    "    # Creates a temporary view using the DataFrame\n",
    "    schemaPeople.createOrReplaceTempView(\"people\")\n",
    "\n",
    "    # SQL can be run over DataFrames that have been registered as a table.\n",
    "    results = spark.sql(\"SELECT name FROM people\")\n",
    "\n",
    "    results.show()\n",
    "    # +-------+\n",
    "    # |   name|\n",
    "    # +-------+\n",
    "    # |Michael|\n",
    "    # |   Andy|\n",
    "    # | Justin|\n",
    "    # +-------+\n",
    "    # $example off:programmatic_schema$\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # $example on:init_session$\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Python Spark SQL basic example\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n",
    "    # $example off:init_session$\n",
    "\n",
    "    basic_df_example(spark)\n",
    "    schema_inference_example(spark)\n",
    "    programmatic_schema_example(spark)\n",
    "\n",
    "    spark.stop()\n",
    "\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sc.appName:\t\t Python Spark SQL basic example\n",
      "sc.applicationId:\t local-1559796355980\n",
      "sc.master:\t\t local[*]\n",
      "sc.appName:\t\t Python Spark SQL basic example\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Spark Properties, the manual way, instead of having to go the UI ! \n",
    "\n",
    "print(\"sc.appName:\\t\\t\", sc.appName)\n",
    "\n",
    "print(\"sc.applicationId:\\t\", sc.applicationId)\n",
    "\n",
    "print(\"sc.master:\\t\\t\", sc.master)\n",
    "\n",
    "print(\"sc.appName:\\t\\t\", sc.appName)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Spark SQL can convert an RDD of Row objects to a DataFrame, inferring the datatypes. Rows are constructed by passing a list of key/value pairs as kwargs to the Row class. The keys of this list define the column names of the table, and the types are inferred by sampling the whole dataset, similar to the inference that is performed on JSON files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Justin\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import Row\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Load a text file and convert each line to a Row.\n",
    "lines = sc.textFile(\"C:/SPARK/examples/src/main/resources/people.txt\")\n",
    "parts = lines.map(lambda l: l.split(\",\"))\n",
    "people = parts.map(lambda p: Row(name=p[0], age=int(p[1])))\n",
    "\n",
    "# Infer the schema, and register the DataFrame as a table.\n",
    "schemaPeople = spark.createDataFrame(people)\n",
    "schemaPeople.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# SQL can be run over DataFrames that have been registered as a table.\n",
    "teenagers = spark.sql(\"SELECT name FROM people WHERE age >= 13 AND age <= 19\")\n",
    "\n",
    "# The results of SQL queries are Dataframe objects.\n",
    "# rdd returns the content as an :class:`pyspark.RDD` of :class:`Row`.\n",
    "teenNames = teenagers.rdd.map(lambda p: \"Name: \" + p.name).collect()\n",
    "for name in teenNames:\n",
    "    print(name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL supports operating on a variety of data sources through the DataFrame interface. A DataFrame can be operated on using relational transformations and can also be used to create a temporary view. Registering a DataFrame as a temporary view allows you to run SQL queries over its data. This section describes the general methods for loading and saving data using the Spark Data Sources and then goes into specific options that are available for the built-in data sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+------+\n",
      "|  name|\n",
      "+------+\n",
      "|Justin|\n",
      "+------+\n",
      "\n",
      "+----------------+----+\n",
      "|         address|name|\n",
      "+----------------+----+\n",
      "|[Columbus, Ohio]| Yin|\n",
      "+----------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "path = \"C:/SPARK/examples/src/main/resources/people.json\"\n",
    "peopleDF = spark.read.json(path)\n",
    "\n",
    "# The inferred schema can be visualized using the printSchema() method\n",
    "peopleDF.printSchema()\n",
    "# root\n",
    "#  |-- age: long (nullable = true)\n",
    "#  |-- name: string (nullable = true)\n",
    "\n",
    "# Creates a temporary view using the DataFrame\n",
    "peopleDF.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# SQL statements can be run by using the sql methods provided by spark\n",
    "teenagerNamesDF = spark.sql(\"SELECT name FROM people WHERE age BETWEEN 13 AND 19\")\n",
    "teenagerNamesDF.show()\n",
    "# +------+\n",
    "# |  name|\n",
    "# +------+\n",
    "# |Justin|\n",
    "# +------+\n",
    "\n",
    "# Alternatively, a DataFrame can be created for a JSON dataset represented by\n",
    "# an RDD[String] storing one JSON object per string\n",
    "jsonStrings = ['{\"name\":\"Yin\",\"address\":{\"city\":\"Columbus\",\"state\":\"Ohio\"}}']\n",
    "otherPeopleRDD = sc.parallelize(jsonStrings)\n",
    "otherPeople = spark.read.json(otherPeopleRDD)\n",
    "otherPeople.show()\n",
    "# +---------------+----+\n",
    "# |        address|name|\n",
    "# +---------------+----+\n",
    "# |[Columbus,Ohio]| Yin|\n",
    "# +---------------+----+\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+---------------+\n",
      "| name|   nickname|number_of_files|\n",
      "+-----+-----------+---------------+\n",
      "| tom1| dragonfly1|            111|\n",
      "| tom2| dragonfly2|            200|\n",
      "| tom3| dragonfly3|            100|\n",
      "| tom4| dragonfly4|            400|\n",
      "| tom5| dragonfly5|            600|\n",
      "| tom6| dragonfly6|            800|\n",
      "| tom7| dragonfly7|            200|\n",
      "| tom8| dragonfly8|            100|\n",
      "| tom9| dragonfly9|             50|\n",
      "|tom10|     dragon|            999|\n",
      "|tom11|  dragonone|            100|\n",
      "|tom12| dragonfly2|            200|\n",
      "|tom13|dragonfly23|            100|\n",
      "|tom14| dragonfly4|            400|\n",
      "|tom15|dragonfly45|            654|\n",
      "|tom16| dragonfly6|            800|\n",
      "|tom17| dragonfly7|            200|\n",
      "|tom18|dragonfly18|            100|\n",
      "|tom19|dragonfly09|             50|\n",
      "|tom20|dragonfly10|            500|\n",
      "+-----+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df2 = spark.read.json(\"C:/SPARK/examples/test.json\")\n",
    "df2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- nickname: string (nullable = true)\n",
      " |-- number_of_files: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.createOrReplaceTempView(\"names\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| name|\n",
      "+-----+\n",
      "| tom1|\n",
      "| tom2|\n",
      "| tom3|\n",
      "| tom7|\n",
      "| tom8|\n",
      "|tom11|\n",
      "|tom12|\n",
      "|tom13|\n",
      "|tom17|\n",
      "|tom18|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "output = spark.sql(\"SELECT name FROM names WHERE number_of_files BETWEEN 100 AND 300\")\n",
    "output.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# # spark is an existing SparkSession\n",
    "# dfs = spark.read.json(\"C:/SPARK/examples/people.json\")\n",
    "\n",
    "# # Displays the content of the DataFrame to stdout\n",
    "# dfs.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------+----------------+---------+\n",
      "|                 _id|        product_name|quantity|        supplier|unit_cost|\n",
      "+--------------------+--------------------+--------+----------------+---------+\n",
      "|5968dd23fc13ae04d...|  sildenafil citrate|     261|      Wisozk Inc|   $10.47|\n",
      "|5968dd23fc13ae04d...|Mountain Juniperu...|     292| Keebler-Hilpert|    $8.74|\n",
      "|5968dd23fc13ae04d...|Dextromathorphan HBr|     211|Schmitt-Weissnat|   $20.53|\n",
      "+--------------------+--------------------+--------+----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# spark is an existing SparkSession\n",
    "dfs = spark.read.json(\"C:/SPARK/examples/example.json\")\n",
    "\n",
    "# Displays the content of the DataFrame to stdout\n",
    "dfs.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  https://spark.apache.org/docs/latest/sql-data-sources-json.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: string (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- quantity: long (nullable = true)\n",
      " |-- supplier: string (nullable = true)\n",
      " |-- unit_cost: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# The inferred schema can be visualized using the printSchema() method\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D.  &nbsp;  SQL go "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Creates a temporary view using the DataFrame\n",
    "# df.createOrReplaceTempView(\"people\")  # in my case would be df.createOrReplace\n",
    "\n",
    "# # SQL statements can be run by using the sql methods provided by spark\n",
    "# df_andy = sqlSparkContext.sql(\"SELECT name FROM people WHERE age BETWEEN 20 and 40\")\n",
    "# df_andy.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df_michael = sqlSparkContext.sql(\"SELECT name FROM people WHERE name = 'Michael'\")\n",
    "# df_michael.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#---  REFERENCE - READING OPTIONS  ---\n",
    "\n",
    "# dir(sqlSparkContext.read)\n",
    "\n",
    "#  'csv',\n",
    "#  'format',\n",
    "#  'jdbc',\n",
    "#  'json',\n",
    "#  'load',\n",
    "#  'option',\n",
    "#  'options',\n",
    "#  'orc',\n",
    "#  'parquet',\n",
    "#  'schema',\n",
    "#  'table',\n",
    "#  'text']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E.  &nbsp; Go Off "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_t = spark.read.load(\"C:/SPARK/examples/src/main/resources/users.parquet\")\n",
    "\n",
    "print(type(df_t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agg\n",
      "alias\n",
      "approxQuantile\n",
      "cache\n",
      "checkpoint\n",
      "coalesce\n",
      "colRegex\n",
      "collect\n",
      "columns\n",
      "corr\n",
      "count\n",
      "cov\n",
      "createGlobalTempView\n",
      "createOrReplaceGlobalTempView\n",
      "createOrReplaceTempView\n",
      "createTempView\n",
      "crossJoin\n",
      "crosstab\n",
      "cube\n",
      "describe\n",
      "distinct\n",
      "drop\n",
      "dropDuplicates\n",
      "drop_duplicates\n",
      "dropna\n",
      "dtypes\n",
      "exceptAll\n",
      "explain\n",
      "fillna\n",
      "filter\n",
      "first\n",
      "foreach\n",
      "foreachPartition\n",
      "freqItems\n",
      "groupBy\n",
      "groupby\n",
      "head\n",
      "hint\n",
      "intersect\n",
      "intersectAll\n",
      "isLocal\n",
      "isStreaming\n",
      "is_cached\n",
      "join\n",
      "limit\n",
      "localCheckpoint\n",
      "na\n",
      "orderBy\n",
      "persist\n",
      "printSchema\n",
      "randomSplit\n",
      "rdd\n",
      "registerTempTable\n",
      "repartition\n",
      "repartitionByRange\n",
      "replace\n",
      "rollup\n",
      "sample\n",
      "sampleBy\n",
      "schema\n",
      "select\n",
      "selectExpr\n",
      "show\n",
      "sort\n",
      "sortWithinPartitions\n",
      "sql_ctx\n",
      "stat\n",
      "storageLevel\n",
      "subtract\n",
      "summary\n",
      "take\n",
      "toDF\n",
      "toJSON\n",
      "toLocalIterator\n",
      "toPandas\n",
      "union\n",
      "unionAll\n",
      "unionByName\n",
      "unpersist\n",
      "where\n",
      "withColumn\n",
      "withColumnRenamed\n",
      "withWatermark\n",
      "write\n",
      "writeStream\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#---  REFERENCE - your option methods for this DF concept  ---\n",
    "\n",
    "for i in dir(df_t):\n",
    "    if not i.startswith(\"_\"):\n",
    "        print(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A DataFrame is a distributed collection of data, which is organized into named columns. Conceptually, it is equivalent to relational tables with good optimization techniques.\n",
    "\n",
    "A DataFrame can be constructed from an array of different sources such as Hive tables, Structured Data files, external databases, or existing RDDs. This API was designed for modern Big Data and data science applications taking inspiration from DataFrame in R Programming and Pandas in Python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "Features of DataFrame\n",
    "\n",
    "Here is a set of few characteristic features of DataFrame\n",
    "\n",
    "Ability to process the data in the size of Kilobytes to Petabytes on a single node cluster to large cluster.\n",
    "\n",
    "Supports different data formats (Avro, csv, elastic search, and Cassandra) and storage systems (HDFS, HIVE tables, mysql, etc).\n",
    "\n",
    "State of art optimization and code generation through the Spark SQL Catalyst optimizer (tree transformation framework).\n",
    "\n",
    "Can be easily integrated with all Big Data tools and frameworks via Spark-Core.\n",
    "\n",
    "Provides API for Python, Java, Scala, and R Programming.\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Spark introduces a programming module for structured data processing called Spark SQL. It provides a programming abstraction called DataFrame and can act as distributed SQL query engine.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "The following command is used for initializing the SparkContext through spark-shell.\n",
    "\n",
    "$ spark-shell\n",
    "By default, the SparkContext object is initialized with the name sc when the spark-shell starts.\n",
    "\n",
    "Use the following command to create SQLContext.\n",
    "\n",
    "scala> val sqlcontext = new org.apache.spark.sql.SQLContext(sc)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* some data files \n",
    "   *  https://github.com/apache/spark/tree/master/examples/src/main/resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# michael = spark.sql(\"SELECT name FROM people WHERE name = 'Michael'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_687',\n",
       " '_504',\n",
       " 'schemaPeople',\n",
       " '_446',\n",
       " '_641',\n",
       " '_417',\n",
       " 'df_emp',\n",
       " 'df2',\n",
       " '___',\n",
       " '_138',\n",
       " 'dfs',\n",
       " '_208',\n",
       " 'otherPeople',\n",
       " 'df_users',\n",
       " '_253',\n",
       " 'teenagers',\n",
       " 'teenagerNamesDF',\n",
       " '_478',\n",
       " 'df_t',\n",
       " '_609',\n",
       " 'df',\n",
       " 'output',\n",
       " '_336',\n",
       " 'sqlDF',\n",
       " '_385',\n",
       " 'peopleDF',\n",
       " '_166',\n",
       " '_299',\n",
       " '_357']"
      ]
     },
     "execution_count": 718,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# ---  REFERENCE - list out the dataframes currently created ! --- \n",
    "\n",
    "def list_dataframes():\n",
    "    from pyspark.sql import DataFrame\n",
    "    return [k for (k, v) in globals().items() if isinstance(v, DataFrame)]\n",
    "\n",
    "list_dataframes()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, salary: bigint]"
      ]
     },
     "execution_count": 719,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|   name|salary|\n",
      "+-------+------+\n",
      "|Michael|  3000|\n",
      "|   Andy|  4500|\n",
      "| Justin|  3500|\n",
      "|  Berta|  4000|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "path = 'C:\\SPARK\\examples\\src\\main\\python\\employees.json'\n",
    "\n",
    "df_emp = spark.read.json(path)    \n",
    "\n",
    "df_emp\n",
    "df_emp.show()\n",
    "\n",
    "# scala> val dfs = sqlContext.read.json(\"employee.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_687', '_504', '_', 'schemaPeople', '_446', '_641', '_417', 'df_emp', 'df2', '_138', 'dfs', '_208', '_719', 'otherPeople', 'df_users', '_253', 'teenagers', 'teenagerNamesDF', '_478', 'df_t', '_609', 'df', 'output', '_336', 'sqlDF', '_385', 'peopleDF', '_166', '_299', '_357']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# how many DF do i have right now ? \n",
    "print(list_dataframes())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_emp.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "|  Berta|\n",
      "+-------+\n",
      "\n",
      "+------+\n",
      "|salary|\n",
      "+------+\n",
      "|  3000|\n",
      "|  4500|\n",
      "|  3500|\n",
      "|  4000|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# direct call ! ! ! \n",
    "\n",
    "df_emp.select(\"name\").show()\n",
    "\n",
    "df_emp.select(\"salary\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|salary|count|\n",
      "+------+-----+\n",
      "|  4500|    1|\n",
      "|  4000|    1|\n",
      "|  3500|    1|\n",
      "|  3000|    1|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# group by salary \n",
    "df_emp.groupBy(\"salary\").count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|  name|salary|\n",
      "+------+------+\n",
      "|  Andy|  4500|\n",
      "|Justin|  3500|\n",
      "| Berta|  4000|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_emp.filter(df_emp[\"salary\"] > 3200).show()\n",
    "# interesting:  scala uses () but python uses []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# peopleDF = spark.read.json(\"examples/src/main/resources/people.json\")\n",
    "\n",
    "# # DataFrames can be saved as Parquet files, maintaining the schema information.\n",
    "# peopleDF.write.parquet(\"people.parquet\")\n",
    "\n",
    "# # Read in the Parquet file created above.\n",
    "# # Parquet files are self-describing so the schema is preserved.\n",
    "# # The result of loading a parquet file is also a DataFrame.\n",
    "# parquetFile = spark.read.parquet(\"people.parquet\")\n",
    "\n",
    "# # Parquet files can also be used to create a temporary view and then used in SQL statements.\n",
    "# parquetFile.createOrReplaceTempView(\"parquetFile\")\n",
    "# teenagers = spark.sql(\"SELECT name FROM parquetFile WHERE age >= 13 AND age <= 19\")\n",
    "# teenagers.show()\n",
    "# # +------+\n",
    "# # |  name|\n",
    "# # +------+\n",
    "# # |Justin|\n",
    "# # +------+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this file already exists, moving along\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try :\n",
    "    df_emp.write.parquet(\"C:/SPARK/examples/src/main/resources/tom_emp2.parquet\")\n",
    "except:\n",
    "    print('this file already exists, moving along')  \n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F.  &nbsp;  Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Parquet is a columnar format, supported by many data processing systems. The advantages of having a columnar storage are as follows −\n",
    "\n",
    "Columnar storage limits IO operations.\n",
    "\n",
    "Columnar storage can fetch specific columns that you need to access.\n",
    "\n",
    "Columnar storage consumes less space.\n",
    "\n",
    "Columnar storage gives better-summarized data and follows type-specific encoding.\n",
    "\n",
    "Spark SQL provides support for both reading and writing parquet files that automatically capture the schema of the original data. Like JSON datasets, parquet files follow the same procedure.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "# $example on:schema_merging$\n",
    "from pyspark.sql import Row\n",
    "# $example off:schema_merging$\n",
    "\n",
    "# $example on:generic_load_save_functions$\n",
    "df_users = spark.read.load(\"C:/SPARK/examples/src/main/resources/users.parquet\")\n",
    "# df.select(\"name\", \"favorite_color\").write.save(\"namesAndFavColors.parquet\")\n",
    "# $example off:generic_load_save_functions$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "peopleDF = spark.read.json(\"examples/src/main/resources/people.json\")\n",
    "\n",
    "# DataFrames can be saved as Parquet files, maintaining the schema information.\n",
    "peopleDF.write.parquet(\"people.parquet\")\n",
    "\n",
    "# Read in the Parquet file created above.\n",
    "# Parquet files are self-describing so the schema is preserved.\n",
    "# The result of loading a parquet file is also a DataFrame.\n",
    "parquetFile = spark.read.parquet(\"people.parquet\")\n",
    "\n",
    "# Parquet files can also be used to create a temporary view and then used in SQL statements.\n",
    "parquetFile.createOrReplaceTempView(\"parquetFile\")\n",
    "teenagers = spark.sql(\"SELECT name FROM parquetFile WHERE age >= 13 AND age <= 19\")\n",
    "teenagers.show()\n",
    "# +------+\n",
    "# |  name|\n",
    "# +------+\n",
    "# |Justin|\n",
    "# +------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from os import walk\n",
    "# from pyspark.sql import SQLContext\n",
    "\n",
    "# sc = SparkContext.getOrCreate()\n",
    "# sqlContext = SQLContext(sc)\n",
    "\n",
    "# parquetdir = r'C:\\PATH\\TO\\YOUR\\PARQUET\\FILES'\n",
    "\n",
    "# # Getting all parquet files in a dir as spark contexts.\n",
    "# # There might be more easy ways to access single parquets, but I had nested dirs\n",
    "# dirpath, dirnames, filenames = next(walk(parquetdir), (None, [], []))\n",
    "\n",
    "# # for each parquet file, i.e. table in our database, spark creates a tempview with\n",
    "# # the respective table name equal the parquet filename\n",
    "# print('New tables available: \\n')\n",
    "\n",
    "# for parquet in filenames:\n",
    "#     print(parquet[:-8])\n",
    "#     spark.read.parquet(parquetdir+'\\\\'+parquet).createOrReplaceTempView(parquet[:-8])\n",
    "    \n",
    "# my_test_query = spark.sql(\"\"\"\n",
    "# select\n",
    "#   field1,\n",
    "#   field2\n",
    "# from parquetfilename1\n",
    "# where\n",
    "#   field1 = 'something'\n",
    "# \"\"\")\n",
    "\n",
    "# my_test_query.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# squaresDF = spark..createDataFrame(sc.parallelize(range(1, 6)).map(lambda i: Row(single=i, double=i ** 2)))\n",
    "\n",
    "# squaresDF.write.parquet(\"data/test_table/key=1\")\n",
    "\n",
    "# # Create another DataFrame in a new partition directory,\n",
    "# # adding a new column and dropping an existing column\n",
    "# cubesDF = sqlSparkContext.createDataFrame(sc.parallelize(range(6, 11)).map(lambda i: Row(single=i, triple=i ** 3)))\n",
    "\n",
    "# cubesDF.write.parquet(\"C:/SPARK/key=2\")\n",
    "\n",
    "# # Read the partitioned table\n",
    "# mergedDF = sqlSparkContext.read.option(\"mergeSchema\", \"true\").parquet(\"C:/SPARKata/test_table\")\n",
    "# mergedDF.printSchema()\n",
    "\n",
    "# # The final schema consists of all 3 columns in the Parquet files together\n",
    "# # with the partitioning column appeared in the partition directory paths.\n",
    "# # root\n",
    "# #  |-- double: long (nullable = true)\n",
    "# #  |-- single: long (nullable = true)\n",
    "# #  |-- triple: long (nullable = true)\n",
    "# #  |-- key: integer (nullable = true)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# .catalog.listTables()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # raw_data_RDD = sc.textFile(\"e://README_spark.md\")  \n",
    "\n",
    "# lines = sc.read.text(\"e://README_spark.md\").rdd.map(lambda r: r[0])\n",
    "\n",
    "# sortedCount = lines.flatMap(lambda x: x.split(' ')).map(lambda x: (int(x), 1)).sortByKey()\n",
    "\n",
    "# # This is just a demo on how to bring all the sorted data back to a single node.\n",
    "\n",
    "# # In reality, we wouldn't want to collect all the data to the driver node.\n",
    "\n",
    "# output = sortedCount.collect()\n",
    "\n",
    "# for (num, unitcount) in output:\n",
    "#         print(num)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# scala \n",
    "\n",
    "# spark: SparkSession = // create the Spark Session\n",
    "# val df = spark.read.csv(\"file.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ./bin/spark-submit examples/src/main/python/sql/basic.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# $example on:init_session$\n",
    "from pyspark.sql import SparkSession\n",
    "# $example off:init_session$\n",
    "\n",
    "# $example on:schema_inferring$\n",
    "from pyspark.sql import Row\n",
    "# $example off:schema_inferring$\n",
    "\n",
    "# $example on:programmatic_schema$\n",
    "# Import data types\n",
    "from pyspark.sql.types import *\n",
    "# $example off:programmatic_schema$\n",
    "\n",
    "# READ IN JSON FILE ! \n",
    "df = spark.read.json(\"C:/SPARK/examples/src/main/resources/people.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agg\n",
      "alias\n",
      "approxQuantile\n",
      "cache\n",
      "checkpoint\n",
      "coalesce\n",
      "colRegex\n",
      "collect\n",
      "columns\n",
      "corr\n",
      "count\n",
      "cov\n",
      "createGlobalTempView\n",
      "createOrReplaceGlobalTempView\n",
      "createOrReplaceTempView\n",
      "createTempView\n",
      "crossJoin\n",
      "crosstab\n",
      "cube\n",
      "describe\n",
      "distinct\n",
      "drop\n",
      "dropDuplicates\n",
      "drop_duplicates\n",
      "dropna\n",
      "dtypes\n",
      "exceptAll\n",
      "explain\n",
      "fillna\n",
      "filter\n",
      "first\n",
      "foreach\n",
      "foreachPartition\n",
      "freqItems\n",
      "groupBy\n",
      "groupby\n",
      "head\n",
      "hint\n",
      "intersect\n",
      "intersectAll\n",
      "isLocal\n",
      "isStreaming\n",
      "is_cached\n",
      "join\n",
      "limit\n",
      "localCheckpoint\n",
      "na\n",
      "orderBy\n",
      "persist\n",
      "printSchema\n",
      "randomSplit\n",
      "rdd\n",
      "registerTempTable\n",
      "repartition\n",
      "repartitionByRange\n",
      "replace\n",
      "rollup\n",
      "sample\n",
      "sampleBy\n",
      "schema\n",
      "select\n",
      "selectExpr\n",
      "show\n",
      "sort\n",
      "sortWithinPartitions\n",
      "sql_ctx\n",
      "stat\n",
      "storageLevel\n",
      "subtract\n",
      "summary\n",
      "take\n",
      "toDF\n",
      "toJSON\n",
      "toLocalIterator\n",
      "toPandas\n",
      "union\n",
      "unionAll\n",
      "unionByName\n",
      "unpersist\n",
      "where\n",
      "withColumn\n",
      "withColumnRenamed\n",
      "withWatermark\n",
      "write\n",
      "writeStream\n"
     ]
    }
   ],
   "source": [
    "for i in dir(df):\n",
    "    if not i.startswith(\"_\"):\n",
    "        print(i)\n",
    "        \n",
    "# agg\n",
    "# alias\n",
    "# approxQuantile\n",
    "# cache\n",
    "# checkpoint\n",
    "# coalesce\n",
    "# colRegex\n",
    "# collect\n",
    "# columns\n",
    "# corr\n",
    "# count\n",
    "# cov\n",
    "# createGlobalTempView\n",
    "# createOrReplaceGlobalTempView\n",
    "# createOrReplaceTempView\n",
    "# createTempView\n",
    "# crossJoin\n",
    "# crosstab\n",
    "# cube\n",
    "# describe\n",
    "# distinct\n",
    "# drop\n",
    "# dropDuplicates\n",
    "# drop_duplicates\n",
    "# dropna\n",
    "# dtypes\n",
    "# exceptAll\n",
    "# explain\n",
    "# fillna\n",
    "# filter\n",
    "# first\n",
    "# foreach\n",
    "# foreachPartition\n",
    "# freqItems\n",
    "# groupBy\n",
    "# groupby\n",
    "# head\n",
    "# hint\n",
    "# intersect\n",
    "# intersectAll\n",
    "# isLocal\n",
    "# isStreaming\n",
    "# is_cached\n",
    "# join\n",
    "# limit\n",
    "# localCheckpoint\n",
    "# na\n",
    "# orderBy\n",
    "# persist\n",
    "# printSchema\n",
    "# randomSplit\n",
    "# rdd\n",
    "# registerTempTable\n",
    "# repartition\n",
    "# repartitionByRange\n",
    "# replace\n",
    "# rollup\n",
    "# sample\n",
    "# sampleBy\n",
    "# schema\n",
    "# select\n",
    "# selectExpr\n",
    "# show\n",
    "# sort\n",
    "# sortWithinPartitions\n",
    "# sql_ctx\n",
    "# stat\n",
    "# storageLevel\n",
    "# subtract\n",
    "# summary\n",
    "# take\n",
    "# toDF\n",
    "# toJSON\n",
    "# toLocalIterator\n",
    "# toPandas\n",
    "# union\n",
    "# unionAll\n",
    "# unionByName\n",
    "# unpersist\n",
    "# where\n",
    "# withColumn\n",
    "# withColumnRenamed\n",
    "# withWatermark\n",
    "# write\n",
    "# writeStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#  people.json\n",
    "\n",
    "# {\"name\":\"Michael\"}\n",
    "# {\"name\":\"Andy\", \"age\":30}\n",
    "# {\"name\":\"Justin\", \"age\":19}\n",
    "\n",
    "# Displays the content of the DataFrame to stdout\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print the schema in a tree format\n",
    "df.printSchema()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Select only the \"name\" column\n",
    "df.select(\"name\").show()  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|   name|(age + 1)|\n",
      "+-------+---------+\n",
      "|Michael|     null|\n",
      "|   Andy|       31|\n",
      "| Justin|       20|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Select everybody, but increment the age by 1\n",
    "df.select(df['name'], df['age'] + 1).show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Select people older than 21\n",
    "\n",
    "df.filter(df['age'] > 21).show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| age|count|\n",
      "+----+-----+\n",
      "|  19|    1|\n",
      "|null|    1|\n",
      "|  30|    1|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Count people by age\n",
    "df.groupBy(\"age\").count().show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# $example on:run_sql$\n",
    "# Register the DataFrame as a SQL temporary view\n",
    "\n",
    "# Register the DataFrame as a SQL temporary view\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "sqlDF = spark.sql(\"SELECT * FROM people\")\n",
    "sqlDF.show()  \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# $example on:global_temp_view$\n",
    "# Register the DataFrame as a global temporary view\n",
    "df.createGlobalTempView(\"people\")\n",
    "\n",
    "# Global temporary view is tied to a system preserved database `global_temp`\n",
    "spark.sql(\"SELECT * FROM global_temp.people\").show()    \n",
    "\n",
    "# error =  AnalysisException: \"Temporary view 'people' already exists;\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## datasource.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> PARQUET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "# $example on:schema_merging$\n",
    "from pyspark.sql import Row\n",
    "# $example off:schema_merging$\n",
    "\n",
    "# $example on:generic_load_save_functions$\n",
    "df = spark.read.load(\"C:/SPARK/examples/src/main/resources/users.parquet\")\n",
    "# df.select(\"name\", \"favorite_color\").write.save(\"namesAndFavColors.parquet\")\n",
    "# $example off:generic_load_save_functions$\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, favorite_color: string, favorite_numbers: array<int>]"
      ]
     },
     "execution_count": 747,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 748,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {},
   "outputs": [],
   "source": [
    "# < i n s r t   -   image as you do it >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------------+\n",
      "|  name|favorite_color|favorite_numbers|\n",
      "+------+--------------+----------------+\n",
      "|Alyssa|          null|  [3, 9, 15, 20]|\n",
      "|   Ben|           red|              []|\n",
      "+------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parquet is a columnar format, supported by many data processing systems. The advantages of having a columnar storage are as follows −\n",
    "\n",
    "Columnar storage limits IO operations.\n",
    "\n",
    "Columnar storage can fetch specific columns that you need to access.\n",
    "\n",
    "Columnar storage consumes less space.\n",
    "\n",
    "Columnar storage gives better-summarized data and follows type-specific encoding.\n",
    "\n",
    "Spark SQL provides support for both reading and writing parquet files that automatically capture the schema of the original data. Like JSON datasets, parquet files follow the same procedure.\n",
    "\n",
    "Let’s take another look at the same example of employee record data named employee.parquet placed in the same directory where spark-shell is running.\n",
    "\n",
    "Given data − Do not bother about converting the input data of employee records into parquet format. We use the following commands that convert the RDD data into Parquet file. Place the employee.json document, which we have used as the input file in our previous examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# $example on:generic_load_save_functions$\n",
    "df2 = spark.read.load(\"E:/userdata1.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+----------+---------+--------------------+------+---------------+-------------------+--------------------+----------+---------+--------------------+--------------------+\n",
      "|  registration_dttm| id|first_name|last_name|               email|gender|     ip_address|                 cc|             country| birthdate|   salary|               title|            comments|\n",
      "+-------------------+---+----------+---------+--------------------+------+---------------+-------------------+--------------------+----------+---------+--------------------+--------------------+\n",
      "|2016-02-03 01:55:29|  1|    Amanda|   Jordan|    ajordan0@com.com|Female|    1.197.201.2|   6759521864920116|           Indonesia|  3/8/1971| 49756.53|    Internal Auditor|               1E+02|\n",
      "|2016-02-03 11:04:03|  2|    Albert|  Freeman|     afreeman1@is.gd|  Male| 218.111.175.34|                   |              Canada| 1/16/1968|150280.17|       Accountant IV|                    |\n",
      "|2016-02-02 19:09:31|  3|    Evelyn|   Morgan|emorgan2@altervis...|Female|   7.161.136.94|   6767119071901597|              Russia|  2/1/1960|144972.51| Structural Engineer|                    |\n",
      "|2016-02-02 18:36:21|  4|    Denise|    Riley|    driley3@gmpg.org|Female|  140.35.109.83|   3576031598965625|               China|  4/8/1997| 90263.05|Senior Cost Accou...|                    |\n",
      "|2016-02-02 23:05:31|  5|    Carlos|    Burns|cburns4@miitbeian...|      | 169.113.235.40|   5602256255204850|        South Africa|          |     null|                    |                    |\n",
      "|2016-02-03 01:22:34|  6|   Kathryn|    White|  kwhite5@google.com|Female| 195.131.81.179|   3583136326049310|           Indonesia| 2/25/1983| 69227.11|   Account Executive|                    |\n",
      "|2016-02-03 02:33:08|  7|    Samuel|   Holmes|sholmes6@foxnews.com|  Male| 232.234.81.197|   3582641366974690|            Portugal|12/18/1987| 14247.62|Senior Financial ...|                    |\n",
      "|2016-02-03 00:47:06|  8|     Harry|   Howell| hhowell7@eepurl.com|  Male|   91.235.51.73|                   |Bosnia and Herzeg...|  3/1/1962|186469.43|    Web Developer IV|                    |\n",
      "|2016-02-02 21:52:53|  9|      Jose|   Foster|   jfoster8@yelp.com|  Male|   132.31.53.61|                   |         South Korea| 3/27/1992|231067.84|Software Test Eng...|               1E+02|\n",
      "|2016-02-03 12:29:47| 10|     Emily|  Stewart|estewart9@opensou...|Female| 143.28.251.245|   3574254110301671|             Nigeria| 1/28/1997| 27234.28|     Health Coach IV|                    |\n",
      "|2016-02-02 18:10:42| 11|     Susan|  Perkins| sperkinsa@patch.com|Female|    180.85.0.62|   3573823609854134|              Russia|          |210001.95|                    |                    |\n",
      "|2016-02-03 12:04:34| 12|     Alice|    Berry|aberryb@wikipedia...|Female| 246.225.12.189|   4917830851454417|               China| 8/12/1968| 22944.53|    Quality Engineer|                    |\n",
      "|2016-02-03 12:48:17| 13|    Justin|    Berry|jberryc@usatoday.com|  Male|   157.7.146.43|6331109912871813274|              Zambia| 8/15/1975| 44165.46|Structural Analys...|                    |\n",
      "|2016-02-03 15:46:52| 14|     Kathy| Reynolds|kreynoldsd@redcro...|Female|  81.254.172.13|   5537178462965976|Bosnia and Herzeg...| 6/27/1970|286592.99|           Librarian|                    |\n",
      "|2016-02-03 02:53:23| 15|   Dorothy|   Hudson|dhudsone@blogger.com|Female|       8.59.7.0|   3542586858224170|               Japan|12/20/1989|157099.71|  Nurse Practicioner|<script>alert('hi...|\n",
      "|2016-02-02 18:44:01| 16|     Bruce|   Willis|bwillisf@bluehost...|  Male|239.182.219.189|   3573030625927601|              Brazil|          |239100.65|                    |                    |\n",
      "|2016-02-02 18:57:45| 17|     Emily|  Andrews|eandrewsg@cornell...|Female| 29.231.180.172|     30271790537626|              Russia| 4/13/1990|116800.65|        Food Chemist|                    |\n",
      "|2016-02-03 10:44:24| 18|   Stephen|  Wallace|swallaceh@netvibe...|  Male|  152.49.213.62|   5433943468526428|             Ukraine| 1/15/1978|248877.99|Account Represent...|                    |\n",
      "|2016-02-03 05:45:54| 19|  Clarence|   Lawson|clawsoni@vkontakt...|  Male| 107.175.15.152|   3544052814080964|              Russia|          |177122.99|                    |                    |\n",
      "|2016-02-03 04:30:36| 20|   Rebecca|     Bell| rbellj@bandcamp.com|Female|172.215.104.127|                   |               China|          |137251.19|                    |                    |\n",
      "+-------------------+---+----------+---------+--------------------+------+---------------+-------------------+--------------------+----------+---------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "userdata[1-5].parquet: These are sample files containing data in PARQUET format.\n",
    "\n",
    "-> Number of rows in each file: 1000\n",
    "-> Column details:\n",
    "column#\t\tcolumn_name\t\thive_datatype\n",
    "=====================================================\n",
    "1\t\tregistration_dttm \ttimestamp\n",
    "2\t\tid \t\t\tint\n",
    "3\t\tfirst_name \t\tstring\n",
    "4\t\tlast_name \t\tstring\n",
    "5\t\temail \t\t\tstring\n",
    "6\t\tgender \t\t\tstring\n",
    "7\t\tip_address \t\tstring\n",
    "8\t\tcc \t\t\tstring\n",
    "9\t\tcountry \t\tstring\n",
    "10\t\tbirthdate \t\tstring\n",
    "11\t\tsalary \t\t\tdouble\n",
    "12\t\ttitle \t\t\tstring\n",
    "13\t\tcomments \t\tstring\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "| 10|\n",
      "| 11|\n",
      "| 12|\n",
      "| 13|\n",
      "| 14|\n",
      "| 15|\n",
      "| 16|\n",
      "| 17|\n",
      "| 18|\n",
      "| 19|\n",
      "| 20|\n",
      "+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# sqlDF = sqlContext.sql(\"SELECT * FROM people\")\n",
    "\n",
    "all_info_in_id_column = df2.select(\"id\").show()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|first_name|\n",
      "+----------+\n",
      "|    Amanda|\n",
      "|    Albert|\n",
      "|    Evelyn|\n",
      "|    Denise|\n",
      "|    Carlos|\n",
      "|   Kathryn|\n",
      "|    Samuel|\n",
      "|     Harry|\n",
      "|      Jose|\n",
      "|     Emily|\n",
      "|     Susan|\n",
      "|     Alice|\n",
      "|    Justin|\n",
      "|     Kathy|\n",
      "|   Dorothy|\n",
      "|     Bruce|\n",
      "|     Emily|\n",
      "|   Stephen|\n",
      "|  Clarence|\n",
      "|   Rebecca|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "first_name = df2.select(\"first_name\").show()   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Reading VCF files in SparkSQL / Python*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> i don't know how to say this.  This will look like giberish until you dive into what the terms mean etc, and even then it takes a while..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Deep Dive into processing .vcf files "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The output we are dealing with is effectively a dict, so even if you dont understand the gene terminology, its just about understanding we are querying the file for certain 'keys'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# READ:  for the love of all holy don't save this to your laptop and associated it with a vCalendar File for outlook or something stupid like that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-allel is a Python package intended to enable exploratory analysis of large-scale genetic variation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variant Call Format (VCF) is a text file format for storing marker and genotype data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  not like you necessarily care, but this is about AGTCs, i.e. bases Adenine, Guanine, Thymine, and Cytosine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://www.genome.gov/sites/default/files/tg/en/illustration/acgt.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import scikit-allel, a sci file \n",
    "import allel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(allel.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {},
   "outputs": [],
   "source": [
    "### sample - vcf file below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "THIS IS A RANDOM SAMPLE.VCF EXAMPLE: \n",
    "    \n",
    " -  the file begins with meta-info lines with ## \n",
    "    \n",
    "\n",
    "##fileformat=VCFv4.0\n",
    "##fileDate=20090805\n",
    "##source=myImputationProgramV3.1\n",
    "##reference=1000GenomesPilot-NCBI36\n",
    "##phasing=partial\n",
    "##INFO=<ID=NS,Number=1,Type=Integer,Description=\"Number of Samples With Data\">\n",
    "##INFO=<ID=AN,Number=1,Type=Integer,Description=\"Total number of alleles in called genotypes\">\n",
    "##INFO=<ID=AC,Number=.,Type=Integer,Description=\"Allele count in genotypes, for each ALT allele, in the same order as listed\">\n",
    "##INFO=<ID=DP,Number=1,Type=Integer,Description=\"Total Depth\">\n",
    "##INFO=<ID=AF,Number=.,Type=Float,Description=\"Allele Frequency\">\n",
    "##INFO=<ID=AA,Number=1,Type=String,Description=\"Ancestral Allele\">\n",
    "##INFO=<ID=DB,Number=0,Type=Flag,Description=\"dbSNP membership, build 129\">\n",
    "##INFO=<ID=H2,Number=0,Type=Flag,Description=\"HapMap2 membership\">\n",
    "##FILTER=<ID=q10,Description=\"Quality below 10\">\n",
    "##FILTER=<ID=s50,Description=\"Less than 50% of samples have data\">\n",
    "##FORMAT=<ID=GT,Number=1,Type=String,Description=\"Genotype\">\n",
    "##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\"Genotype Quality\">\n",
    "##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\"Read Depth\">\n",
    "##FORMAT=<ID=HQ,Number=2,Type=Integer,Description=\"Haplotype Quality\">\n",
    "##ALT=<ID=DEL:ME:ALU,Description=\"Deletion of ALU element\">\n",
    "##ALT=<ID=CNV,Description=\"Copy number variable region\">       < - - im inserting a space for clarity \n",
    "\n",
    "\n",
    "#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO\tFORMAT\tNA00001\tNA00002\tNA00003\n",
    "\n",
    "19\t111\t.\tA\tC\t9.6\t.\t.\tGT:HQ\t0|0:10,10\t0|0:10,10\t0/1:3,3\n",
    "19\t112\t.\tA\tG\t10\t.\t.\tGT:HQ\t0|0:10,10\t0|0:10,10\t0/1:3,3\n",
    "20\t14370\trs6054257\tG\tA\t29\tPASS\tNS=3;DP=14;AF=0.5;DB;H2\tGT:GQ:DP:HQ\t0|0:48:1:51,51\t1|0:48:8:51,51\t1/1:43:5:.,.\n",
    "20\t17330\t.\tT\tA\t3\tq10\tNS=3;DP=11;AF=0.017\tGT:GQ:DP:HQ\t0|0:49:3:58,50\t0|1:3:5:65,3\t0/0:41:3:.,.\n",
    "20\t1110696\trs6040355\tA\tG,T\t67\tPASS\tNS=2;DP=10;AF=0.333,0.667;AA=T;DB\tGT:GQ:DP:HQ\t1|2:21:6:23,27\t2|1:2:0:18,2\t2/2:35:4:.,.\n",
    "20\t1230237\t.\tT\t.\t47\tPASS\tNS=3;DP=13;AA=T\tGT:GQ:DP:HQ\t0|0:54:.:56,60\t0|0:48:4:51,51\t0/0:61:2:.,.\n",
    "20\t1234567\tmicrosat1\tG\tGA,GAC\t50\tPASS\tNS=3;DP=9;AA=G;AN=6;AC=3,1\tGT:GQ:DP\t0/1:.:4\t0/2:17:2\t1/1:40:3\n",
    "20\t1235237\t.\tT\t.\t.\t.\t.\tGT\t0/0\t0|0\t./.\n",
    "X\t10\trsTest\tAC\tA,ATG\t10\tPASS\t.\tGT\t0\t0/1\t0|2\n",
    "\n",
    "\n",
    "\n",
    "FYI, this is the header:\n",
    "#CHROM    POS    ID    REF    ALT    QUAL    FILTER    INFO    FORMAT    NA00001    NA00002    NA00003\n",
    "\n",
    "\n",
    "So what is this ? \n",
    "After the header, there are DATA LINES, with each data line describing a genetic variant at a particular \n",
    "position relative to the reference genome of whichever species you are studying. \n",
    "\n",
    "In my case:\n",
    "CHROM\t\n",
    "POS\t\n",
    "ID\t\n",
    "REF\t\n",
    "ALT\t\n",
    "QUAL\n",
    "FILTER\n",
    "INFO\n",
    "FORMAT\n",
    "NA00001\n",
    "NA00002\n",
    "NA00003\n",
    "\n",
    "Data lines contain marker and genotype data (one variant per line). A data line is called a VCF record.\n",
    "\n",
    "My first line describes a variant on chromosome **19** at position **11** relative to the to ____\n",
    "assembly of the human genome. The reference allele is ‘C’ and the alternate allele is ‘A’, so this etc etc \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "CHROM\t\n",
    "the chromosome.\n",
    "\n",
    "\n",
    "POS\t\n",
    "the genome coordinate of the first base in the variant. Within a chromosome, VCF records are sorted in order of increasing position.\n",
    "\n",
    "ID\t\n",
    "a semicolon-separated list of marker identifiers.\n",
    "\n",
    "REF\t\n",
    "the reference allele expressed as a sequence of one or more A/C/G/T nucleotides (e.g. \"A\" or \"AAC\")\n",
    "\n",
    "ALT\t\n",
    "the alternate allele expressed as a sequence of one or more A/C/G/T nucleotides (e.g. \"A\" or \"AAC\"). If there is more than one alternate alleles, the field should be a comma-separated list of alternate alleles.\n",
    "\n",
    "QUAL\t\n",
    "probability that the ALT allele is incorrectly specified, expressed on the the phred scale (-10log10(probability)).\n",
    "\n",
    "FILTER\t\n",
    "Either \"PASS\" or a semicolon-separated list of failed quality control filters.\n",
    "\n",
    "INFO\t\n",
    "additional information (no white space, tabs, or semi-colons permitted).\n",
    "\n",
    "FORMAT\t\n",
    "colon-separated list of data subfields reported for each sample. The format fields in the Example are explained below.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##fileformat=VCFv4.0\n",
      "##fileDate=20090805\n",
      "##source=myImputationProgramV3.1\n",
      "##reference=1000GenomesPilot-NCBI36\n",
      "##phasing=partial\n",
      "##INFO=<ID=NS,Number=1,Type=Integer,Description=\"Number of Samples With Data\">\n",
      "##INFO=<ID=AN,Number=1,Type=Integer,Description=\"Total number of alleles in called genotypes\">\n",
      "##INFO=<ID=AC,Number=.,Type=Integer,Description=\"Allele count in genotypes, for each ALT allele, in the same order as listed\">\n",
      "##INFO=<ID=DP,Number=1,Type=Integer,Description=\"Total Depth\">\n",
      "##INFO=<ID=AF,Number=.,Type=Float,Description=\"Allele Frequency\">\n",
      "##INFO=<ID=AA,Number=1,Type=String,Description=\"Ancestral Allele\">\n",
      "##INFO=<ID=DB,Number=0,Type=Flag,Description=\"dbSNP membership, build 129\">\n",
      "##INFO=<ID=H2,Number=0,Type=Flag,Description=\"HapMap2 membership\">\n",
      "##FILTER=<ID=q10,Description=\"Quality below 10\">\n",
      "##FILTER=<ID=s50,Description=\"Less than 50% of samples have data\">\n",
      "##FORMAT=<ID=GT,Number=1,Type=String,Description=\"Genotype\">\n",
      "##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\"Genotype Quality\">\n",
      "##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\"Read Depth\">\n",
      "##FORMAT=<ID=HQ,Number=2,Type=Integer,Description=\"Haplotype Quality\">\n",
      "##ALT=<ID=DEL:ME:ALU,Description=\"Deletion of ALU element\">\n",
      "##ALT=<ID=CNV,Description=\"Copy number variable region\">\n",
      "#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO\tFORMAT\tNA00001\tNA00002\tNA00003\n",
      "19\t111\t.\tA\tC\t9.6\t.\t.\tGT:HQ\t0|0:10,10\t0|0:10,10\t0/1:3,3\n",
      "19\t112\t.\tA\tG\t10\t.\t.\tGT:HQ\t0|0:10,10\t0|0:10,10\t0/1:3,3\n",
      "20\t14370\trs6054257\tG\tA\t29\tPASS\tNS=3;DP=14;AF=0.5;DB;H2\tGT:GQ:DP:HQ\t0|0:48:1:51,51\t1|0:48:8:51,51\t1/1:43:5:.,.\n",
      "20\t17330\t.\tT\tA\t3\tq10\tNS=3;DP=11;AF=0.017\tGT:GQ:DP:HQ\t0|0:49:3:58,50\t0|1:3:5:65,3\t0/0:41:3:.,.\n",
      "20\t1110696\trs6040355\tA\tG,T\t67\tPASS\tNS=2;DP=10;AF=0.333,0.667;AA=T;DB\tGT:GQ:DP:HQ\t1|2:21:6:23,27\t2|1:2:0:18,2\t2/2:35:4:.,.\n",
      "20\t1230237\t.\tT\t.\t47\tPASS\tNS=3;DP=13;AA=T\tGT:GQ:DP:HQ\t0|0:54:.:56,60\t0|0:48:4:51,51\t0/0:61:2:.,.\n",
      "20\t1234567\tmicrosat1\tG\tGA,GAC\t50\tPASS\tNS=3;DP=9;AA=G;AN=6;AC=3,1\tGT:GQ:DP\t0/1:.:4\t0/2:17:2\t1/1:40:3\n",
      "20\t1235237\t.\tT\t.\t.\t.\t.\tGT\t0/0\t0|0\t./.\n",
      "X\t10\trsTest\tAC\tA,ATG\t10\tPASS\t.\tGT\t0\t0/1\t0|2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# very handy way of printing out the actual .vcf file as a way of getting an idea how this stuff works\n",
    "\n",
    "with open('C:/SPARK/sample.vcf', mode='r') as vcf:\n",
    "    print(vcf.read())\n",
    "    \n",
    "# prints out the vcf sample file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  *if you are really good you know this is actually the .vcf from the spec that talks about the file formatted columns, so download the spec and then youc an use this verbatim !* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANNTransformer\n",
      "ANN_AA_LENGTH_FIELD\n",
      "ANN_AA_POS_FIELD\n",
      "ANN_ANNOTATION_FIELD\n",
      "ANN_ANNOTATION_IMPACT_FIELD\n",
      "ANN_CDNA_LENGTH_FIELD\n",
      "ANN_CDNA_POS_FIELD\n",
      "ANN_CDS_LENGTH_FIELD\n",
      "ANN_CDS_POS_FIELD\n",
      "ANN_DISTANCE_FIELD\n",
      "ANN_FEATURE_ID_FIELD\n",
      "ANN_FEATURE_TYPE_FIELD\n",
      "ANN_FIELD\n",
      "ANN_FIELDS\n",
      "ANN_GENE_ID_FIELD\n",
      "ANN_GENE_NAME_FIELD\n",
      "ANN_HGVS_C_FIELD\n",
      "ANN_HGVS_P_FIELD\n",
      "ANN_RANK_FIELD\n",
      "ANN_TRANSCRIPT_BIOTYPE_FIELD\n",
      "AlleleCountsArray\n",
      "AlleleCountsChunkedArray\n",
      "AlleleCountsChunkedTable\n",
      "AlleleCountsDaskArray\n",
      "CenterScaler\n",
      "DEFAULT_ALT_NUMBER\n",
      "DEFAULT_BUFFER_SIZE\n",
      "DEFAULT_CHUNK_LENGTH\n",
      "DEFAULT_CHUNK_WIDTH\n",
      "FIXED_VARIANTS_FIELDS\n",
      "FeatureTable\n",
      "FileInputStream\n",
      "FileNotFoundError\n",
      "GenotypeAlleleCounts\n",
      "GenotypeAlleleCountsArray\n",
      "GenotypeAlleleCountsChunkedArray\n",
      "GenotypeAlleleCountsDaskArray\n",
      "GenotypeAlleleCountsDaskVector\n",
      "GenotypeAlleleCountsVector\n",
      "GenotypeArray\n",
      "GenotypeChunkedArray\n",
      "GenotypeDaskArray\n",
      "GenotypeDaskVector\n",
      "GenotypeVector\n",
      "Genotypes\n",
      "HaplotypeArray\n",
      "HaplotypeChunkedArray\n",
      "HaplotypeDaskArray\n",
      "INHERIT_MISSING\n",
      "INHERIT_NONPARENTAL\n",
      "INHERIT_NONSEG_ALT\n",
      "INHERIT_NONSEG_REF\n",
      "INHERIT_PARENT1\n",
      "INHERIT_PARENT2\n",
      "INHERIT_PARENT_MISSING\n",
      "INHERIT_UNDETERMINED\n",
      "PY2\n",
      "PattersonScaler\n",
      "SortedIndex\n",
      "SortedMultiIndex\n",
      "StandardScaler\n",
      "UniqueIndex\n",
      "VCFChunkIterator\n",
      "VCFHeaders\n",
      "VCF_FIXED_FIELDS\n",
      "VariantChunkedTable\n",
      "VariantTable\n",
      "abc\n",
      "absolute_import\n",
      "admixture\n",
      "allel\n",
      "array_to_hdf5\n",
      "asarray_ndim\n",
      "average_hudson_fst\n",
      "average_patterson_d\n",
      "average_patterson_f3\n",
      "average_patterson_fst\n",
      "average_weir_cockerham_fst\n",
      "blockwise_hudson_fst\n",
      "blockwise_patterson_d\n",
      "blockwise_patterson_f3\n",
      "blockwise_patterson_fst\n",
      "blockwise_weir_cockerham_fst\n",
      "chunked\n",
      "compat\n",
      "condensed_coords\n",
      "condensed_coords_between\n",
      "condensed_coords_within\n",
      "constants\n",
      "create_allele_mapping\n",
      "csv\n",
      "dask\n",
      "date\n",
      "debug\n",
      "decomposition\n",
      "default_float_dtype\n",
      "default_integer_dtype\n",
      "default_numbers\n",
      "default_string_dtype\n",
      "default_types\n",
      "distance\n",
      "diversity\n",
      "division\n",
      "ehh_decay\n",
      "equally_accessible_windows\n",
      "fasta\n",
      "fig_voight_painting\n",
      "fold_joint_sfs\n",
      "fold_sfs\n",
      "fst\n",
      "garud_h\n",
      "get_scaler\n",
      "gff\n",
      "gff3_parse_attributes\n",
      "gff3_to_dataframe\n",
      "gff3_to_recarray\n",
      "gzip\n",
      "haplotype_diversity\n",
      "heterozygosity_expected\n",
      "heterozygosity_observed\n",
      "hudson_fst\n",
      "hw\n",
      "ihs\n",
      "inbreeding_coefficient\n",
      "io\n",
      "itemgetter\n",
      "iter_gff3\n",
      "iter_vcf_chunks\n",
      "itertools\n",
      "joint_sfs\n",
      "joint_sfs_folded\n",
      "joint_sfs_folded_scaled\n",
      "joint_sfs_scaled\n",
      "ld\n",
      "locate_fixed_differences\n",
      "locate_private_alleles\n",
      "locate_unlinked\n",
      "logger\n",
      "logging\n",
      "mean_pairwise_difference\n",
      "mean_pairwise_difference_between\n",
      "mendel\n",
      "mendel_errors\n",
      "misc\n",
      "model\n",
      "moving_delta_tajima_d\n",
      "moving_garud_h\n",
      "moving_haplotype_diversity\n",
      "moving_hudson_fst\n",
      "moving_mean\n",
      "moving_midpoint\n",
      "moving_patterson_d\n",
      "moving_patterson_f3\n",
      "moving_patterson_fst\n",
      "moving_statistic\n",
      "moving_std\n",
      "moving_tajima_d\n",
      "moving_weir_cockerham_fst\n",
      "namedtuple\n",
      "normalize_callset\n",
      "np\n",
      "nsl\n",
      "opt\n",
      "os\n",
      "paint_transmission\n",
      "pairwise_distance\n",
      "pairwise_dxy\n",
      "patterson_d\n",
      "patterson_f2\n",
      "patterson_f3\n",
      "patterson_fst\n",
      "pca\n",
      "pcoa\n",
      "per_base\n",
      "phase_by_transmission\n",
      "phase_parents_by_transmission\n",
      "phase_progeny_by_transmission\n",
      "plot\n",
      "plot_haplotype_frequencies\n",
      "plot_joint_sfs\n",
      "plot_joint_sfs_folded\n",
      "plot_joint_sfs_folded_scaled\n",
      "plot_joint_sfs_scaled\n",
      "plot_moving_haplotype_frequencies\n",
      "plot_pairwise_distance\n",
      "plot_pairwise_ld\n",
      "plot_sfs\n",
      "plot_sfs_folded\n",
      "plot_sfs_folded_scaled\n",
      "plot_sfs_scaled\n",
      "plot_variant_locator\n",
      "plot_voight_painting\n",
      "preprocessing\n",
      "print_function\n",
      "randomized_pca\n",
      "re\n",
      "read_vcf\n",
      "recarray_from_hdf5_group\n",
      "recarray_to_hdf5_group\n",
      "rogers_huff_r\n",
      "rogers_huff_r_between\n",
      "roh\n",
      "roh_mhmm\n",
      "sample_to_haplotype_selection\n",
      "scale_joint_sfs\n",
      "scale_joint_sfs_folded\n",
      "scale_sfs\n",
      "scale_sfs_folded\n",
      "selection\n",
      "sequence_divergence\n",
      "sequence_diversity\n",
      "sf\n",
      "sfs\n",
      "sfs_folded\n",
      "sfs_folded_scaled\n",
      "sfs_scaled\n",
      "standardize\n",
      "standardize_by_allele_count\n",
      "stats\n",
      "subprocess\n",
      "tabulate_state_blocks\n",
      "tabulate_state_transitions\n",
      "tajima_d\n",
      "text_type\n",
      "time\n",
      "unquote_plus\n",
      "util\n",
      "vcf_read\n",
      "vcf_to_csv\n",
      "vcf_to_dataframe\n",
      "vcf_to_hdf5\n",
      "vcf_to_npz\n",
      "vcf_to_recarray\n",
      "vcf_to_zarr\n",
      "vcf_write\n",
      "voight_painting\n",
      "warnings\n",
      "watterson_theta\n",
      "weir_cockerham_fst\n",
      "window\n",
      "windowed_count\n",
      "windowed_df\n",
      "windowed_divergence\n",
      "windowed_diversity\n",
      "windowed_hudson_fst\n",
      "windowed_patterson_fst\n",
      "windowed_r_squared\n",
      "windowed_statistic\n",
      "windowed_tajima_d\n",
      "windowed_watterson_theta\n",
      "windowed_weir_cockerham_fst\n",
      "write_fasta\n",
      "write_vcf\n",
      "write_vcf_data\n",
      "write_vcf_header\n",
      "xpehh\n",
      "xpnsl\n",
      "zip_longest\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# to understand how powerful this library is, look at its .methods available for you:\n",
    "\n",
    "for i in dir(allel): \n",
    "    if not i.startswith(\"_\"):\n",
    "        print(i)\n",
    "        \n",
    "# ever heard of the massive library scikit-learn ?  \n",
    "# think genomic version of that ! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We will focus on extracting data from Variant Call Format (VCF) files and loading into NumPy arrays, \n",
    "pandas data frames, HDF5 files or Zarr arrays for ease of analysis. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key is to focus on extracting the necessary data from the VCF file and loading it into a more efficient storage container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "callset = allel.read_vcf('C:/SPARK/sample.vcf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(type(callset))   # key/value dict ! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['calldata/GT',\n",
       " 'samples',\n",
       " 'variants/ALT',\n",
       " 'variants/CHROM',\n",
       " 'variants/FILTER_PASS',\n",
       " 'variants/ID',\n",
       " 'variants/POS',\n",
       " 'variants/QUAL',\n",
       " 'variants/REF']"
      ]
     },
     "execution_count": 763,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Here are my available keys:\n",
    "sorted(callset.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All arrays with keys beginning ‘variants/’ come from the fixed fields in the VCF file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 765,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'read_vcf' in dir(allel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 766,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'read_vcf_headers' in dir(allel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'variants/POS': array([    111,     112,   14370,   17330, 1110696, 1230237, 1234567,\n",
      "       1235237,      10]), 'variants/REF': array(['A', 'A', 'G', 'T', 'A', 'T', 'G', 'T', 'AC'], dtype=object), 'variants/ID': array(['.', '.', 'rs6054257', '.', 'rs6040355', '.', 'microsat1', '.',\n",
      "       'rsTest'], dtype=object), 'variants/ALT': array([['C', '', ''],\n",
      "       ['G', '', ''],\n",
      "       ['A', '', ''],\n",
      "       ['A', '', ''],\n",
      "       ['G', 'T', ''],\n",
      "       ['', '', ''],\n",
      "       ['GA', 'GAC', ''],\n",
      "       ['', '', ''],\n",
      "       ['A', 'ATG', '']], dtype=object), 'samples': array(['NA00001', 'NA00002', 'NA00003'], dtype=object), 'calldata/GT': array([[[ 0,  0],\n",
      "        [ 0,  0],\n",
      "        [ 0,  1]],\n",
      "\n",
      "       [[ 0,  0],\n",
      "        [ 0,  0],\n",
      "        [ 0,  1]],\n",
      "\n",
      "       [[ 0,  0],\n",
      "        [ 1,  0],\n",
      "        [ 1,  1]],\n",
      "\n",
      "       [[ 0,  0],\n",
      "        [ 0,  1],\n",
      "        [ 0,  0]],\n",
      "\n",
      "       [[ 1,  2],\n",
      "        [ 2,  1],\n",
      "        [ 2,  2]],\n",
      "\n",
      "       [[ 0,  0],\n",
      "        [ 0,  0],\n",
      "        [ 0,  0]],\n",
      "\n",
      "       [[ 0,  1],\n",
      "        [ 0,  2],\n",
      "        [ 1,  1]],\n",
      "\n",
      "       [[ 0,  0],\n",
      "        [ 0,  0],\n",
      "        [-1, -1]],\n",
      "\n",
      "       [[ 0, -1],\n",
      "        [ 0,  1],\n",
      "        [ 0,  2]]], dtype=int8), 'variants/FILTER_PASS': array([False, False,  True, False,  True,  True,  True, False,  True]), 'variants/QUAL': array([ 9.6, 10. , 29. ,  3. , 67. , 47. , 50. ,  nan, 10. ],\n",
      "      dtype=float32), 'variants/CHROM': array(['19', '19', '20', '20', '20', '20', '20', '20', 'X'], dtype=object)}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(callset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clear\n",
      "copy\n",
      "fromkeys\n",
      "get\n",
      "items\n",
      "keys\n",
      "pop\n",
      "popitem\n",
      "setdefault\n",
      "update\n",
      "values\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Commands (methods) you can use:\n",
    "\n",
    "for i in dir(callset):\n",
    "    if not i.startswith(\"_\"):\n",
    "        print(i)\n",
    "\n",
    "# methods you can call, as a reference...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [    111     112   14370   17330 1110696 1230237 1234567 1235237      10]\n",
      "\n",
      " ['A' 'A' 'G' 'T' 'A' 'T' 'G' 'T' 'AC']\n",
      "\n",
      " ['.' '.' 'rs6054257' '.' 'rs6040355' '.' 'microsat1' '.' 'rsTest']\n",
      "\n",
      " [['C' '' '']\n",
      " ['G' '' '']\n",
      " ['A' '' '']\n",
      " ['A' '' '']\n",
      " ['G' 'T' '']\n",
      " ['' '' '']\n",
      " ['GA' 'GAC' '']\n",
      " ['' '' '']\n",
      " ['A' 'ATG' '']]\n",
      "\n",
      " ['NA00001' 'NA00002' 'NA00003']\n",
      "\n",
      " [[[ 0  0]\n",
      "  [ 0  0]\n",
      "  [ 0  1]]\n",
      "\n",
      " [[ 0  0]\n",
      "  [ 0  0]\n",
      "  [ 0  1]]\n",
      "\n",
      " [[ 0  0]\n",
      "  [ 1  0]\n",
      "  [ 1  1]]\n",
      "\n",
      " [[ 0  0]\n",
      "  [ 0  1]\n",
      "  [ 0  0]]\n",
      "\n",
      " [[ 1  2]\n",
      "  [ 2  1]\n",
      "  [ 2  2]]\n",
      "\n",
      " [[ 0  0]\n",
      "  [ 0  0]\n",
      "  [ 0  0]]\n",
      "\n",
      " [[ 0  1]\n",
      "  [ 0  2]\n",
      "  [ 1  1]]\n",
      "\n",
      " [[ 0  0]\n",
      "  [ 0  0]\n",
      "  [-1 -1]]\n",
      "\n",
      " [[ 0 -1]\n",
      "  [ 0  1]\n",
      "  [ 0  2]]]\n",
      "\n",
      " [False False  True False  True  True  True False  True]\n",
      "\n",
      " [ 9.6 10.  29.   3.  67.  47.  50.   nan 10. ]\n",
      "\n",
      " ['19' '19' '20' '20' '20' '20' '20' '20' 'X']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in callset.values():  print(\"\\n\",i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The callset object returned by read_vcf() is a Python dictionary (dict). It contains several NumPy arrays, each of which can be accessed via a key. Here are the available keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['calldata/GT',\n",
       " 'samples',\n",
       " 'variants/ALT',\n",
       " 'variants/CHROM',\n",
       " 'variants/FILTER_PASS',\n",
       " 'variants/ID',\n",
       " 'variants/POS',\n",
       " 'variants/QUAL',\n",
       " 'variants/REF']"
      ]
     },
     "execution_count": 770,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# The callset object returned by read_vcf() is a Python dictionary (dict). \n",
    "# It contains several NumPy arrays, each of which can be accessed via a key. \n",
    "# Here are the available keys:\n",
    "\n",
    "# keys:\n",
    "sorted(callset.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- calldata/GT\n",
      "- samples\n",
      "- variants/ALT\n",
      "- variants/CHROM\n",
      "- variants/FILTER_PASS\n",
      "- variants/ID\n",
      "- variants/POS\n",
      "- variants/QUAL\n",
      "- variants/REF\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in sorted(callset.keys()):  print(\"-\",i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NA00001', 'NA00002', 'NA00003'], dtype=object)"
      ]
     },
     "execution_count": 772,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# The ‘samples’ array contains sample identifiers extracted from the header line in the VCF file.\n",
    "\n",
    "callset['samples']\n",
    "\n",
    "# look to the far right:\n",
    "# #CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO\tFORMAT\tNA00001\tNA00002\tNA00003\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['19', '19', '20', '20', '20', '20', '20', '20', 'X'], dtype=object)"
      ]
     },
     "execution_count": 773,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# All arrays with keys beginning ‘variants/’ come from the fixed fields in the VCF file.\n",
    "# For example, here is the data from the ‘CHROM’ field:\n",
    "\n",
    "callset['variants/CHROM']\n",
    "\n",
    "# chromosomes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    111,     112,   14370,   17330, 1110696, 1230237, 1234567,\n",
       "       1235237,      10])"
      ]
     },
     "execution_count": 774,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Here is the data from the ‘POS’ field:\n",
    "\n",
    "callset['variants/POS']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.6, 10. , 29. ,  3. , 67. , 47. , 50. ,  nan, 10. ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 775,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Here is the data from the ‘QUAL’ field:\n",
    "\n",
    "callset['variants/QUAL']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  0],\n",
       "        [ 0,  0],\n",
       "        [ 0,  1]],\n",
       "\n",
       "       [[ 0,  0],\n",
       "        [ 0,  0],\n",
       "        [ 0,  1]],\n",
       "\n",
       "       [[ 0,  0],\n",
       "        [ 1,  0],\n",
       "        [ 1,  1]],\n",
       "\n",
       "       [[ 0,  0],\n",
       "        [ 0,  1],\n",
       "        [ 0,  0]],\n",
       "\n",
       "       [[ 1,  2],\n",
       "        [ 2,  1],\n",
       "        [ 2,  2]],\n",
       "\n",
       "       [[ 0,  0],\n",
       "        [ 0,  0],\n",
       "        [ 0,  0]],\n",
       "\n",
       "       [[ 0,  1],\n",
       "        [ 0,  2],\n",
       "        [ 1,  1]],\n",
       "\n",
       "       [[ 0,  0],\n",
       "        [ 0,  0],\n",
       "        [-1, -1]],\n",
       "\n",
       "       [[ 0, -1],\n",
       "        [ 0,  1],\n",
       "        [ 0,  2]]], dtype=int8)"
      ]
     },
     "execution_count": 776,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# All arrays with keys beginning ‘calldata/’ come from the sample fields in the VCF file. \n",
    "# For example, here are the actual genotype calls from the ‘GT’ field:\n",
    "\n",
    "callset['calldata/GT']\n",
    "\n",
    "# Note the -1 values for one of the genotype calls. By default scikit-allel uses \n",
    "# -1 to indicate a missing value for any array with a signed integer data type \n",
    "# d(although you can change this if you want).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"allel allel-DisplayAs2D\"><span>&lt;GenotypeArray shape=(9, 3, 2) dtype=int8&gt;</span><table><thead><tr><th></th><th style=\"text-align: center\">0</th><th style=\"text-align: center\">1</th><th style=\"text-align: center\">2</th></tr></thead><tbody><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">0</th><td style=\"text-align: center\">0/0</td><td style=\"text-align: center\">0/0</td><td style=\"text-align: center\">0/1</td></tr><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">1</th><td style=\"text-align: center\">0/0</td><td style=\"text-align: center\">0/0</td><td style=\"text-align: center\">0/1</td></tr><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">2</th><td style=\"text-align: center\">0/0</td><td style=\"text-align: center\">1/0</td><td style=\"text-align: center\">1/1</td></tr><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">...</th><td style=\"text-align: center\" colspan=\"4\">...</td></tr><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">6</th><td style=\"text-align: center\">0/1</td><td style=\"text-align: center\">0/2</td><td style=\"text-align: center\">1/1</td></tr><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">7</th><td style=\"text-align: center\">0/0</td><td style=\"text-align: center\">0/0</td><td style=\"text-align: center\">./.</td></tr><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">8</th><td style=\"text-align: center\">0/.</td><td style=\"text-align: center\">0/1</td><td style=\"text-align: center\">0/2</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "<GenotypeArray shape=(9, 3, 2) dtype=int8>\n",
       "0/0 0/0 0/1\n",
       "0/0 0/0 0/1\n",
       "0/0 1/0 1/1\n",
       "...\n",
       "0/1 0/2 1/1\n",
       "0/0 0/0 ./.\n",
       "0/. 0/1 0/2"
      ]
     },
     "execution_count": 777,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Aside: genotype arrays\n",
    "# Because working with genotype calls is a very common task, scikit-allel has \n",
    "# a GenotypeArray class which adds some convenient functionality to an array \n",
    "# of genotype calls. To use this class, pass the raw NumPy array into the GenotypeArray \n",
    "# class constructor, e.g.:\n",
    "\n",
    "\n",
    "gt = allel.GenotypeArray(callset['calldata/GT'])\n",
    "\n",
    "gt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the things that the GenotypeArray class does is provide a slightly more visually-appealing representation when used in a Jupyter notebook, as can be seen above. There are also methods for making various computations over the genotype calls. For example, the is_het() method locates all heterozygous genotype calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [ True,  True, False],\n",
       "       [False, False, False],\n",
       "       [ True,  True, False],\n",
       "       [False, False, False],\n",
       "       [False,  True,  True]])"
      ]
     },
     "execution_count": 778,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt.is_het()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 2, 0, 2, 0, 2])"
      ]
     },
     "execution_count": 779,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# To give another example, the count_het() method will count heterozygous calls, summing over \n",
    "# variants (axis=0) or samples (axis=1) if requested.\n",
    "# E.g., to count the number of het calls per variant:\n",
    "    \n",
    "gt.count_het(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"allel allel-DisplayAs2D\"><span>&lt;AlleleCountsArray shape=(9, 3) dtype=int32&gt;</span><table><thead><tr><th></th><th style=\"text-align: center\">0</th><th style=\"text-align: center\">1</th><th style=\"text-align: center\">2</th></tr></thead><tbody><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">0</th><td style=\"text-align: center\">5</td><td style=\"text-align: center\">1</td><td style=\"text-align: center\">0</td></tr><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">1</th><td style=\"text-align: center\">5</td><td style=\"text-align: center\">1</td><td style=\"text-align: center\">0</td></tr><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">2</th><td style=\"text-align: center\">3</td><td style=\"text-align: center\">3</td><td style=\"text-align: center\">0</td></tr><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">...</th><td style=\"text-align: center\" colspan=\"4\">...</td></tr><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">6</th><td style=\"text-align: center\">2</td><td style=\"text-align: center\">3</td><td style=\"text-align: center\">1</td></tr><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">7</th><td style=\"text-align: center\">4</td><td style=\"text-align: center\">0</td><td style=\"text-align: center\">0</td></tr><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">8</th><td style=\"text-align: center\">3</td><td style=\"text-align: center\">1</td><td style=\"text-align: center\">1</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "<AlleleCountsArray shape=(9, 3) dtype=int32>\n",
       "5 1 0\n",
       "5 1 0\n",
       "3 3 0\n",
       "...\n",
       "2 3 1\n",
       "4 0 0\n",
       "3 1 1"
      ]
     },
     "execution_count": 780,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# One more example, here is how to perform an allele count,\n",
    "# i.e., count the number times each allele (0=reference, 1=first alternate, \n",
    "# 2=second alternate, etc.) is observed for each variant:\n",
    "    \n",
    "ac = gt.count_alleles()\n",
    "ac \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fields: \n",
    "    \n",
    "VCF files can often contain many fields of data, and you may only need to extract \n",
    "some of them to perform a particular analysis. You can select which fields to extract by passing a list of strings as the fields parameter. For example, let’s extract the ‘DP’ field from within the ‘INFO’ field, and let’s also extract the ‘DP’ field from the genotype call data:\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 781,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['calldata/DP', 'variants/DP']"
      ]
     },
     "execution_count": 781,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "callset = allel.read_vcf('C:/SPARK/sample.vcf', fields=['variants/DP', 'calldata/DP'])\n",
    "sorted(callset.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1, 14, 11, 10, 13,  9, -1, -1])"
      ]
     },
     "execution_count": 782,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# here is the data we just extracted:\n",
    "callset['variants/DP']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 783,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1, -1, -1],\n",
       "       [-1, -1, -1],\n",
       "       [ 1,  8,  5],\n",
       "       [ 3,  5,  3],\n",
       "       [ 6,  0,  4],\n",
       "       [-1,  4,  2],\n",
       "       [ 4,  2,  3],\n",
       "       [-1, -1, -1],\n",
       "       [-1, -1, -1]], dtype=int16)"
      ]
     },
     "execution_count": 783,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "callset['calldata/DP']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "I chose these two fields to illustrate the point that sometimes the same field name (e.g., ‘DP’) can be used both within the INFO field of a VCF and also within the genotype call data. When selecting fields, to make sure there is no ambiguity, you can include a prefix which is either ‘variants/’ or ‘calldata/’. For example, if you provide ‘variants/DP’, then the read_vcf() function will look for an INFO field named ‘DP’. If you provide ‘calldata/DP’ then read_vcf() will look for a FORMAT field named ‘DP’ within the call data.\n",
    "\n",
    "If you are feeling lazy, you can drop the ‘variants/’ and ‘calldata/’ prefixes, in which case read_vcf() will assume you mean ‘variants/’ if there is any ambiguity. E.g.:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['calldata/GT', 'variants/DP']"
      ]
     },
     "execution_count": 784,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "callset = allel.read_vcf('C:/SPARK/sample.vcf', fields=['DP', 'GT'])\n",
    "sorted(callset.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to extract absolutely everything from a VCF file, then you can provide a special value '*' as the fields parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['calldata/DP',\n",
       " 'calldata/GQ',\n",
       " 'calldata/GT',\n",
       " 'calldata/HQ',\n",
       " 'samples',\n",
       " 'variants/AA',\n",
       " 'variants/AC',\n",
       " 'variants/AF',\n",
       " 'variants/ALT',\n",
       " 'variants/AN',\n",
       " 'variants/CHROM',\n",
       " 'variants/DB',\n",
       " 'variants/DP',\n",
       " 'variants/FILTER_PASS',\n",
       " 'variants/FILTER_q10',\n",
       " 'variants/FILTER_s50',\n",
       " 'variants/H2',\n",
       " 'variants/ID',\n",
       " 'variants/NS',\n",
       " 'variants/POS',\n",
       " 'variants/QUAL',\n",
       " 'variants/REF',\n",
       " 'variants/is_snp',\n",
       " 'variants/numalt',\n",
       " 'variants/svlen']"
      ]
     },
     "execution_count": 785,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "callset = allel.read_vcf('C:/SPARK/sample.vcf', fields='*')\n",
    "sorted(callset.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also provide the special value 'variants/*' to request all variants fields (including all INFO), and the special value 'calldata/*' to request all call data fields.\n",
    "\n",
    "If you don’t specify the fields parameter, scikit-allel will default to extracting data from the fixed variants fields (but no INFO) and the GT genotype field if present (but no other call data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy arrays can have various data types, including signed integers (‘int8’, ‘int16’, ‘int32’, ‘int64’), unsigned integers (‘uint8’, ‘uint16’, ‘uint32’, ‘uint64’), floating point numbers (‘float32’, ‘float64’), variable length strings (‘object’) and fixed length strings (e.g., ‘S4’ for a 4-character ASCII string). scikit-allel will try to choose a sensible default data type for the fields you want to extract, based on the meta-information in the VCF file, but you can override these via the types parameter.\n",
    "\n",
    "For example, by default the ‘DP’ INFO field is loaded into a 32-bit integer array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1, 14, 11, 10, 13,  9, -1, -1])"
      ]
     },
     "execution_count": 786,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "callset = allel.read_vcf('C:/SPARK/sample.vcf', fields=['DP'])\n",
    "callset['variants/DP']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fields containing textual data, there are two choices for data type. By default, scikit-allel will use an ‘object’ data type, which means that values are stored as an array of Python strings. E.g.:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bam !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['A', 'A', 'G', 'T', 'A', 'T', 'G', 'T', 'AC'], dtype=object)"
      ]
     },
     "execution_count": 787,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "callset = allel.read_vcf('C:/SPARK/sample.vcf')\n",
    "callset['variants/REF']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'A', b'A', b'G', b'T', b'A', b'T', b'G', b'T', b'AC'], dtype='|S3')"
      ]
     },
     "execution_count": 788,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# The advantage of using ‘object’ dtype is that strings can be of any length. \n",
    "# Alternatively, you can use a fixed-length string dtype, e.g.:\n",
    "\n",
    "callset = allel.read_vcf('C:/SPARK/sample.vcf', types={'REF': 'S3'})\n",
    "callset['variants/REF']\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Some fields like ‘ALT’, ‘AC’ and ‘AF’ can have a variable number of values. I.e., each variant may have a different number of data values for these fields. One trade-off you have to make when loading data into NumPy arrays is that you cannot have arrays with a variable number of items per row. Rather, you have to fix the maximum number of possible items. While you lose some flexibility, you gain speed of access.\n",
    "\n",
    "For fields like ‘ALT’, scikit-allel will choose a default number of expected values, which is set at 3. E.g., here is what you get by default:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['C', '', ''],\n",
       "       ['G', '', ''],\n",
       "       ['A', '', ''],\n",
       "       ['A', '', ''],\n",
       "       ['G', 'T', ''],\n",
       "       ['', '', ''],\n",
       "       ['GA', 'GAC', ''],\n",
       "       ['', '', ''],\n",
       "       ['A', 'ATG', '']], dtype=object)"
      ]
     },
     "execution_count": 789,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "callset = allel.read_vcf('C:/SPARK/sample.vcf')\n",
    "callset['variants/ALT']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, 3 is more that we need, because no variant has more than 2 ALT values. However, some VCF files (especially those including INDELs) may have more than 3 ALT values.\n",
    "\n",
    "If you need to increase or decrease the expected number of values for any field, you can do this via the numbers parameter. E.g., increase the number of ALT values to 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['C', '', '', '', ''],\n",
       "       ['G', '', '', '', ''],\n",
       "       ['A', '', '', '', ''],\n",
       "       ['A', '', '', '', ''],\n",
       "       ['G', 'T', '', '', ''],\n",
       "       ['', '', '', '', ''],\n",
       "       ['GA', 'GAC', '', '', ''],\n",
       "       ['', '', '', '', ''],\n",
       "       ['A', 'ATG', '', '', '']], dtype=object)"
      ]
     },
     "execution_count": 790,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "callset = allel.read_vcf('C:/SPARK/sample.vcf', numbers={'ALT': 5})\n",
    "callset['variants/ALT']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genotype ploidy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "By default, scikit-allel assumes you are working with a diploid organism, and so expects to parse out 2 alleles for each genotype call. If you are working with an organism with some other ploidy, you can change the expected number of alleles via the numbers parameter.\n",
    "\n",
    "For example, here is an example VCF with tetraploid genotype calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can extract data for only a specific chromosome or genome region via the region parameter. The value of the parameter should be a region string of the format ‘{chromosome}:{begin}-{end}’, just like you would give to tabix or samtools. E.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1110696, 1230237])"
      ]
     },
     "execution_count": 792,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "callset = allel.read_vcf('C:/SPARK/sample.vcf', region='20:1000000-1231000')\n",
    "callset['variants/POS']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NA00001', 'NA00003'], dtype=object)"
      ]
     },
     "execution_count": 793,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# You can extract data for only specific samples via the samples parameter. \n",
    "# e.g. extract data for samples ‘NA00001’ and ‘NA00003’:\n",
    "\n",
    "callset = allel.read_vcf('C:/SPARK/sample.vcf', samples=['NA00001', 'NA00003'])\n",
    "callset['samples']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"allel allel-DisplayAs2D\"><span>&lt;GenotypeArray shape=(9, 2, 2) dtype=int8&gt;</span><table><thead><tr><th></th><th style=\"text-align: center\">0</th><th style=\"text-align: center\">1</th></tr></thead><tbody><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">0</th><td style=\"text-align: center\">0/0</td><td style=\"text-align: center\">0/1</td></tr><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">1</th><td style=\"text-align: center\">0/0</td><td style=\"text-align: center\">0/1</td></tr><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">2</th><td style=\"text-align: center\">0/0</td><td style=\"text-align: center\">1/1</td></tr><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">...</th><td style=\"text-align: center\" colspan=\"3\">...</td></tr><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">6</th><td style=\"text-align: center\">0/1</td><td style=\"text-align: center\">1/1</td></tr><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">7</th><td style=\"text-align: center\">0/0</td><td style=\"text-align: center\">./.</td></tr><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">8</th><td style=\"text-align: center\">0/.</td><td style=\"text-align: center\">0/2</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "<GenotypeArray shape=(9, 2, 2) dtype=int8>\n",
       "0/0 0/1\n",
       "0/0 0/1\n",
       "0/0 1/1\n",
       "...\n",
       "0/1 1/1\n",
       "0/0 ./.\n",
       "0/. 0/2"
      ]
     },
     "execution_count": 794,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "allel.GenotypeArray(callset['calldata/GT'])\n",
    "\n",
    "# Note that the genotype array now only has two columns, corresponding to the two samples requested.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## we can also take the .vcf and store it as hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 795,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  but will store extracted data into an HDF5 file stored on disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vcf_to_hdf5()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For large datasets, the vcf_to_hdf5() function is available. This function again takes similar parameters to read_vcf(), but will store extracted data into an HDF5 file stored on disk. The extraction process works through the VCF file in chunks, and so the entire dataset is never loaded entirely into main memory. A bit further below I give worked examples with a large dataset, but for now here is a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file already exists, moving along\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# STOP:  if this file already exists, this will error out, so make sure it doesn't already exist\n",
    "# saving the file directly as hdf5 ! ! ! \n",
    "\n",
    "try:\n",
    "    allel.vcf_to_hdf5('C:/SPARK/sample.vcf', 'C:/SPARK/sample2_hdf5.h5', fields='*', overwrite=True)\n",
    "except:\n",
    "    print('file already exists, moving along')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 file \"sample2_hdf5.h5\" (mode r)>"
      ]
     },
     "execution_count": 797,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 file \"sample2_hdf5.h5\" (mode r)>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# now lets assume i just showed up and wanted to review this information (the hdf5 file):\n",
    "\n",
    "import h5py  # conda install this \n",
    "\n",
    "callset = h5py.File('C:/SPARK/sample2_hdf5.h5', mode='r')\n",
    "\n",
    "callset\n",
    "\n",
    "print(callset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"CHROM\": shape (9,), type \"|O\">"
      ]
     },
     "execution_count": 798,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "chrom = callset['variants/CHROM']\n",
    "chrom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 799,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"POS\": shape (9,), type \"<i4\">"
      ]
     },
     "execution_count": 799,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pos = callset['variants/POS']\n",
    "pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 800,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"GT\": shape (9, 3, 2), type \"|i1\">"
      ]
     },
     "execution_count": 800,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "gt = callset['calldata/GT']\n",
    "gt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 801,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['19', '20'], dtype=object)"
      ]
     },
     "execution_count": 801,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# This dataset object is useful because you can then load all or only part of \n",
    "# the underlying data into main memory via slicing. e.g.\n",
    "\n",
    "# load second to fourth items into NumPy array\n",
    "chrom[1:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"allel allel-DisplayAs2D\"><span>&lt;GenotypeArray shape=(2, 3, 2) dtype=int8&gt;</span><table><thead><tr><th></th><th style=\"text-align: center\">0</th><th style=\"text-align: center\">1</th><th style=\"text-align: center\">2</th></tr></thead><tbody><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">0</th><td style=\"text-align: center\">0/0</td><td style=\"text-align: center\">0/0</td><td style=\"text-align: center\">0/1</td></tr><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">1</th><td style=\"text-align: center\">0/0</td><td style=\"text-align: center\">1/0</td><td style=\"text-align: center\">1/1</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "<GenotypeArray shape=(2, 3, 2) dtype=int8>\n",
       "0/0 0/0 0/1\n",
       "0/0 1/0 1/1"
      ]
     },
     "execution_count": 802,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# load genotype calls into memory for second to fourth variants, all samples\n",
    "allel.GenotypeArray(gt[1:3, :])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assume you want all of this in a dataframe ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 803,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CHROM</th>\n",
       "      <th>POS</th>\n",
       "      <th>ID</th>\n",
       "      <th>REF</th>\n",
       "      <th>ALT_1</th>\n",
       "      <th>ALT_2</th>\n",
       "      <th>ALT_3</th>\n",
       "      <th>QUAL</th>\n",
       "      <th>FILTER_PASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>111</td>\n",
       "      <td>.</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>9.6</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>112</td>\n",
       "      <td>.</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>14370</td>\n",
       "      <td>rs6054257</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>29.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>17330</td>\n",
       "      <td>.</td>\n",
       "      <td>T</td>\n",
       "      <td>A</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>1110696</td>\n",
       "      <td>rs6040355</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>T</td>\n",
       "      <td></td>\n",
       "      <td>67.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>1230237</td>\n",
       "      <td>.</td>\n",
       "      <td>T</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>47.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20</td>\n",
       "      <td>1234567</td>\n",
       "      <td>microsat1</td>\n",
       "      <td>G</td>\n",
       "      <td>GA</td>\n",
       "      <td>GAC</td>\n",
       "      <td></td>\n",
       "      <td>50.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20</td>\n",
       "      <td>1235237</td>\n",
       "      <td>.</td>\n",
       "      <td>T</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>X</td>\n",
       "      <td>10</td>\n",
       "      <td>rsTest</td>\n",
       "      <td>AC</td>\n",
       "      <td>A</td>\n",
       "      <td>ATG</td>\n",
       "      <td></td>\n",
       "      <td>10.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CHROM      POS         ID REF ALT_1 ALT_2 ALT_3  QUAL  FILTER_PASS\n",
       "0    19      111          .   A     C               9.6        False\n",
       "1    19      112          .   A     G              10.0        False\n",
       "2    20    14370  rs6054257   G     A              29.0         True\n",
       "3    20    17330          .   T     A               3.0        False\n",
       "4    20  1110696  rs6040355   A     G     T        67.0         True\n",
       "5    20  1230237          .   T                    47.0         True\n",
       "6    20  1234567  microsat1   G    GA   GAC        50.0         True\n",
       "7    20  1235237          .   T                     NaN        False\n",
       "8     X       10     rsTest  AC     A   ATG        10.0         True"
      ]
     },
     "execution_count": 803,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# For some analyses it can be useful to think of the data in a VCF file as a table or data frame, \n",
    "# especially if you are only analysing data from the fixed fields and don’t need the genotype \n",
    "# calls or any other call data. The vcf_to_dataframe() function extracts data from a VCF and \n",
    "# loads into a pandas DataFrame. E.g.:\n",
    "\n",
    "df = allel.vcf_to_dataframe('C:/SPARK/sample.vcf')\n",
    "\n",
    "df\n",
    "\n",
    "# extract my data from the vcf and put into a dataframe ! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CHROM      POS         ID REF ALT_1 ALT_2 ALT_3  QUAL  FILTER_PASS\n",
      "0    19      111          .   A     C               9.6        False\n",
      "1    19      112          .   A     G              10.0        False\n",
      "2    20    14370  rs6054257   G     A              29.0         True\n",
      "3    20    17330          .   T     A               3.0        False\n",
      "4    20  1110696  rs6040355   A     G     T        67.0         True\n",
      "5    20  1230237          .   T                    47.0         True\n",
      "6    20  1234567  microsat1   G    GA   GAC        50.0         True\n",
      "7    20  1235237          .   T                     NaN        False\n",
      "8     X       10     rsTest  AC     A   ATG        10.0         True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# so its just the original vcf data but put into a dataframe, which is cool:\n",
    "\n",
    "# #CHROM  POS    ID    REF    ALT    QUAL   \n",
    "\n",
    "# 19      111         .    A    C    9.6   \n",
    "# 19      112          .    A    G    10    \n",
    "# 20      14370        rs6054257    G    A    29    \n",
    "# 20      17330    .    T    A    3    q10       \n",
    "# 20      1110696    rs6040355    A    G,T    67   \n",
    "# 20      1230237    .    T    .    47      \n",
    "# 20      1234567    microsat1    G    GA,GAC    50   \n",
    "# 20      1235237    .    T    .    .    .    .   \n",
    "# X       10    rsTest    AC    A,ATG    10    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CHROM</th>\n",
       "      <th>POS</th>\n",
       "      <th>ID</th>\n",
       "      <th>REF</th>\n",
       "      <th>ALT_1</th>\n",
       "      <th>ALT_2</th>\n",
       "      <th>QUAL</th>\n",
       "      <th>DB</th>\n",
       "      <th>AN</th>\n",
       "      <th>DP</th>\n",
       "      <th>...</th>\n",
       "      <th>NS</th>\n",
       "      <th>AC_1</th>\n",
       "      <th>AC_2</th>\n",
       "      <th>FILTER_PASS</th>\n",
       "      <th>FILTER_q10</th>\n",
       "      <th>FILTER_s50</th>\n",
       "      <th>numalt</th>\n",
       "      <th>svlen_1</th>\n",
       "      <th>svlen_2</th>\n",
       "      <th>is_snp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>111</td>\n",
       "      <td>.</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td></td>\n",
       "      <td>9.6</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>112</td>\n",
       "      <td>.</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td></td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>14370</td>\n",
       "      <td>rs6054257</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td></td>\n",
       "      <td>29.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>17330</td>\n",
       "      <td>.</td>\n",
       "      <td>T</td>\n",
       "      <td>A</td>\n",
       "      <td></td>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>1110696</td>\n",
       "      <td>rs6040355</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>T</td>\n",
       "      <td>67.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>1230237</td>\n",
       "      <td>.</td>\n",
       "      <td>T</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>47.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20</td>\n",
       "      <td>1234567</td>\n",
       "      <td>microsat1</td>\n",
       "      <td>G</td>\n",
       "      <td>GA</td>\n",
       "      <td>GAC</td>\n",
       "      <td>50.0</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20</td>\n",
       "      <td>1235237</td>\n",
       "      <td>.</td>\n",
       "      <td>T</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>X</td>\n",
       "      <td>10</td>\n",
       "      <td>rsTest</td>\n",
       "      <td>AC</td>\n",
       "      <td>A</td>\n",
       "      <td>ATG</td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  CHROM      POS         ID REF ALT_1 ALT_2  QUAL     DB  AN  DP   ...    NS  \\\n",
       "0    19      111          .   A     C         9.6  False  -1  -1   ...    -1   \n",
       "1    19      112          .   A     G        10.0  False  -1  -1   ...    -1   \n",
       "2    20    14370  rs6054257   G     A        29.0   True  -1  14   ...     3   \n",
       "3    20    17330          .   T     A         3.0  False  -1  11   ...     3   \n",
       "4    20  1110696  rs6040355   A     G     T  67.0   True  -1  10   ...     2   \n",
       "5    20  1230237          .   T              47.0  False  -1  13   ...     3   \n",
       "6    20  1234567  microsat1   G    GA   GAC  50.0  False   6   9   ...     3   \n",
       "7    20  1235237          .   T               NaN  False  -1  -1   ...    -1   \n",
       "8     X       10     rsTest  AC     A   ATG  10.0  False  -1  -1   ...    -1   \n",
       "\n",
       "  AC_1  AC_2  FILTER_PASS  FILTER_q10  FILTER_s50  numalt  svlen_1  svlen_2  \\\n",
       "0   -1    -1        False       False       False       1        0        0   \n",
       "1   -1    -1        False       False       False       1        0        0   \n",
       "2   -1    -1         True       False       False       1        0        0   \n",
       "3   -1    -1        False        True       False       1        0        0   \n",
       "4   -1    -1         True       False       False       2        0        0   \n",
       "5   -1    -1         True       False       False       0        0        0   \n",
       "6    3     1         True       False       False       2        1        2   \n",
       "7   -1    -1        False       False       False       0        0        0   \n",
       "8   -1    -1         True       False       False       2       -1        1   \n",
       "\n",
       "   is_snp  \n",
       "0    True  \n",
       "1    True  \n",
       "2    True  \n",
       "3    True  \n",
       "4    True  \n",
       "5   False  \n",
       "6   False  \n",
       "7   False  \n",
       "8   False  \n",
       "\n",
       "[9 rows x 24 columns]"
      ]
     },
     "execution_count": 807,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# some values (cols) on the right were missing, but if you want ALL of them:\n",
    "\n",
    "df = allel.vcf_to_dataframe('C:/SPARK/sample.vcf', fields='*', alt_number=2)\n",
    "df\n",
    "\n",
    "# pandas type reflection \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CHROM      POS         ID REF ALT_1 ALT_2  QUAL     DB  AN  DP   ...    NS  \\\n",
      "0    19      111          .   A     C         9.6  False  -1  -1   ...    -1   \n",
      "1    19      112          .   A     G        10.0  False  -1  -1   ...    -1   \n",
      "2    20    14370  rs6054257   G     A        29.0   True  -1  14   ...     3   \n",
      "3    20    17330          .   T     A         3.0  False  -1  11   ...     3   \n",
      "4    20  1110696  rs6040355   A     G     T  67.0   True  -1  10   ...     2   \n",
      "5    20  1230237          .   T              47.0  False  -1  13   ...     3   \n",
      "6    20  1234567  microsat1   G    GA   GAC  50.0  False   6   9   ...     3   \n",
      "7    20  1235237          .   T               NaN  False  -1  -1   ...    -1   \n",
      "8     X       10     rsTest  AC     A   ATG  10.0  False  -1  -1   ...    -1   \n",
      "\n",
      "  AC_1  AC_2  FILTER_PASS  FILTER_q10  FILTER_s50  numalt  svlen_1  svlen_2  \\\n",
      "0   -1    -1        False       False       False       1        0        0   \n",
      "1   -1    -1        False       False       False       1        0        0   \n",
      "2   -1    -1         True       False       False       1        0        0   \n",
      "3   -1    -1        False        True       False       1        0        0   \n",
      "4   -1    -1         True       False       False       2        0        0   \n",
      "5   -1    -1         True       False       False       0        0        0   \n",
      "6    3     1         True       False       False       2        1        2   \n",
      "7   -1    -1        False       False       False       0        0        0   \n",
      "8   -1    -1         True       False       False       2       -1        1   \n",
      "\n",
      "   is_snp  \n",
      "0    True  \n",
      "1    True  \n",
      "2    True  \n",
      "3    True  \n",
      "4    True  \n",
      "5   False  \n",
      "6   False  \n",
      "7   False  \n",
      "8   False  \n",
      "\n",
      "[9 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(df)\n",
    "\n",
    "# bam ! \n",
    "\n",
    "# i actually think this format is clearer ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CHROM</th>\n",
       "      <th>POS</th>\n",
       "      <th>ID</th>\n",
       "      <th>REF</th>\n",
       "      <th>ALT_1</th>\n",
       "      <th>ALT_2</th>\n",
       "      <th>QUAL</th>\n",
       "      <th>DB</th>\n",
       "      <th>AN</th>\n",
       "      <th>DP</th>\n",
       "      <th>...</th>\n",
       "      <th>NS</th>\n",
       "      <th>AC_1</th>\n",
       "      <th>AC_2</th>\n",
       "      <th>FILTER_PASS</th>\n",
       "      <th>FILTER_q10</th>\n",
       "      <th>FILTER_s50</th>\n",
       "      <th>numalt</th>\n",
       "      <th>svlen_1</th>\n",
       "      <th>svlen_2</th>\n",
       "      <th>is_snp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>14370</td>\n",
       "      <td>rs6054257</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td></td>\n",
       "      <td>29.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>1230237</td>\n",
       "      <td>.</td>\n",
       "      <td>T</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>47.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  CHROM      POS         ID REF ALT_1 ALT_2  QUAL     DB  AN  DP   ...    NS  \\\n",
       "2    20    14370  rs6054257   G     A        29.0   True  -1  14   ...     3   \n",
       "5    20  1230237          .   T              47.0  False  -1  13   ...     3   \n",
       "\n",
       "  AC_1  AC_2  FILTER_PASS  FILTER_q10  FILTER_s50  numalt  svlen_1  svlen_2  \\\n",
       "2   -1    -1         True       False       False       1        0        0   \n",
       "5   -1    -1         True       False       False       0        0        0   \n",
       "\n",
       "   is_snp  \n",
       "2    True  \n",
       "5   False  \n",
       "\n",
       "[2 rows x 24 columns]"
      ]
     },
     "execution_count": 809,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# wow, you can query with SparkSQL and bendify anything you want !\n",
    "\n",
    "df.query('DP > 10 and QUAL > 20')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHROM,POS,DP\n",
      "19,111,-1\n",
      "19,112,-1\n",
      "20,14370,14\n",
      "20,17330,11\n",
      "20,1110696,10\n",
      "20,1230237,13\n",
      "20,1234567,9\n",
      "20,1235237,-1\n",
      "X,10,-1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### export as .csv\n",
    "\n",
    "allel.vcf_to_csv('C:/SPARK/sample.vcf', 'C:/SPARK/example.csv', fields=['CHROM', 'POS', 'DP'])\n",
    "\n",
    "\n",
    "with open('C:/SPARK/example.csv', mode='r') as f:\n",
    "    print(f.read())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### so you have thme file saved within a dataframe, you can now manipulate it with SQL commands if you wanted to ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Now lets pull down a massive .vcf file and process it ! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* http://hgdownload.cse.ucsc.edu/gbdb/hg19/1000Genomes/phase3/?C=S;O=A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# downloaded a 1.2 GB file of VCF raw data\n",
    "\n",
    "vcf_path = 'C:/SPARK/ALL.chr1.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz'\n",
    "#http://alimanfoo.github.io/2017/06/14/read-vcf.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 TBresee mkpasswd 1.2G Jun  4 19:33 C:/SPARK/ALL.chr1.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!ls -lh {vcf_path}\n",
    "\n",
    "# bam, a 1.2G file \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[read_vcf] 65536 rows in 4.13s; chunk in 4.13s (15885 rows/s); 1\u0000:2308933\n",
      "[read_vcf] 131072 rows in 8.07s; chunk in 3.94s (16631 rows/s); 1\u0000:4177969\n",
      "[read_vcf] 196608 rows in 11.93s; chunk in 3.86s (16981 rows/s); 1\u0000:6022445\n",
      "[read_vcf] 262144 rows in 15.72s; chunk in 3.79s (17277 rows/s); 1\u0000:8078286\n",
      "[read_vcf] 327680 rows in 19.77s; chunk in 4.05s (16194 rows/s); 1\u0000:10246876\n",
      "[read_vcf] 393216 rows in 23.59s; chunk in 3.82s (17146 rows/s); 1\u0000:12313599\n",
      "[read_vcf] 458752 rows in 27.66s; chunk in 4.07s (16092 rows/s); 1\u0000:15033300\n",
      "[read_vcf] 524288 rows in 31.80s; chunk in 4.14s (15827 rows/s); 1\u0000:17226235\n",
      "[read_vcf] 589824 rows in 36.17s; chunk in 4.37s (14999 rows/s); 1\u0000:19176875\n",
      "[read_vcf] 655360 rows in 40.34s; chunk in 4.17s (15700 rows/s); 1\u0000:21331176\n",
      "[read_vcf] 720896 rows in 44.54s; chunk in 4.20s (15600 rows/s); 1\u0000:23514706\n",
      "[read_vcf] 786432 rows in 48.65s; chunk in 4.10s (15967 rows/s); 1\u0000:25882976\n",
      "[read_vcf] 851968 rows in 52.66s; chunk in 4.01s (16332 rows/s); 1\u0000:28279507\n",
      "[read_vcf] 917504 rows in 56.58s; chunk in 3.92s (16728 rows/s); 1\u0000:30713629\n",
      "[read_vcf] 983040 rows in 60.56s; chunk in 3.99s (16444 rows/s); 1\u0000:32885037\n",
      "[read_vcf] 1048576 rows in 64.51s; chunk in 3.94s (16617 rows/s); 1\u0000:35172507\n",
      "[read_vcf] 1114112 rows in 68.63s; chunk in 4.13s (15886 rows/s); 1\u0000:37478330\n",
      "[read_vcf] 1179648 rows in 72.60s; chunk in 3.96s (16538 rows/s); 1\u0000:39748103\n",
      "[read_vcf] 1245184 rows in 76.60s; chunk in 4.00s (16382 rows/s); 1\u0000:42086852\n",
      "[read_vcf] 1310720 rows in 80.73s; chunk in 4.14s (15846 rows/s); 1\u0000:44445975\n",
      "[read_vcf] 1376256 rows in 84.82s; chunk in 4.09s (16016 rows/s); 1\u0000:46867070\n",
      "[read_vcf] 1441792 rows in 89.18s; chunk in 4.35s (15051 rows/s); 1\u0000:49152193\n",
      "[read_vcf] 1507328 rows in 93.03s; chunk in 3.85s (17024 rows/s); 1\u0000:51791580\n",
      "[read_vcf] 1572864 rows in 96.93s; chunk in 3.91s (16776 rows/s); 1\u0000:54133705\n",
      "[read_vcf] 1638400 rows in 101.17s; chunk in 4.23s (15477 rows/s); 1\u0000:56272802\n",
      "[read_vcf] 1703936 rows in 105.13s; chunk in 3.97s (16522 rows/s); 1\u0000:58498999\n",
      "[read_vcf] 1769472 rows in 109.00s; chunk in 3.87s (16941 rows/s); 1\u0000:60743161\n",
      "[read_vcf] 1835008 rows in 112.92s; chunk in 3.91s (16749 rows/s); 1\u0000:63047401\n",
      "[read_vcf] 1900544 rows in 117.72s; chunk in 4.80s (13651 rows/s); 1\u0000:65448230\n",
      "[read_vcf] 1966080 rows in 121.84s; chunk in 4.12s (15897 rows/s); 1\u0000:67804819\n",
      "[read_vcf] 2031616 rows in 125.88s; chunk in 4.04s (16214 rows/s); 1\u0000:70086719\n",
      "[read_vcf] 2097152 rows in 130.06s; chunk in 4.18s (15690 rows/s); 1\u0000:72517936\n",
      "[read_vcf] 2162688 rows in 134.49s; chunk in 4.44s (14776 rows/s); 1\u0000:74833059\n",
      "[read_vcf] 2228224 rows in 138.43s; chunk in 3.93s (16658 rows/s); 1\u0000:77144733\n",
      "[read_vcf] 2293760 rows in 142.26s; chunk in 3.83s (17108 rows/s); 1\u0000:79495341\n",
      "[read_vcf] 2359296 rows in 146.37s; chunk in 4.11s (15930 rows/s); 1\u0000:81601478\n",
      "[read_vcf] 2424832 rows in 150.63s; chunk in 4.26s (15378 rows/s); 1\u0000:83811587\n",
      "[read_vcf] 2490368 rows in 154.53s; chunk in 3.90s (16798 rows/s); 1\u0000:86217241\n",
      "[read_vcf] 2555904 rows in 158.48s; chunk in 3.94s (16618 rows/s); 1\u0000:88536030\n",
      "[read_vcf] 2621440 rows in 162.39s; chunk in 3.91s (16767 rows/s); 1\u0000:90958154\n",
      "[read_vcf] 2686976 rows in 166.74s; chunk in 4.35s (15051 rows/s); 1\u0000:93388079\n",
      "[read_vcf] 2752512 rows in 170.70s; chunk in 3.96s (16567 rows/s); 1\u0000:95785935\n",
      "[read_vcf] 2818048 rows in 174.96s; chunk in 4.27s (15363 rows/s); 1\u0000:98123341\n",
      "[read_vcf] 2883584 rows in 179.39s; chunk in 4.43s (14806 rows/s); 1\u0000:100426497\n",
      "[read_vcf] 2949120 rows in 183.42s; chunk in 4.03s (16257 rows/s); 1\u0000:102706814\n",
      "[read_vcf] 3014656 rows in 187.44s; chunk in 4.02s (16303 rows/s); 1\u0000:105066606\n",
      "[read_vcf] 3080192 rows in 191.45s; chunk in 4.01s (16341 rows/s); 1\u0000:107035468\n",
      "[read_vcf] 3145728 rows in 195.68s; chunk in 4.23s (15501 rows/s); 1\u0000:109497713\n",
      "[read_vcf] 3211264 rows in 200.89s; chunk in 5.22s (12562 rows/s); 1\u0000:111823987\n",
      "[read_vcf] 3276800 rows in 205.07s; chunk in 4.18s (15679 rows/s); 1\u0000:114123013\n",
      "[read_vcf] 3342336 rows in 209.66s; chunk in 4.58s (14302 rows/s); 1\u0000:116449117\n",
      "[read_vcf] 3407872 rows in 213.67s; chunk in 4.02s (16319 rows/s); 1\u0000:118807176\n",
      "[read_vcf] 3473408 rows in 217.56s; chunk in 3.89s (16853 rows/s); 1\u0000:121361045\n",
      "[read_vcf] 3538944 rows in 221.50s; chunk in 3.94s (16628 rows/s); 1\u0000:146730296\n",
      "[read_vcf] 3604480 rows in 225.85s; chunk in 4.35s (15081 rows/s); 1\u0000:150430552\n",
      "[read_vcf] 3670016 rows in 230.41s; chunk in 4.56s (14365 rows/s); 1\u0000:152825060\n",
      "[read_vcf] 3735552 rows in 234.59s; chunk in 4.18s (15660 rows/s); 1\u0000:155138015\n",
      "[read_vcf] 3801088 rows in 239.08s; chunk in 4.49s (14594 rows/s); 1\u0000:157518826\n",
      "[read_vcf] 3866624 rows in 243.39s; chunk in 4.31s (15216 rows/s); 1\u0000:159647947\n",
      "[read_vcf] 3932160 rows in 247.88s; chunk in 4.49s (14607 rows/s); 1\u0000:161935207\n",
      "[read_vcf] 3997696 rows in 251.86s; chunk in 3.98s (16474 rows/s); 1\u0000:164144117\n",
      "[read_vcf] 4063232 rows in 256.21s; chunk in 4.36s (15043 rows/s); 1\u0000:166317740\n",
      "[read_vcf] 4128768 rows in 260.96s; chunk in 4.75s (13806 rows/s); 1\u0000:168527049\n",
      "[read_vcf] 4194304 rows in 265.00s; chunk in 4.04s (16214 rows/s); 1\u0000:170801668\n",
      "[read_vcf] 4259840 rows in 269.12s; chunk in 4.12s (15912 rows/s); 1\u0000:173165318\n",
      "[read_vcf] 4325376 rows in 273.03s; chunk in 3.91s (16746 rows/s); 1\u0000:175588900\n",
      "[read_vcf] 4390912 rows in 277.16s; chunk in 4.13s (15883 rows/s); 1\u0000:177907681\n",
      "[read_vcf] 4456448 rows in 281.08s; chunk in 3.92s (16718 rows/s); 1\u0000:180243702\n",
      "[read_vcf] 4521984 rows in 285.20s; chunk in 4.12s (15906 rows/s); 1\u0000:182517611\n",
      "[read_vcf] 4587520 rows in 289.31s; chunk in 4.11s (15932 rows/s); 1\u0000:184881497\n",
      "[read_vcf] 4653056 rows in 293.58s; chunk in 4.27s (15356 rows/s); 1\u0000:187198495\n",
      "[read_vcf] 4718592 rows in 297.63s; chunk in 4.05s (16168 rows/s); 1\u0000:189298208\n",
      "[read_vcf] 4784128 rows in 301.83s; chunk in 4.19s (15628 rows/s); 1\u0000:191463222\n",
      "[read_vcf] 4849664 rows in 305.81s; chunk in 3.98s (16466 rows/s); 1\u0000:193813991\n",
      "[read_vcf] 4915200 rows in 309.85s; chunk in 4.04s (16208 rows/s); 1\u0000:195952433\n",
      "[read_vcf] 4980736 rows in 313.68s; chunk in 3.83s (17096 rows/s); 1\u0000:198270907\n",
      "[read_vcf] 5046272 rows in 317.76s; chunk in 4.08s (16076 rows/s); 1\u0000:200656420\n",
      "[read_vcf] 5111808 rows in 321.66s; chunk in 3.90s (16793 rows/s); 1\u0000:202897382\n",
      "[read_vcf] 5177344 rows in 325.71s; chunk in 4.05s (16176 rows/s); 1\u0000:205098886\n",
      "[read_vcf] 5242880 rows in 329.85s; chunk in 4.14s (15837 rows/s); 1\u0000:207870968\n",
      "[read_vcf] 5308416 rows in 334.03s; chunk in 4.18s (15677 rows/s); 1\u0000:210058821\n",
      "[read_vcf] 5373952 rows in 338.10s; chunk in 4.07s (16108 rows/s); 1\u0000:212310880\n",
      "[read_vcf] 5439488 rows in 342.29s; chunk in 4.19s (15629 rows/s); 1\u0000:214630869\n",
      "[read_vcf] 5505024 rows in 346.22s; chunk in 3.93s (16682 rows/s); 1\u0000:216886865\n",
      "[read_vcf] 5570560 rows in 350.16s; chunk in 3.94s (16622 rows/s); 1\u0000:219052919\n",
      "[read_vcf] 5636096 rows in 354.21s; chunk in 4.05s (16178 rows/s); 1\u0000:221366760\n",
      "[read_vcf] 5701632 rows in 358.34s; chunk in 4.13s (15881 rows/s); 1\u0000:223680945\n",
      "[read_vcf] 5767168 rows in 362.73s; chunk in 4.39s (14941 rows/s); 1\u0000:226104697\n",
      "[read_vcf] 5832704 rows in 366.95s; chunk in 4.23s (15510 rows/s); 1\u0000:228405212\n",
      "[read_vcf] 5898240 rows in 370.92s; chunk in 3.97s (16500 rows/s); 1\u0000:230542914\n",
      "[read_vcf] 5963776 rows in 374.89s; chunk in 3.97s (16525 rows/s); 1\u0000:232742459\n",
      "[read_vcf] 6029312 rows in 378.89s; chunk in 4.00s (16392 rows/s); 1\u0000:234886959\n",
      "[read_vcf] 6094848 rows in 383.10s; chunk in 4.21s (15562 rows/s); 1\u0000:237093353\n",
      "[read_vcf] 6160384 rows in 387.58s; chunk in 4.48s (14637 rows/s); 1\u0000:239206947\n",
      "[read_vcf] 6225920 rows in 391.99s; chunk in 4.41s (14848 rows/s); 1\u0000:241305636\n",
      "[read_vcf] 6291456 rows in 395.92s; chunk in 3.93s (16690 rows/s); 1\u0000:243454558\n",
      "[read_vcf] 6356992 rows in 399.82s; chunk in 3.91s (16781 rows/s); 1\u0000:245642248\n",
      "[read_vcf] 6422528 rows in 403.99s; chunk in 4.16s (15739 rows/s); 1\u0000:247641903\n",
      "[read_vcf] 6468094 rows in 406.83s; chunk in 2.85s (15993 rows/s)\n",
      "[read_vcf] all done (15898 rows/s)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "callset = allel.read_vcf(vcf_path, fields=['numalt'], log=sys.stdout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "When processing larger VCF files it’s useful to get some feedback on how fast things are going. \n",
    "\n",
    "Ultimately I am going to extract all the data from this VCF file into a Zarr store. \n",
    "However, before I do that, I’m going to check how many alternate alleles I should expect. \n",
    "I’m going to do that by extracting just the ‘numalt’ field, which scikit-allel will compute \n",
    "from the number of values in the ‘ALT’ field:\n",
    "``` \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 814,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# let’s see what the largest number of alternate alleles is:\n",
    "\n",
    "numalt = callset['variants/numalt']\n",
    "\n",
    "np.max(numalt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([      0, 6437262,   29538,    1064,     165,      49,      10,\n",
       "             5,       0,       0,       0,       0,       1], dtype=int64)"
      ]
     },
     "execution_count": 815,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Out of interest, how many variants are multi-allelic?\n",
    "count_numalt = np.bincount(numalt)\n",
    "count_numalt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30832"
      ]
     },
     "execution_count": 816,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "n_multiallelic = np.sum(count_numalt[2:])\n",
    "n_multiallelic\n",
    "\n",
    "# So there are only a very small number of multi-allelic variants (30,832), the vast majority (6,437,262) \n",
    "# have just one alternate allele.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
