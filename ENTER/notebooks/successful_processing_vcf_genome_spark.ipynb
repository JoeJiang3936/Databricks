{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Generation DNA Genome Sequencing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  *introduction to SparkSQL*   \n",
    ">  *reading DNA .vcf files*    \n",
    ">  *processing .vcf files in bulk to derive insights*  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diving In"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Tom Bresee*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.  &nbsp;  Load Relevant Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# my setup:\n",
    "\n",
    "#   In my case, this is running Anaconda Windows10 scenario\n",
    "\n",
    "#   Ill do the same version but on Databricks and straight ApachSpark \n",
    "#   in another file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load basic libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import random\n",
    "from random import random\n",
    "from operator import add\n",
    "import os, sys, time\n",
    "from time import time\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import more libraries\n",
    "import pyspark\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "# SQL\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "# Alchemy\n",
    "from sqlalchemy.engine import create_engine\n",
    "# ALS\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.mllib.recommendation import ALS\n",
    "# Logistic Regression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "# Kmeans\n",
    "from pyspark.mllib.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the spark \n",
    "import findspark\n",
    "#findspark.find()\n",
    "findspark.init()\n",
    "# to be safe on windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ![My Title](Images/mypic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.  &nbsp; This is the command you start with on Windows 10 to get SparkSQL going"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkSession in Spark 2.0 provides builtin support for Hive features including the ability to write queries using HiveQL, access to Hive UDFs, and the ability to read data from Hive tables. To use these features, you do not need to have an existing Hive setup FYI..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# READ:\n",
    "#   The entry point into **all** functionality in Spark is the *SparkSession* class.\n",
    "#   There are no longer sep instances depending on your application\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# SparkSession in Spark 2.0 provides builtin support for Hive features including \n",
    "# the ability to write queries using HiveQL, access to Hive UDFs, and the ability \n",
    "# to read data from Hive tables. To use these features, you do not need to have an \n",
    "# existing Hive setup.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x000001CC6FF767F0>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(spark)\n",
    "\n",
    "# see how you now have a .sql.session.SparkSession (it is sql specific)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.session.SparkSession'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(type(spark))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://TM0493322.gsm1900.org:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Python Spark SQL basic example</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1cc6ff767f0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Builder\n",
      "builder\n",
      "catalog\n",
      "conf\n",
      "createDataFrame\n",
      "newSession\n",
      "range\n",
      "read\n",
      "readStream\n",
      "sparkContext\n",
      "sql\n",
      "stop\n",
      "streams\n",
      "table\n",
      "udf\n",
      "version\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in dir(spark):\n",
    "    if not i.startswith(\"_\"):\n",
    "        print(i)\n",
    "        \n",
    "# these are the spark.methods available once you create the context \n",
    "# i.e. spark.read is a method or spark.udf is a method, etc\n",
    "\n",
    "# spark.stop() for instance will stop the sparksession\n",
    "# spark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.3'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# what is your spark version ? \n",
    "\n",
    "spark.version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-12-47b9844c153f>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-12-47b9844c153f>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    spark.\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In earlier versions of Spark, spark context was entry point for Spark.  As RDD was main API, it was created and manipulated using context API’s. For every other API,we needed to use different contexts. For streaming, we needed StreamingContext, for SQL sqlContext and for hive HiveContext. \n",
    "But as DataSet and Dataframe API’s are becoming new standard API’s we need an entry point build for them. So in Spark 2.0, we have a new entry point for DataSet and Dataframe API’s called as Spark Session.\n",
    "SparkSession is essentially combination of SQLContext, HiveContext and future StreamingContext. All the API’s available on those contexts are available on spark session also. Spark session internally has a spark context for actual computation.\n",
    "\n",
    "So if you get confused, you need to remember there was a major shift in how you created contexts with Spark 2.x..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# spark.catalog()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.  &nbsp;  Begin Diving into SparkSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inferring the Schema\n",
    "With a SQLContext, we are ready to create a DataFrame from our existing RDD. But first we need to tell Spark SQL the schema in our data.  Spark SQL can convert an RDD of Row objects to a DataFrame. Rows are constructed by passing a list of key/value pairs as kwargs to the Row class. The keys define the column names, and the types are inferred by looking at the first row. Therefore, it is important that there is no missing data in the first row of the RDD in order to properly infer the schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a SparkSession, applications can create DataFrames from an existing RDD, from a Hive table, or from Spark data sources.\n",
    "As an example, the following creates a DataFrame based on the content of a JSON file:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print out all existing databases ! \n",
    "\n",
    "spark.catalog.listDatabases()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# current database:\n",
    "\n",
    "spark.catalog.currentDatabase()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# list all current tables, there are none yet ...\n",
    "\n",
    "spark.catalog.listTables()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# list all .methods under catalog:\n",
    "\n",
    "for i in dir(spark.catalog): \n",
    "    if not i.startswith(\"_\"): \n",
    "        print(i)\n",
    "\n",
    "# you will see me do this alot, you want to see the commands that are possible for spark.catalog.X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading a json file into a dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# spark is the existing sql SparkSession\n",
    "df = spark.read.json(\"C:/SPARK/examples/example.json\")\n",
    "\n",
    "# Displays the content of the DataFrame to stdout\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"the df instance is this type ->  \", type(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# list out the methods under read, i.e. how can i read stuff in,what are my read options ?  \n",
    "\n",
    "for i in dir(spark.read):\n",
    "    if not i.startswith(\"_\"):\n",
    "        print(i)\n",
    "\n",
    "# i.e. u have spark.read.csv option, spark.read.json option, spark.read.parquet option, etc ! \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# basic examples of structured data processing using Datasets\n",
    "\n",
    "# In Python, it’s possible to access a DataFrame’s columns either by attribute (df.age) or \n",
    "# by indexing (df['age']).   Its recommended  you use the second, but yes, the first is convenient, \n",
    "# we have all done it at some point ...\n",
    "\n",
    "# Print the schema in a tree format\n",
    "df.printSchema()\n",
    "\n",
    "# you are in the databases world now, think like that...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# so if you have ever messed with pandas dataframes, this is similiar, but it does \n",
    "# have a fair amount of differences, so think of this as a new construct...\n",
    "\n",
    "# Select only the \"supplier\" column\n",
    "df.select(\"supplier\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Select the quantity col but increment the 'quantity' by 1000\n",
    "df.select(df['quantity'] + 1000).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.select()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create a new df with certain cols only \n",
    "\n",
    "df2 = df.select(\"quantity\", \"supplier\")\n",
    "df2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# when you use SQL, you use commands like select * from <name>, its the same thing...\n",
    "\n",
    "df.select('*').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(type(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# note:  select is a transformation, not an action \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Select people older than 21\n",
    "df.filter(df['quantity'] > 250).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Count products by quantity\n",
    "df.groupBy(\"quantity\").count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The sql function on a SparkSession enables applications to run SQL queries \n",
    "# programmatically and returns the result as a DataFrame.\n",
    "# Register the DataFrame as a SQL temporary view\n",
    "df.createOrReplaceTempView(\"products\")\n",
    "\n",
    "sqlDF = spark.sql(\"SELECT * FROM products\")\n",
    "sqlDF.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # $example on:global_temp_view$\n",
    "# # Register the DataFrame as a global temporary view\n",
    "# df.createGlobalTempView(\"people\")\n",
    "\n",
    "# # Global temporary view is tied to a system preserved database `global_temp`\n",
    "# spark.sql(\"SELECT * FROM global_temp.people\").show()\n",
    "\n",
    "# # Global temporary view is cross-session\n",
    "# spark.newSession().sql(\"SELECT * FROM global_temp.people\").show()    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# $example on:schema_inferring$\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Load a text file and convert each line to a Row.\n",
    "lines = sc.textFile(\"C:/SPARK/examples/src/main/resources/people.txt\")\n",
    "parts = lines.map(lambda l: l.split(\",\"))\n",
    "people = parts.map(lambda p: Row(name=p[0], age=int(p[1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Infer the schema, and register the DataFrame as a table.\n",
    "schemaPeople = spark.createDataFrame(people)\n",
    "schemaPeople.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# SQL can be run over DataFrames that have been registered as a table\n",
    "teenagers = spark.sql(\"SELECT name FROM people WHERE age >= 13 AND age <= 19\")\n",
    "\n",
    "# The results of SQL queries are Dataframe objects...\n",
    "# rdd returns the content as an :class:`pyspark.RDD` of :class:`Row`.\n",
    "teenNames = teenagers.rdd.map(lambda p: \"Name: \" + p.name).collect()\n",
    "\n",
    "for name in teenNames:\n",
    "      print(name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "teenagers.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### quickref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "A simple example demonstrating basic Spark SQL features.\n",
    "Run with:\n",
    "  ./bin/spark-submit examples/src/main/python/sql/basic.py\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "# $example on:init_session$\n",
    "from pyspark.sql import SparkSession\n",
    "# $example off:init_session$\n",
    "\n",
    "# $example on:schema_inferring$\n",
    "from pyspark.sql import Row\n",
    "# $example off:schema_inferring$\n",
    "\n",
    "# $example on:programmatic_schema$\n",
    "# Import data types\n",
    "from pyspark.sql.types import *\n",
    "# $example off:programmatic_schema$\n",
    "\n",
    "\n",
    "def basic_df_example(spark):\n",
    "    # $example on:create_df$\n",
    "    # spark is an existing SparkSession\n",
    "    df = spark.read.json(\"examples/src/main/resources/people.json\")\n",
    "    # Displays the content of the DataFrame to stdout\n",
    "    df.show()\n",
    "    # +----+-------+\n",
    "    # | age|   name|\n",
    "    # +----+-------+\n",
    "    # |null|Michael|\n",
    "    # |  30|   Andy|\n",
    "    # |  19| Justin|\n",
    "    # +----+-------+\n",
    "    # $example off:create_df$\n",
    "\n",
    "    # $example on:untyped_ops$\n",
    "    # spark, df are from the previous example\n",
    "    # Print the schema in a tree format\n",
    "    df.printSchema()\n",
    "    # root\n",
    "    # |-- age: long (nullable = true)\n",
    "    # |-- name: string (nullable = true)\n",
    "\n",
    "    # Select only the \"name\" column\n",
    "    df.select(\"name\").show()\n",
    "    # +-------+\n",
    "    # |   name|\n",
    "    # +-------+\n",
    "    # |Michael|\n",
    "    # |   Andy|\n",
    "    # | Justin|\n",
    "    # +-------+\n",
    "\n",
    "    # Select everybody, but increment the age by 1\n",
    "    df.select(df['name'], df['age'] + 1).show()\n",
    "    # +-------+---------+\n",
    "    # |   name|(age + 1)|\n",
    "    # +-------+---------+\n",
    "    # |Michael|     null|\n",
    "    # |   Andy|       31|\n",
    "    # | Justin|       20|\n",
    "    # +-------+---------+\n",
    "\n",
    "    # Select people older than 21\n",
    "    df.filter(df['age'] > 21).show()\n",
    "    # +---+----+\n",
    "    # |age|name|\n",
    "    # +---+----+\n",
    "    # | 30|Andy|\n",
    "    # +---+----+\n",
    "\n",
    "    # Count people by age\n",
    "    df.groupBy(\"age\").count().show()\n",
    "    # +----+-----+\n",
    "    # | age|count|\n",
    "    # +----+-----+\n",
    "    # |  19|    1|\n",
    "    # |null|    1|\n",
    "    # |  30|    1|\n",
    "    # +----+-----+\n",
    "    # $example off:untyped_ops$\n",
    "\n",
    "    # $example on:run_sql$\n",
    "    # Register the DataFrame as a SQL temporary view\n",
    "    df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "    sqlDF = spark.sql(\"SELECT * FROM people\")\n",
    "    sqlDF.show()\n",
    "    # +----+-------+\n",
    "    # | age|   name|\n",
    "    # +----+-------+\n",
    "    # |null|Michael|\n",
    "    # |  30|   Andy|\n",
    "    # |  19| Justin|\n",
    "    # +----+-------+\n",
    "    # $example off:run_sql$\n",
    "\n",
    "    # $example on:global_temp_view$\n",
    "    # Register the DataFrame as a global temporary view\n",
    "    df.createGlobalTempView(\"people\")\n",
    "\n",
    "    # Global temporary view is tied to a system preserved database `global_temp`\n",
    "    spark.sql(\"SELECT * FROM global_temp.people\").show()\n",
    "    # +----+-------+\n",
    "    # | age|   name|\n",
    "    # +----+-------+\n",
    "    # |null|Michael|\n",
    "    # |  30|   Andy|\n",
    "    # |  19| Justin|\n",
    "    # +----+-------+\n",
    "\n",
    "    # Global temporary view is cross-session\n",
    "    spark.newSession().sql(\"SELECT * FROM global_temp.people\").show()\n",
    "    # +----+-------+\n",
    "    # | age|   name|\n",
    "    # +----+-------+\n",
    "    # |null|Michael|\n",
    "    # |  30|   Andy|\n",
    "    # |  19| Justin|\n",
    "    # +----+-------+\n",
    "    # $example off:global_temp_view$\n",
    "\n",
    "\n",
    "def schema_inference_example(spark):\n",
    "    # $example on:schema_inferring$\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "    # Load a text file and convert each line to a Row.\n",
    "    lines = sc.textFile(\"examples/src/main/resources/people.txt\")\n",
    "    parts = lines.map(lambda l: l.split(\",\"))\n",
    "    people = parts.map(lambda p: Row(name=p[0], age=int(p[1])))\n",
    "\n",
    "    # Infer the schema, and register the DataFrame as a table.\n",
    "    schemaPeople = spark.createDataFrame(people)\n",
    "    schemaPeople.createOrReplaceTempView(\"people\")\n",
    "\n",
    "    # SQL can be run over DataFrames that have been registered as a table.\n",
    "    teenagers = spark.sql(\"SELECT name FROM people WHERE age >= 13 AND age <= 19\")\n",
    "\n",
    "    # The results of SQL queries are Dataframe objects.\n",
    "    # rdd returns the content as an :class:`pyspark.RDD` of :class:`Row`.\n",
    "    teenNames = teenagers.rdd.map(lambda p: \"Name: \" + p.name).collect()\n",
    "    for name in teenNames:\n",
    "        print(name)\n",
    "    # Name: Justin\n",
    "    # $example off:schema_inferring$\n",
    "\n",
    "\n",
    "def programmatic_schema_example(spark):\n",
    "    # $example on:programmatic_schema$\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "    # Load a text file and convert each line to a Row.\n",
    "    lines = sc.textFile(\"examples/src/main/resources/people.txt\")\n",
    "    parts = lines.map(lambda l: l.split(\",\"))\n",
    "    # Each line is converted to a tuple.\n",
    "    people = parts.map(lambda p: (p[0], p[1].strip()))\n",
    "\n",
    "    # The schema is encoded in a string.\n",
    "    schemaString = \"name age\"\n",
    "\n",
    "    fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\n",
    "    schema = StructType(fields)\n",
    "\n",
    "    # Apply the schema to the RDD.\n",
    "    schemaPeople = spark.createDataFrame(people, schema)\n",
    "\n",
    "    # Creates a temporary view using the DataFrame\n",
    "    schemaPeople.createOrReplaceTempView(\"people\")\n",
    "\n",
    "    # SQL can be run over DataFrames that have been registered as a table.\n",
    "    results = spark.sql(\"SELECT name FROM people\")\n",
    "\n",
    "    results.show()\n",
    "    # +-------+\n",
    "    # |   name|\n",
    "    # +-------+\n",
    "    # |Michael|\n",
    "    # |   Andy|\n",
    "    # | Justin|\n",
    "    # +-------+\n",
    "    # $example off:programmatic_schema$\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # $example on:init_session$\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Python Spark SQL basic example\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n",
    "    # $example off:init_session$\n",
    "\n",
    "    basic_df_example(spark)\n",
    "    schema_inference_example(spark)\n",
    "    programmatic_schema_example(spark)\n",
    "\n",
    "    spark.stop()\n",
    "\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Spark Properties, the manual way, instead of having to go the UI ! \n",
    "\n",
    "print(\"sc.appName:\\t\\t\", sc.appName)\n",
    "\n",
    "print(\"sc.applicationId:\\t\", sc.applicationId)\n",
    "\n",
    "print(\"sc.master:\\t\\t\", sc.master)\n",
    "\n",
    "print(\"sc.appName:\\t\\t\", sc.appName)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Spark SQL can convert an RDD of Row objects to a DataFrame, inferring the datatypes. Rows are constructed by passing a list of key/value pairs as kwargs to the Row class. The keys of this list define the column names of the table, and the types are inferred by sampling the whole dataset, similar to the inference that is performed on JSON files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import Row\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Load a text file and convert each line to a Row.\n",
    "lines = sc.textFile(\"C:/SPARK/examples/src/main/resources/people.txt\")\n",
    "parts = lines.map(lambda l: l.split(\",\"))\n",
    "people = parts.map(lambda p: Row(name=p[0], age=int(p[1])))\n",
    "\n",
    "# Infer the schema, and register the DataFrame as a table.\n",
    "schemaPeople = spark.createDataFrame(people)\n",
    "schemaPeople.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# SQL can be run over DataFrames that have been registered as a table.\n",
    "teenagers = spark.sql(\"SELECT name FROM people WHERE age >= 13 AND age <= 19\")\n",
    "\n",
    "# The results of SQL queries are Dataframe objects.\n",
    "# rdd returns the content as an :class:`pyspark.RDD` of :class:`Row`.\n",
    "teenNames = teenagers.rdd.map(lambda p: \"Name: \" + p.name).collect()\n",
    "for name in teenNames:\n",
    "    print(name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL supports operating on a variety of data sources through the DataFrame interface. A DataFrame can be operated on using relational transformations and can also be used to create a temporary view. Registering a DataFrame as a temporary view allows you to run SQL queries over its data. This section describes the general methods for loading and saving data using the Spark Data Sources and then goes into specific options that are available for the built-in data sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "path = \"examples/src/main/resources/people.json\"\n",
    "peopleDF = spark.read.json(path)\n",
    "\n",
    "# The inferred schema can be visualized using the printSchema() method\n",
    "peopleDF.printSchema()\n",
    "# root\n",
    "#  |-- age: long (nullable = true)\n",
    "#  |-- name: string (nullable = true)\n",
    "\n",
    "# Creates a temporary view using the DataFrame\n",
    "peopleDF.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# SQL statements can be run by using the sql methods provided by spark\n",
    "teenagerNamesDF = spark.sql(\"SELECT name FROM people WHERE age BETWEEN 13 AND 19\")\n",
    "teenagerNamesDF.show()\n",
    "# +------+\n",
    "# |  name|\n",
    "# +------+\n",
    "# |Justin|\n",
    "# +------+\n",
    "\n",
    "# Alternatively, a DataFrame can be created for a JSON dataset represented by\n",
    "# an RDD[String] storing one JSON object per string\n",
    "jsonStrings = ['{\"name\":\"Yin\",\"address\":{\"city\":\"Columbus\",\"state\":\"Ohio\"}}']\n",
    "otherPeopleRDD = sc.parallelize(jsonStrings)\n",
    "otherPeople = spark.read.json(otherPeopleRDD)\n",
    "otherPeople.show()\n",
    "# +---------------+----+\n",
    "# |        address|name|\n",
    "# +---------------+----+\n",
    "# |[Columbus,Ohio]| Yin|\n",
    "# +---------------+----+\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df2 = spark.read.json(\"C:/SPARK/examples/test.json\")\n",
    "df2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.createOrReplaceTempView(\"names\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output = spark.sql(\"SELECT name FROM names WHERE number_of_files BETWEEN 100 AND 300\")\n",
    "output.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# # spark is an existing SparkSession\n",
    "# dfs = spark.read.json(\"C:/SPARK/examples/people.json\")\n",
    "\n",
    "# # Displays the content of the DataFrame to stdout\n",
    "# dfs.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# spark is an existing SparkSession\n",
    "dfs = spark.read.json(\"C:/SPARK/examples/example.json\")\n",
    "\n",
    "# Displays the content of the DataFrame to stdout\n",
    "dfs.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# spark is an existing SparkSession\n",
    "\n",
    "path = 'C:/SPARK/examples/src/main/resources/people.json'\n",
    "\n",
    "df = sqlSparkContext.read.json(path) \n",
    "\n",
    "# A JSON dataset is pointed to by path.\n",
    "# The path can be either a single text file or a directory storing text files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  https://spark.apache.org/docs/latest/sql-data-sources-json.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The inferred schema can be visualized using the printSchema() method\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(type(sqlSparkContext))\n",
    "print(type(sc))\n",
    "print(type(df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D.  &nbsp;  SQL go "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creates a temporary view using the DataFrame\n",
    "df.createOrReplaceTempView(\"people\")  # in my case would be df.createOrReplace\n",
    "\n",
    "# SQL statements can be run by using the sql methods provided by spark\n",
    "df_andy = sqlSparkContext.sql(\"SELECT name FROM people WHERE age BETWEEN 20 and 40\")\n",
    "df_andy.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_michael = sqlSparkContext.sql(\"SELECT name FROM people WHERE name = 'Michael'\")\n",
    "df_michael.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dir(sqlSparkContext)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#---  REFERENCE - READING OPTIONS  ---\n",
    "\n",
    "# dir(sqlSparkContext.read)\n",
    "\n",
    "#  'csv',\n",
    "#  'format',\n",
    "#  'jdbc',\n",
    "#  'json',\n",
    "#  'load',\n",
    "#  'option',\n",
    "#  'options',\n",
    "#  'orc',\n",
    "#  'parquet',\n",
    "#  'schema',\n",
    "#  'table',\n",
    "#  'text']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sqlSparkContext.tables()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sqlSparkContext.tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dir(sqlSparkContext.sql)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E.  &nbsp; Go Off "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_t = sqlSparkContext.read.load(\"C:/SPARK/examples/src/main/resources/users.parquet\")\n",
    "\n",
    "print(type(df_t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#---  REFERENCE - your option methods for this DF concept  ---\n",
    "\n",
    "for i in dir(df_t):\n",
    "    if not i.startswith(\"_\"):\n",
    "        print(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A DataFrame is a distributed collection of data, which is organized into named columns. Conceptually, it is equivalent to relational tables with good optimization techniques.\n",
    "\n",
    "A DataFrame can be constructed from an array of different sources such as Hive tables, Structured Data files, external databases, or existing RDDs. This API was designed for modern Big Data and data science applications taking inspiration from DataFrame in R Programming and Pandas in Python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "Features of DataFrame\n",
    "\n",
    "Here is a set of few characteristic features of DataFrame\n",
    "\n",
    "Ability to process the data in the size of Kilobytes to Petabytes on a single node cluster to large cluster.\n",
    "\n",
    "Supports different data formats (Avro, csv, elastic search, and Cassandra) and storage systems (HDFS, HIVE tables, mysql, etc).\n",
    "\n",
    "State of art optimization and code generation through the Spark SQL Catalyst optimizer (tree transformation framework).\n",
    "\n",
    "Can be easily integrated with all Big Data tools and frameworks via Spark-Core.\n",
    "\n",
    "Provides API for Python, Java, Scala, and R Programming.\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Spark introduces a programming module for structured data processing called Spark SQL. It provides a programming abstraction called DataFrame and can act as distributed SQL query engine.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "The following command is used for initializing the SparkContext through spark-shell.\n",
    "\n",
    "$ spark-shell\n",
    "By default, the SparkContext object is initialized with the name sc when the spark-shell starts.\n",
    "\n",
    "Use the following command to create SQLContext.\n",
    "\n",
    "scala> val sqlcontext = new org.apache.spark.sql.SQLContext(sc)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* some data files \n",
    "   *  https://github.com/apache/spark/tree/master/examples/src/main/resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "michael = sqlSparkContext.sql(\"SELECT name FROM people WHERE name = 'Michael'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---  REFERENCE - list out the dataframes currently created ! --- \n",
    "\n",
    "def list_dataframes():\n",
    "    from pyspark.sql import DataFrame\n",
    "    return [k for (k, v) in globals().items() if isinstance(v, DataFrame)]\n",
    "\n",
    "list_dataframes()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "path = 'C:\\SPARK\\examples\\src\\main\\python\\employees.json'\n",
    "\n",
    "df_emp = sqlSparkContext.read.json(path)    \n",
    "\n",
    "df_emp\n",
    "df_emp.show()\n",
    "\n",
    "# scala> val dfs = sqlContext.read.json(\"employee.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# how many DF do i have right now ? \n",
    "print(list_dataframes())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_emp.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# direct call ! ! ! \n",
    "\n",
    "df_emp.select(\"name\").show()\n",
    "\n",
    "df_emp.select(\"salary\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# group by salary \n",
    "df_emp.groupBy(\"salary\").count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_emp.filter(df_emp[\"salary\"] > 3200).show()\n",
    "# interesting:  scala uses () but python uses []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# peopleDF = spark.read.json(\"examples/src/main/resources/people.json\")\n",
    "\n",
    "# # DataFrames can be saved as Parquet files, maintaining the schema information.\n",
    "# peopleDF.write.parquet(\"people.parquet\")\n",
    "\n",
    "# # Read in the Parquet file created above.\n",
    "# # Parquet files are self-describing so the schema is preserved.\n",
    "# # The result of loading a parquet file is also a DataFrame.\n",
    "# parquetFile = spark.read.parquet(\"people.parquet\")\n",
    "\n",
    "# # Parquet files can also be used to create a temporary view and then used in SQL statements.\n",
    "# parquetFile.createOrReplaceTempView(\"parquetFile\")\n",
    "# teenagers = spark.sql(\"SELECT name FROM parquetFile WHERE age >= 13 AND age <= 19\")\n",
    "# teenagers.show()\n",
    "# # +------+\n",
    "# # |  name|\n",
    "# # +------+\n",
    "# # |Justin|\n",
    "# # +------+\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_emp.write.parquet(\"C:/SPARK/examples/src/main/resources/tom_emp.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F.  &nbsp;  Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Parquet is a columnar format, supported by many data processing systems. The advantages of having a columnar storage are as follows −\n",
    "\n",
    "Columnar storage limits IO operations.\n",
    "\n",
    "Columnar storage can fetch specific columns that you need to access.\n",
    "\n",
    "Columnar storage consumes less space.\n",
    "\n",
    "Columnar storage gives better-summarized data and follows type-specific encoding.\n",
    "\n",
    "Spark SQL provides support for both reading and writing parquet files that automatically capture the schema of the original data. Like JSON datasets, parquet files follow the same procedure.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "# $example on:schema_merging$\n",
    "from pyspark.sql import Row\n",
    "# $example off:schema_merging$\n",
    "\n",
    "# $example on:generic_load_save_functions$\n",
    "df_users = sqlSparkContext.read.load(\"C:/SPARK/examples/src/main/resources/users.parquet\")\n",
    "# df.select(\"name\", \"favorite_color\").write.save(\"namesAndFavColors.parquet\")\n",
    "# $example off:generic_load_save_functions$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "peopleDF = spark.read.json(\"examples/src/main/resources/people.json\")\n",
    "\n",
    "# DataFrames can be saved as Parquet files, maintaining the schema information.\n",
    "peopleDF.write.parquet(\"people.parquet\")\n",
    "\n",
    "# Read in the Parquet file created above.\n",
    "# Parquet files are self-describing so the schema is preserved.\n",
    "# The result of loading a parquet file is also a DataFrame.\n",
    "parquetFile = spark.read.parquet(\"people.parquet\")\n",
    "\n",
    "# Parquet files can also be used to create a temporary view and then used in SQL statements.\n",
    "parquetFile.createOrReplaceTempView(\"parquetFile\")\n",
    "teenagers = spark.sql(\"SELECT name FROM parquetFile WHERE age >= 13 AND age <= 19\")\n",
    "teenagers.show()\n",
    "# +------+\n",
    "# |  name|\n",
    "# +------+\n",
    "# |Justin|\n",
    "# +------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from os import walk\n",
    "# from pyspark.sql import SQLContext\n",
    "\n",
    "# sc = SparkContext.getOrCreate()\n",
    "# sqlContext = SQLContext(sc)\n",
    "\n",
    "# parquetdir = r'C:\\PATH\\TO\\YOUR\\PARQUET\\FILES'\n",
    "\n",
    "# # Getting all parquet files in a dir as spark contexts.\n",
    "# # There might be more easy ways to access single parquets, but I had nested dirs\n",
    "# dirpath, dirnames, filenames = next(walk(parquetdir), (None, [], []))\n",
    "\n",
    "# # for each parquet file, i.e. table in our database, spark creates a tempview with\n",
    "# # the respective table name equal the parquet filename\n",
    "# print('New tables available: \\n')\n",
    "\n",
    "# for parquet in filenames:\n",
    "#     print(parquet[:-8])\n",
    "#     spark.read.parquet(parquetdir+'\\\\'+parquet).createOrReplaceTempView(parquet[:-8])\n",
    "    \n",
    "# my_test_query = spark.sql(\"\"\"\n",
    "# select\n",
    "#   field1,\n",
    "#   field2\n",
    "# from parquetfilename1\n",
    "# where\n",
    "#   field1 = 'something'\n",
    "# \"\"\")\n",
    "\n",
    "# my_test_query.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "squaresDF = sqlSparkContext.createDataFrame(sc.parallelize(range(1, 6)).map(lambda i: Row(single=i, double=i ** 2)))\n",
    "\n",
    "squaresDF.write.parquet(\"data/test_table/key=1\")\n",
    "\n",
    "# Create another DataFrame in a new partition directory,\n",
    "# adding a new column and dropping an existing column\n",
    "cubesDF = sqlSparkContext.createDataFrame(sc.parallelize(range(6, 11)).map(lambda i: Row(single=i, triple=i ** 3)))\n",
    "\n",
    "cubesDF.write.parquet(\"C:/SPARK/key=2\")\n",
    "\n",
    "# Read the partitioned table\n",
    "mergedDF = sqlSparkContext.read.option(\"mergeSchema\", \"true\").parquet(\"C:/SPARKata/test_table\")\n",
    "mergedDF.printSchema()\n",
    "\n",
    "# The final schema consists of all 3 columns in the Parquet files together\n",
    "# with the partitioning column appeared in the partition directory paths.\n",
    "# root\n",
    "#  |-- double: long (nullable = true)\n",
    "#  |-- single: long (nullable = true)\n",
    "#  |-- triple: long (nullable = true)\n",
    "#  |-- key: integer (nullable = true)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# .catalog.listTables()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # raw_data_RDD = sc.textFile(\"e://README_spark.md\")  \n",
    "\n",
    "# lines = sc.read.text(\"e://README_spark.md\").rdd.map(lambda r: r[0])\n",
    "\n",
    "# sortedCount = lines.flatMap(lambda x: x.split(' ')).map(lambda x: (int(x), 1)).sortByKey()\n",
    "\n",
    "# # This is just a demo on how to bring all the sorted data back to a single node.\n",
    "\n",
    "# # In reality, we wouldn't want to collect all the data to the driver node.\n",
    "\n",
    "# output = sortedCount.collect()\n",
    "\n",
    "# for (num, unitcount) in output:\n",
    "#         print(num)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# scala \n",
    "\n",
    "# spark: SparkSession = // create the Spark Session\n",
    "# val df = spark.read.csv(\"file.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#\n",
    "# Licensed to the Apache Software Foundation (ASF) under one or more\n",
    "# contributor license agreements.  See the NOTICE file distributed with\n",
    "# this work for additional information regarding copyright ownership.\n",
    "# The ASF licenses this file to You under the Apache License, Version 2.0\n",
    "# (the \"License\"); you may not use this file except in compliance with\n",
    "# the License.  You may obtain a copy of the License at\n",
    "#\n",
    "#    http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#\n",
    "\n",
    "\"\"\"\n",
    "A simple example demonstrating basic Spark SQL features.\n",
    "Run with:\n",
    "  ./bin/spark-submit examples/src/main/python/sql/basic.py\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "# $example on:init_session$\n",
    "from pyspark.sql import SparkSession\n",
    "# $example off:init_session$\n",
    "\n",
    "# $example on:schema_inferring$\n",
    "from pyspark.sql import Row\n",
    "# $example off:schema_inferring$\n",
    "\n",
    "# $example on:programmatic_schema$\n",
    "# Import data types\n",
    "from pyspark.sql.types import *\n",
    "# $example off:programmatic_schema$\n",
    "\n",
    "\n",
    "def basic_df_example(spark):\n",
    "    # $example on:create_df$\n",
    "    # spark is an existing SparkSession\n",
    "    df = spark.read.json(\"examples/src/main/resources/people.json\")\n",
    "    # Displays the content of the DataFrame to stdout\n",
    "    df.show()\n",
    "    # +----+-------+\n",
    "    # | age|   name|\n",
    "    # +----+-------+\n",
    "    # |null|Michael|\n",
    "    # |  30|   Andy|\n",
    "    # |  19| Justin|\n",
    "    # +----+-------+\n",
    "    # $example off:create_df$\n",
    "\n",
    "    # $example on:untyped_ops$\n",
    "    # spark, df are from the previous example\n",
    "    # Print the schema in a tree format\n",
    "    df.printSchema()\n",
    "    # root\n",
    "    # |-- age: long (nullable = true)\n",
    "    # |-- name: string (nullable = true)\n",
    "\n",
    "    # Select only the \"name\" column\n",
    "    df.select(\"name\").show()\n",
    "    # +-------+\n",
    "    # |   name|\n",
    "    # +-------+\n",
    "    # |Michael|\n",
    "    # |   Andy|\n",
    "    # | Justin|\n",
    "    # +-------+\n",
    "\n",
    "    # Select everybody, but increment the age by 1\n",
    "    df.select(df['name'], df['age'] + 1).show()\n",
    "    # +-------+---------+\n",
    "    # |   name|(age + 1)|\n",
    "    # +-------+---------+\n",
    "    # |Michael|     null|\n",
    "    # |   Andy|       31|\n",
    "    # | Justin|       20|\n",
    "    # +-------+---------+\n",
    "\n",
    "    # Select people older than 21\n",
    "    df.filter(df['age'] > 21).show()\n",
    "    # +---+----+\n",
    "    # |age|name|\n",
    "    # +---+----+\n",
    "    # | 30|Andy|\n",
    "    # +---+----+\n",
    "\n",
    "    # Count people by age\n",
    "    df.groupBy(\"age\").count().show()\n",
    "    # +----+-----+\n",
    "    # | age|count|\n",
    "    # +----+-----+\n",
    "    # |  19|    1|\n",
    "    # |null|    1|\n",
    "    # |  30|    1|\n",
    "    # +----+-----+\n",
    "    # $example off:untyped_ops$\n",
    "\n",
    "    # $example on:run_sql$\n",
    "    # Register the DataFrame as a SQL temporary view\n",
    "    df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "    sqlDF = spark.sql(\"SELECT * FROM people\")\n",
    "    sqlDF.show()\n",
    "    # +----+-------+\n",
    "    # | age|   name|\n",
    "    # +----+-------+\n",
    "    # |null|Michael|\n",
    "    # |  30|   Andy|\n",
    "    # |  19| Justin|\n",
    "    # +----+-------+\n",
    "    # $example off:run_sql$\n",
    "\n",
    "    # $example on:global_temp_view$\n",
    "    # Register the DataFrame as a global temporary view\n",
    "    df.createGlobalTempView(\"people\")\n",
    "\n",
    "    # Global temporary view is tied to a system preserved database `global_temp`\n",
    "    spark.sql(\"SELECT * FROM global_temp.people\").show()\n",
    "    # +----+-------+\n",
    "    # | age|   name|\n",
    "    # +----+-------+\n",
    "    # |null|Michael|\n",
    "    # |  30|   Andy|\n",
    "    # |  19| Justin|\n",
    "    # +----+-------+\n",
    "\n",
    "    # Global temporary view is cross-session\n",
    "    spark.newSession().sql(\"SELECT * FROM global_temp.people\").show()\n",
    "    # +----+-------+\n",
    "    # | age|   name|\n",
    "    # +----+-------+\n",
    "    # |null|Michael|\n",
    "    # |  30|   Andy|\n",
    "    # |  19| Justin|\n",
    "    # +----+-------+\n",
    "    # $example off:global_temp_view$\n",
    "\n",
    "\n",
    "def schema_inference_example(spark):\n",
    "    # $example on:schema_inferring$\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "    # Load a text file and convert each line to a Row.\n",
    "    lines = sc.textFile(\"examples/src/main/resources/people.txt\")\n",
    "    parts = lines.map(lambda l: l.split(\",\"))\n",
    "    people = parts.map(lambda p: Row(name=p[0], age=int(p[1])))\n",
    "\n",
    "    # Infer the schema, and register the DataFrame as a table.\n",
    "    schemaPeople = spark.createDataFrame(people)\n",
    "    schemaPeople.createOrReplaceTempView(\"people\")\n",
    "\n",
    "    # SQL can be run over DataFrames that have been registered as a table.\n",
    "    teenagers = spark.sql(\"SELECT name FROM people WHERE age >= 13 AND age <= 19\")\n",
    "\n",
    "    # The results of SQL queries are Dataframe objects.\n",
    "    # rdd returns the content as an :class:`pyspark.RDD` of :class:`Row`.\n",
    "    teenNames = teenagers.rdd.map(lambda p: \"Name: \" + p.name).collect()\n",
    "    for name in teenNames:\n",
    "        print(name)\n",
    "    # Name: Justin\n",
    "    # $example off:schema_inferring$\n",
    "\n",
    "\n",
    "def programmatic_schema_example(spark):\n",
    "    # $example on:programmatic_schema$\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "    # Load a text file and convert each line to a Row.\n",
    "    lines = sc.textFile(\"examples/src/main/resources/people.txt\")\n",
    "    parts = lines.map(lambda l: l.split(\",\"))\n",
    "    # Each line is converted to a tuple.\n",
    "    people = parts.map(lambda p: (p[0], p[1].strip()))\n",
    "\n",
    "    # The schema is encoded in a string.\n",
    "    schemaString = \"name age\"\n",
    "\n",
    "    fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\n",
    "    schema = StructType(fields)\n",
    "\n",
    "    # Apply the schema to the RDD.\n",
    "    schemaPeople = spark.createDataFrame(people, schema)\n",
    "\n",
    "    # Creates a temporary view using the DataFrame\n",
    "    schemaPeople.createOrReplaceTempView(\"people\")\n",
    "\n",
    "    # SQL can be run over DataFrames that have been registered as a table.\n",
    "    results = spark.sql(\"SELECT name FROM people\")\n",
    "\n",
    "    results.show()\n",
    "    # +-------+\n",
    "    # |   name|\n",
    "    # +-------+\n",
    "    # |Michael|\n",
    "    # |   Andy|\n",
    "    # | Justin|\n",
    "    # +-------+\n",
    "    # $example off:programmatic_schema$\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # $example on:init_session$\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Python Spark SQL basic example\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n",
    "    # $example off:init_session$\n",
    "\n",
    "    basic_df_example(spark)\n",
    "    schema_inference_example(spark)\n",
    "    programmatic_schema_example(spark)\n",
    "\n",
    "    spark.stop()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# $example on:init_session$\n",
    "from pyspark.sql import SparkSession\n",
    "# $example off:init_session$\n",
    "\n",
    "# $example on:schema_inferring$\n",
    "from pyspark.sql import Row\n",
    "# $example off:schema_inferring$\n",
    "\n",
    "# $example on:programmatic_schema$\n",
    "# Import data types\n",
    "from pyspark.sql.types import *\n",
    "# $example off:programmatic_schema$\n",
    "\n",
    "# READ IN JSON FILE ! \n",
    "df = sqlContext.read.json(\"C:/SPARK/examples/src/main/resources/people.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in dir(df):\n",
    "    if not i.startswith(\"_\"):\n",
    "        print(i)\n",
    "        \n",
    "# agg\n",
    "# alias\n",
    "# approxQuantile\n",
    "# cache\n",
    "# checkpoint\n",
    "# coalesce\n",
    "# colRegex\n",
    "# collect\n",
    "# columns\n",
    "# corr\n",
    "# count\n",
    "# cov\n",
    "# createGlobalTempView\n",
    "# createOrReplaceGlobalTempView\n",
    "# createOrReplaceTempView\n",
    "# createTempView\n",
    "# crossJoin\n",
    "# crosstab\n",
    "# cube\n",
    "# describe\n",
    "# distinct\n",
    "# drop\n",
    "# dropDuplicates\n",
    "# drop_duplicates\n",
    "# dropna\n",
    "# dtypes\n",
    "# exceptAll\n",
    "# explain\n",
    "# fillna\n",
    "# filter\n",
    "# first\n",
    "# foreach\n",
    "# foreachPartition\n",
    "# freqItems\n",
    "# groupBy\n",
    "# groupby\n",
    "# head\n",
    "# hint\n",
    "# intersect\n",
    "# intersectAll\n",
    "# isLocal\n",
    "# isStreaming\n",
    "# is_cached\n",
    "# join\n",
    "# limit\n",
    "# localCheckpoint\n",
    "# na\n",
    "# orderBy\n",
    "# persist\n",
    "# printSchema\n",
    "# randomSplit\n",
    "# rdd\n",
    "# registerTempTable\n",
    "# repartition\n",
    "# repartitionByRange\n",
    "# replace\n",
    "# rollup\n",
    "# sample\n",
    "# sampleBy\n",
    "# schema\n",
    "# select\n",
    "# selectExpr\n",
    "# show\n",
    "# sort\n",
    "# sortWithinPartitions\n",
    "# sql_ctx\n",
    "# stat\n",
    "# storageLevel\n",
    "# subtract\n",
    "# summary\n",
    "# take\n",
    "# toDF\n",
    "# toJSON\n",
    "# toLocalIterator\n",
    "# toPandas\n",
    "# union\n",
    "# unionAll\n",
    "# unionByName\n",
    "# unpersist\n",
    "# where\n",
    "# withColumn\n",
    "# withColumnRenamed\n",
    "# withWatermark\n",
    "# write\n",
    "# writeStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  people.json\n",
    "\n",
    "# {\"name\":\"Michael\"}\n",
    "# {\"name\":\"Andy\", \"age\":30}\n",
    "# {\"name\":\"Justin\", \"age\":19}\n",
    "\n",
    "# Displays the content of the DataFrame to stdout\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print the schema in a tree format\n",
    "df.printSchema()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Select only the \"name\" column\n",
    "df.select(\"name\").show()  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Select everybody, but increment the age by 1\n",
    "df.select(df['name'], df['age'] + 1).show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Select people older than 21\n",
    "\n",
    "df.filter(df['age'] > 21).show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Count people by age\n",
    "df.groupBy(\"age\").count().show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# $example on:run_sql$\n",
    "# Register the DataFrame as a SQL temporary view\n",
    "\n",
    "# Register the DataFrame as a SQL temporary view\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "sqlDF = sqlContext.sql(\"SELECT * FROM people\")\n",
    "sqlDF.show()  \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# $example on:global_temp_view$\n",
    "# Register the DataFrame as a global temporary view\n",
    "df.createGlobalTempView(\"people\")\n",
    "\n",
    "# Global temporary view is tied to a system preserved database `global_temp`\n",
    "sqlContext.sql(\"SELECT * FROM global_temp.people\").show()    \n",
    "\n",
    "# error =  AnalysisException: \"Temporary view 'people' already exists;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Global temporary view is cross-session\n",
    "\n",
    "sqlContext.newSession().sql(\"SELECT * FROM global_temp.people\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## datasource.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> PARQUET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "# $example on:schema_merging$\n",
    "from pyspark.sql import Row\n",
    "# $example off:schema_merging$\n",
    "\n",
    "# $example on:generic_load_save_functions$\n",
    "df = sqlContext.read.load(\"C:/SPARK/examples/src/main/resources/users.parquet\")\n",
    "# df.select(\"name\", \"favorite_color\").write.save(\"namesAndFavColors.parquet\")\n",
    "# $example off:generic_load_save_functions$\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# < i n s r t   -   image as you do it >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parquet is a columnar format, supported by many data processing systems. The advantages of having a columnar storage are as follows −\n",
    "\n",
    "Columnar storage limits IO operations.\n",
    "\n",
    "Columnar storage can fetch specific columns that you need to access.\n",
    "\n",
    "Columnar storage consumes less space.\n",
    "\n",
    "Columnar storage gives better-summarized data and follows type-specific encoding.\n",
    "\n",
    "Spark SQL provides support for both reading and writing parquet files that automatically capture the schema of the original data. Like JSON datasets, parquet files follow the same procedure.\n",
    "\n",
    "Let’s take another look at the same example of employee record data named employee.parquet placed in the same directory where spark-shell is running.\n",
    "\n",
    "Given data − Do not bother about converting the input data of employee records into parquet format. We use the following commands that convert the RDD data into Parquet file. Place the employee.json document, which we have used as the input file in our previous examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sqlContext' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-cfcf947761ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# $example on:generic_load_save_functions$\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"E:/userdata1.parquet\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'sqlContext' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# $example on:generic_load_save_functions$\n",
    "df2 = sqlContext.read.load(\"E:/userdata1.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "userdata[1-5].parquet: These are sample files containing data in PARQUET format.\n",
    "\n",
    "-> Number of rows in each file: 1000\n",
    "-> Column details:\n",
    "column#\t\tcolumn_name\t\thive_datatype\n",
    "=====================================================\n",
    "1\t\tregistration_dttm \ttimestamp\n",
    "2\t\tid \t\t\tint\n",
    "3\t\tfirst_name \t\tstring\n",
    "4\t\tlast_name \t\tstring\n",
    "5\t\temail \t\t\tstring\n",
    "6\t\tgender \t\t\tstring\n",
    "7\t\tip_address \t\tstring\n",
    "8\t\tcc \t\t\tstring\n",
    "9\t\tcountry \t\tstring\n",
    "10\t\tbirthdate \t\tstring\n",
    "11\t\tsalary \t\t\tdouble\n",
    "12\t\ttitle \t\t\tstring\n",
    "13\t\tcomments \t\tstring\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sqlDF = sqlContext.sql(\"SELECT * FROM people\")\n",
    "\n",
    "all_info_in_id_column = df2.select(\"id\").show()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "first_name = df2.select(\"first_name\").show()   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Reading VCF files in SparkSQL / Python*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> i don't know how to say this.  This will look like giberish until you dive into what the terms mean etc, and even then it takes a while..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Deep Dive into processing .vcf files "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The output we are dealing with is effectively a dict, so even if you dont understand the gene terminology, its just about understanding we are querying the file for certain 'keys'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# READ:  for the love of all holy don't save this to your laptop and associated it with a vCalendar File for outlook or something stupid like that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-allel is a Python package intended to enable exploratory analysis of large-scale genetic variation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variant Call Format (VCF) is a text file format for storing marker and genotype data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  not like you necessarily care, but this is about AGTCs, i.e. bases Adenine, Guanine, Thymine, and Cytosine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://www.genome.gov/sites/default/files/tg/en/illustration/acgt.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import scikit-allel, a sci file \n",
    "import allel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(allel.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### sample - vcf file below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "THIS IS A RANDOM SAMPLE.VCF EXAMPLE: \n",
    "    \n",
    " -  the file begins with meta-info lines with ## \n",
    "    \n",
    "\n",
    "##fileformat=VCFv4.0\n",
    "##fileDate=20090805\n",
    "##source=myImputationProgramV3.1\n",
    "##reference=1000GenomesPilot-NCBI36\n",
    "##phasing=partial\n",
    "##INFO=<ID=NS,Number=1,Type=Integer,Description=\"Number of Samples With Data\">\n",
    "##INFO=<ID=AN,Number=1,Type=Integer,Description=\"Total number of alleles in called genotypes\">\n",
    "##INFO=<ID=AC,Number=.,Type=Integer,Description=\"Allele count in genotypes, for each ALT allele, in the same order as listed\">\n",
    "##INFO=<ID=DP,Number=1,Type=Integer,Description=\"Total Depth\">\n",
    "##INFO=<ID=AF,Number=.,Type=Float,Description=\"Allele Frequency\">\n",
    "##INFO=<ID=AA,Number=1,Type=String,Description=\"Ancestral Allele\">\n",
    "##INFO=<ID=DB,Number=0,Type=Flag,Description=\"dbSNP membership, build 129\">\n",
    "##INFO=<ID=H2,Number=0,Type=Flag,Description=\"HapMap2 membership\">\n",
    "##FILTER=<ID=q10,Description=\"Quality below 10\">\n",
    "##FILTER=<ID=s50,Description=\"Less than 50% of samples have data\">\n",
    "##FORMAT=<ID=GT,Number=1,Type=String,Description=\"Genotype\">\n",
    "##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\"Genotype Quality\">\n",
    "##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\"Read Depth\">\n",
    "##FORMAT=<ID=HQ,Number=2,Type=Integer,Description=\"Haplotype Quality\">\n",
    "##ALT=<ID=DEL:ME:ALU,Description=\"Deletion of ALU element\">\n",
    "##ALT=<ID=CNV,Description=\"Copy number variable region\">       < - - im inserting a space for clarity \n",
    "\n",
    "\n",
    "#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO\tFORMAT\tNA00001\tNA00002\tNA00003\n",
    "\n",
    "19\t111\t.\tA\tC\t9.6\t.\t.\tGT:HQ\t0|0:10,10\t0|0:10,10\t0/1:3,3\n",
    "19\t112\t.\tA\tG\t10\t.\t.\tGT:HQ\t0|0:10,10\t0|0:10,10\t0/1:3,3\n",
    "20\t14370\trs6054257\tG\tA\t29\tPASS\tNS=3;DP=14;AF=0.5;DB;H2\tGT:GQ:DP:HQ\t0|0:48:1:51,51\t1|0:48:8:51,51\t1/1:43:5:.,.\n",
    "20\t17330\t.\tT\tA\t3\tq10\tNS=3;DP=11;AF=0.017\tGT:GQ:DP:HQ\t0|0:49:3:58,50\t0|1:3:5:65,3\t0/0:41:3:.,.\n",
    "20\t1110696\trs6040355\tA\tG,T\t67\tPASS\tNS=2;DP=10;AF=0.333,0.667;AA=T;DB\tGT:GQ:DP:HQ\t1|2:21:6:23,27\t2|1:2:0:18,2\t2/2:35:4:.,.\n",
    "20\t1230237\t.\tT\t.\t47\tPASS\tNS=3;DP=13;AA=T\tGT:GQ:DP:HQ\t0|0:54:.:56,60\t0|0:48:4:51,51\t0/0:61:2:.,.\n",
    "20\t1234567\tmicrosat1\tG\tGA,GAC\t50\tPASS\tNS=3;DP=9;AA=G;AN=6;AC=3,1\tGT:GQ:DP\t0/1:.:4\t0/2:17:2\t1/1:40:3\n",
    "20\t1235237\t.\tT\t.\t.\t.\t.\tGT\t0/0\t0|0\t./.\n",
    "X\t10\trsTest\tAC\tA,ATG\t10\tPASS\t.\tGT\t0\t0/1\t0|2\n",
    "\n",
    "\n",
    "\n",
    "FYI, this is the header:\n",
    "#CHROM    POS    ID    REF    ALT    QUAL    FILTER    INFO    FORMAT    NA00001    NA00002    NA00003\n",
    "\n",
    "\n",
    "So what is this ? \n",
    "After the header, there are DATA LINES, with each data line describing a genetic variant at a particular \n",
    "position relative to the reference genome of whichever species you are studying. \n",
    "\n",
    "In my case:\n",
    "CHROM\t\n",
    "POS\t\n",
    "ID\t\n",
    "REF\t\n",
    "ALT\t\n",
    "QUAL\n",
    "FILTER\n",
    "INFO\n",
    "FORMAT\n",
    "NA00001\n",
    "NA00002\n",
    "NA00003\n",
    "\n",
    "Data lines contain marker and genotype data (one variant per line). A data line is called a VCF record.\n",
    "\n",
    "My first line describes a variant on chromosome **19** at position **11** relative to the to ____\n",
    "assembly of the human genome. The reference allele is ‘C’ and the alternate allele is ‘A’, so this etc etc \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "CHROM\t\n",
    "the chromosome.\n",
    "\n",
    "\n",
    "POS\t\n",
    "the genome coordinate of the first base in the variant. Within a chromosome, VCF records are sorted in order of increasing position.\n",
    "\n",
    "ID\t\n",
    "a semicolon-separated list of marker identifiers.\n",
    "\n",
    "REF\t\n",
    "the reference allele expressed as a sequence of one or more A/C/G/T nucleotides (e.g. \"A\" or \"AAC\")\n",
    "\n",
    "ALT\t\n",
    "the alternate allele expressed as a sequence of one or more A/C/G/T nucleotides (e.g. \"A\" or \"AAC\"). If there is more than one alternate alleles, the field should be a comma-separated list of alternate alleles.\n",
    "\n",
    "QUAL\t\n",
    "probability that the ALT allele is incorrectly specified, expressed on the the phred scale (-10log10(probability)).\n",
    "\n",
    "FILTER\t\n",
    "Either \"PASS\" or a semicolon-separated list of failed quality control filters.\n",
    "\n",
    "INFO\t\n",
    "additional information (no white space, tabs, or semi-colons permitted).\n",
    "\n",
    "FORMAT\t\n",
    "colon-separated list of data subfields reported for each sample. The format fields in the Example are explained below.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##fileformat=VCFv4.0\n",
      "##fileDate=20090805\n",
      "##source=myImputationProgramV3.1\n",
      "##reference=1000GenomesPilot-NCBI36\n",
      "##phasing=partial\n",
      "##INFO=<ID=NS,Number=1,Type=Integer,Description=\"Number of Samples With Data\">\n",
      "##INFO=<ID=AN,Number=1,Type=Integer,Description=\"Total number of alleles in called genotypes\">\n",
      "##INFO=<ID=AC,Number=.,Type=Integer,Description=\"Allele count in genotypes, for each ALT allele, in the same order as listed\">\n",
      "##INFO=<ID=DP,Number=1,Type=Integer,Description=\"Total Depth\">\n",
      "##INFO=<ID=AF,Number=.,Type=Float,Description=\"Allele Frequency\">\n",
      "##INFO=<ID=AA,Number=1,Type=String,Description=\"Ancestral Allele\">\n",
      "##INFO=<ID=DB,Number=0,Type=Flag,Description=\"dbSNP membership, build 129\">\n",
      "##INFO=<ID=H2,Number=0,Type=Flag,Description=\"HapMap2 membership\">\n",
      "##FILTER=<ID=q10,Description=\"Quality below 10\">\n",
      "##FILTER=<ID=s50,Description=\"Less than 50% of samples have data\">\n",
      "##FORMAT=<ID=GT,Number=1,Type=String,Description=\"Genotype\">\n",
      "##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\"Genotype Quality\">\n",
      "##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\"Read Depth\">\n",
      "##FORMAT=<ID=HQ,Number=2,Type=Integer,Description=\"Haplotype Quality\">\n",
      "##ALT=<ID=DEL:ME:ALU,Description=\"Deletion of ALU element\">\n",
      "##ALT=<ID=CNV,Description=\"Copy number variable region\">\n",
      "#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO\tFORMAT\tNA00001\tNA00002\tNA00003\n",
      "19\t111\t.\tA\tC\t9.6\t.\t.\tGT:HQ\t0|0:10,10\t0|0:10,10\t0/1:3,3\n",
      "19\t112\t.\tA\tG\t10\t.\t.\tGT:HQ\t0|0:10,10\t0|0:10,10\t0/1:3,3\n",
      "20\t14370\trs6054257\tG\tA\t29\tPASS\tNS=3;DP=14;AF=0.5;DB;H2\tGT:GQ:DP:HQ\t0|0:48:1:51,51\t1|0:48:8:51,51\t1/1:43:5:.,.\n",
      "20\t17330\t.\tT\tA\t3\tq10\tNS=3;DP=11;AF=0.017\tGT:GQ:DP:HQ\t0|0:49:3:58,50\t0|1:3:5:65,3\t0/0:41:3:.,.\n",
      "20\t1110696\trs6040355\tA\tG,T\t67\tPASS\tNS=2;DP=10;AF=0.333,0.667;AA=T;DB\tGT:GQ:DP:HQ\t1|2:21:6:23,27\t2|1:2:0:18,2\t2/2:35:4:.,.\n",
      "20\t1230237\t.\tT\t.\t47\tPASS\tNS=3;DP=13;AA=T\tGT:GQ:DP:HQ\t0|0:54:.:56,60\t0|0:48:4:51,51\t0/0:61:2:.,.\n",
      "20\t1234567\tmicrosat1\tG\tGA,GAC\t50\tPASS\tNS=3;DP=9;AA=G;AN=6;AC=3,1\tGT:GQ:DP\t0/1:.:4\t0/2:17:2\t1/1:40:3\n",
      "20\t1235237\t.\tT\t.\t.\t.\t.\tGT\t0/0\t0|0\t./.\n",
      "X\t10\trsTest\tAC\tA,ATG\t10\tPASS\t.\tGT\t0\t0/1\t0|2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# very handy way of printing out the actual .vcf file as a way of getting an idea how this stuff works\n",
    "\n",
    "with open('C:/SPARK/sample.vcf', mode='r') as vcf:\n",
    "    print(vcf.read())\n",
    "    \n",
    "# prints out the vcf sample file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  *if you are really good you know this is actually the .vcf from the spec that talks about the file formatted columns, so download the spec and then youc an use this verbatim !* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANNTransformer\n",
      "ANN_AA_LENGTH_FIELD\n",
      "ANN_AA_POS_FIELD\n",
      "ANN_ANNOTATION_FIELD\n",
      "ANN_ANNOTATION_IMPACT_FIELD\n",
      "ANN_CDNA_LENGTH_FIELD\n",
      "ANN_CDNA_POS_FIELD\n",
      "ANN_CDS_LENGTH_FIELD\n",
      "ANN_CDS_POS_FIELD\n",
      "ANN_DISTANCE_FIELD\n",
      "ANN_FEATURE_ID_FIELD\n",
      "ANN_FEATURE_TYPE_FIELD\n",
      "ANN_FIELD\n",
      "ANN_FIELDS\n",
      "ANN_GENE_ID_FIELD\n",
      "ANN_GENE_NAME_FIELD\n",
      "ANN_HGVS_C_FIELD\n",
      "ANN_HGVS_P_FIELD\n",
      "ANN_RANK_FIELD\n",
      "ANN_TRANSCRIPT_BIOTYPE_FIELD\n",
      "AlleleCountsArray\n",
      "AlleleCountsChunkedArray\n",
      "AlleleCountsChunkedTable\n",
      "AlleleCountsDaskArray\n",
      "CenterScaler\n",
      "DEFAULT_ALT_NUMBER\n",
      "DEFAULT_BUFFER_SIZE\n",
      "DEFAULT_CHUNK_LENGTH\n",
      "DEFAULT_CHUNK_WIDTH\n",
      "FIXED_VARIANTS_FIELDS\n",
      "FeatureTable\n",
      "FileInputStream\n",
      "FileNotFoundError\n",
      "GenotypeAlleleCounts\n",
      "GenotypeAlleleCountsArray\n",
      "GenotypeAlleleCountsChunkedArray\n",
      "GenotypeAlleleCountsDaskArray\n",
      "GenotypeAlleleCountsDaskVector\n",
      "GenotypeAlleleCountsVector\n",
      "GenotypeArray\n",
      "GenotypeChunkedArray\n",
      "GenotypeDaskArray\n",
      "GenotypeDaskVector\n",
      "GenotypeVector\n",
      "Genotypes\n",
      "HaplotypeArray\n",
      "HaplotypeChunkedArray\n",
      "HaplotypeDaskArray\n",
      "INHERIT_MISSING\n",
      "INHERIT_NONPARENTAL\n",
      "INHERIT_NONSEG_ALT\n",
      "INHERIT_NONSEG_REF\n",
      "INHERIT_PARENT1\n",
      "INHERIT_PARENT2\n",
      "INHERIT_PARENT_MISSING\n",
      "INHERIT_UNDETERMINED\n",
      "PY2\n",
      "PattersonScaler\n",
      "SortedIndex\n",
      "SortedMultiIndex\n",
      "StandardScaler\n",
      "UniqueIndex\n",
      "VCFChunkIterator\n",
      "VCFHeaders\n",
      "VCF_FIXED_FIELDS\n",
      "VariantChunkedTable\n",
      "VariantTable\n",
      "abc\n",
      "absolute_import\n",
      "admixture\n",
      "allel\n",
      "array_to_hdf5\n",
      "asarray_ndim\n",
      "average_hudson_fst\n",
      "average_patterson_d\n",
      "average_patterson_f3\n",
      "average_patterson_fst\n",
      "average_weir_cockerham_fst\n",
      "blockwise_hudson_fst\n",
      "blockwise_patterson_d\n",
      "blockwise_patterson_f3\n",
      "blockwise_patterson_fst\n",
      "blockwise_weir_cockerham_fst\n",
      "chunked\n",
      "compat\n",
      "condensed_coords\n",
      "condensed_coords_between\n",
      "condensed_coords_within\n",
      "constants\n",
      "create_allele_mapping\n",
      "csv\n",
      "dask\n",
      "date\n",
      "debug\n",
      "decomposition\n",
      "default_float_dtype\n",
      "default_integer_dtype\n",
      "default_numbers\n",
      "default_string_dtype\n",
      "default_types\n",
      "distance\n",
      "diversity\n",
      "division\n",
      "ehh_decay\n",
      "equally_accessible_windows\n",
      "fasta\n",
      "fig_voight_painting\n",
      "fold_joint_sfs\n",
      "fold_sfs\n",
      "fst\n",
      "garud_h\n",
      "get_scaler\n",
      "gff\n",
      "gff3_parse_attributes\n",
      "gff3_to_dataframe\n",
      "gff3_to_recarray\n",
      "gzip\n",
      "haplotype_diversity\n",
      "heterozygosity_expected\n",
      "heterozygosity_observed\n",
      "hudson_fst\n",
      "hw\n",
      "ihs\n",
      "inbreeding_coefficient\n",
      "io\n",
      "itemgetter\n",
      "iter_gff3\n",
      "iter_vcf_chunks\n",
      "itertools\n",
      "joint_sfs\n",
      "joint_sfs_folded\n",
      "joint_sfs_folded_scaled\n",
      "joint_sfs_scaled\n",
      "ld\n",
      "locate_fixed_differences\n",
      "locate_private_alleles\n",
      "locate_unlinked\n",
      "logger\n",
      "logging\n",
      "mean_pairwise_difference\n",
      "mean_pairwise_difference_between\n",
      "mendel\n",
      "mendel_errors\n",
      "misc\n",
      "model\n",
      "moving_delta_tajima_d\n",
      "moving_garud_h\n",
      "moving_haplotype_diversity\n",
      "moving_hudson_fst\n",
      "moving_mean\n",
      "moving_midpoint\n",
      "moving_patterson_d\n",
      "moving_patterson_f3\n",
      "moving_patterson_fst\n",
      "moving_statistic\n",
      "moving_std\n",
      "moving_tajima_d\n",
      "moving_weir_cockerham_fst\n",
      "namedtuple\n",
      "normalize_callset\n",
      "np\n",
      "nsl\n",
      "opt\n",
      "os\n",
      "paint_transmission\n",
      "pairwise_distance\n",
      "pairwise_dxy\n",
      "patterson_d\n",
      "patterson_f2\n",
      "patterson_f3\n",
      "patterson_fst\n",
      "pca\n",
      "pcoa\n",
      "per_base\n",
      "phase_by_transmission\n",
      "phase_parents_by_transmission\n",
      "phase_progeny_by_transmission\n",
      "plot\n",
      "plot_haplotype_frequencies\n",
      "plot_joint_sfs\n",
      "plot_joint_sfs_folded\n",
      "plot_joint_sfs_folded_scaled\n",
      "plot_joint_sfs_scaled\n",
      "plot_moving_haplotype_frequencies\n",
      "plot_pairwise_distance\n",
      "plot_pairwise_ld\n",
      "plot_sfs\n",
      "plot_sfs_folded\n",
      "plot_sfs_folded_scaled\n",
      "plot_sfs_scaled\n",
      "plot_variant_locator\n",
      "plot_voight_painting\n",
      "preprocessing\n",
      "print_function\n",
      "randomized_pca\n",
      "re\n",
      "read_vcf\n",
      "recarray_from_hdf5_group\n",
      "recarray_to_hdf5_group\n",
      "rogers_huff_r\n",
      "rogers_huff_r_between\n",
      "roh\n",
      "roh_mhmm\n",
      "sample_to_haplotype_selection\n",
      "scale_joint_sfs\n",
      "scale_joint_sfs_folded\n",
      "scale_sfs\n",
      "scale_sfs_folded\n",
      "selection\n",
      "sequence_divergence\n",
      "sequence_diversity\n",
      "sf\n",
      "sfs\n",
      "sfs_folded\n",
      "sfs_folded_scaled\n",
      "sfs_scaled\n",
      "standardize\n",
      "standardize_by_allele_count\n",
      "stats\n",
      "subprocess\n",
      "tabulate_state_blocks\n",
      "tabulate_state_transitions\n",
      "tajima_d\n",
      "text_type\n",
      "time\n",
      "unquote_plus\n",
      "util\n",
      "vcf_read\n",
      "vcf_to_csv\n",
      "vcf_to_dataframe\n",
      "vcf_to_hdf5\n",
      "vcf_to_npz\n",
      "vcf_to_recarray\n",
      "vcf_to_zarr\n",
      "vcf_write\n",
      "voight_painting\n",
      "warnings\n",
      "watterson_theta\n",
      "weir_cockerham_fst\n",
      "window\n",
      "windowed_count\n",
      "windowed_df\n",
      "windowed_divergence\n",
      "windowed_diversity\n",
      "windowed_hudson_fst\n",
      "windowed_patterson_fst\n",
      "windowed_r_squared\n",
      "windowed_statistic\n",
      "windowed_tajima_d\n",
      "windowed_watterson_theta\n",
      "windowed_weir_cockerham_fst\n",
      "write_fasta\n",
      "write_vcf\n",
      "write_vcf_data\n",
      "write_vcf_header\n",
      "xpehh\n",
      "xpnsl\n",
      "zip_longest\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# to understand how powerful this library is, look at its .methods available for you:\n",
    "\n",
    "for i in dir(allel): \n",
    "    if not i.startswith(\"_\"):\n",
    "        print(i)\n",
    "        \n",
    "# ever heard of the massive library scikit-learn ?  \n",
    "# think genomic version of that ! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We will focus on extracting data from Variant Call Format (VCF) files and loading into NumPy arrays, \n",
    "pandas data frames, HDF5 files or Zarr arrays for ease of analysis. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key is to focus on extracting the necessary data from the VCF file and loading it into a more efficient storage container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "callset = allel.read_vcf('C:/SPARK/sample.vcf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(type(callset))   # key/value dict ! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['calldata/GT',\n",
       " 'samples',\n",
       " 'variants/ALT',\n",
       " 'variants/CHROM',\n",
       " 'variants/FILTER_PASS',\n",
       " 'variants/ID',\n",
       " 'variants/POS',\n",
       " 'variants/QUAL',\n",
       " 'variants/REF']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Here are my available keys:\n",
    "sorted(callset.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All arrays with keys beginning ‘variants/’ come from the fixed fields in the VCF file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'read_vcf' in dir(allel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'read_vcf_headers' in dir(allel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'samples': array(['NA00001', 'NA00002', 'NA00003'], dtype=object), 'variants/ID': array(['.', '.', 'rs6054257', '.', 'rs6040355', '.', 'microsat1', '.',\n",
      "       'rsTest'], dtype=object), 'variants/ALT': array([['C', '', ''],\n",
      "       ['G', '', ''],\n",
      "       ['A', '', ''],\n",
      "       ['A', '', ''],\n",
      "       ['G', 'T', ''],\n",
      "       ['', '', ''],\n",
      "       ['GA', 'GAC', ''],\n",
      "       ['', '', ''],\n",
      "       ['A', 'ATG', '']], dtype=object), 'variants/POS': array([    111,     112,   14370,   17330, 1110696, 1230237, 1234567,\n",
      "       1235237,      10]), 'variants/CHROM': array(['19', '19', '20', '20', '20', '20', '20', '20', 'X'], dtype=object), 'variants/FILTER_PASS': array([False, False,  True, False,  True,  True,  True, False,  True]), 'variants/REF': array(['A', 'A', 'G', 'T', 'A', 'T', 'G', 'T', 'AC'], dtype=object), 'calldata/GT': array([[[ 0,  0],\n",
      "        [ 0,  0],\n",
      "        [ 0,  1]],\n",
      "\n",
      "       [[ 0,  0],\n",
      "        [ 0,  0],\n",
      "        [ 0,  1]],\n",
      "\n",
      "       [[ 0,  0],\n",
      "        [ 1,  0],\n",
      "        [ 1,  1]],\n",
      "\n",
      "       [[ 0,  0],\n",
      "        [ 0,  1],\n",
      "        [ 0,  0]],\n",
      "\n",
      "       [[ 1,  2],\n",
      "        [ 2,  1],\n",
      "        [ 2,  2]],\n",
      "\n",
      "       [[ 0,  0],\n",
      "        [ 0,  0],\n",
      "        [ 0,  0]],\n",
      "\n",
      "       [[ 0,  1],\n",
      "        [ 0,  2],\n",
      "        [ 1,  1]],\n",
      "\n",
      "       [[ 0,  0],\n",
      "        [ 0,  0],\n",
      "        [-1, -1]],\n",
      "\n",
      "       [[ 0, -1],\n",
      "        [ 0,  1],\n",
      "        [ 0,  2]]], dtype=int8), 'variants/QUAL': array([ 9.6, 10. , 29. ,  3. , 67. , 47. , 50. ,  nan, 10. ],\n",
      "      dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(callset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clear\n",
      "copy\n",
      "fromkeys\n",
      "get\n",
      "items\n",
      "keys\n",
      "pop\n",
      "popitem\n",
      "setdefault\n",
      "update\n",
      "values\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Commands (methods) you can use:\n",
    "\n",
    "for i in dir(callset):\n",
    "    if not i.startswith(\"_\"):\n",
    "        print(i)\n",
    "\n",
    "# methods you can call, as a reference...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ['NA00001' 'NA00002' 'NA00003']\n",
      "\n",
      " ['.' '.' 'rs6054257' '.' 'rs6040355' '.' 'microsat1' '.' 'rsTest']\n",
      "\n",
      " [['C' '' '']\n",
      " ['G' '' '']\n",
      " ['A' '' '']\n",
      " ['A' '' '']\n",
      " ['G' 'T' '']\n",
      " ['' '' '']\n",
      " ['GA' 'GAC' '']\n",
      " ['' '' '']\n",
      " ['A' 'ATG' '']]\n",
      "\n",
      " [    111     112   14370   17330 1110696 1230237 1234567 1235237      10]\n",
      "\n",
      " ['19' '19' '20' '20' '20' '20' '20' '20' 'X']\n",
      "\n",
      " [False False  True False  True  True  True False  True]\n",
      "\n",
      " ['A' 'A' 'G' 'T' 'A' 'T' 'G' 'T' 'AC']\n",
      "\n",
      " [[[ 0  0]\n",
      "  [ 0  0]\n",
      "  [ 0  1]]\n",
      "\n",
      " [[ 0  0]\n",
      "  [ 0  0]\n",
      "  [ 0  1]]\n",
      "\n",
      " [[ 0  0]\n",
      "  [ 1  0]\n",
      "  [ 1  1]]\n",
      "\n",
      " [[ 0  0]\n",
      "  [ 0  1]\n",
      "  [ 0  0]]\n",
      "\n",
      " [[ 1  2]\n",
      "  [ 2  1]\n",
      "  [ 2  2]]\n",
      "\n",
      " [[ 0  0]\n",
      "  [ 0  0]\n",
      "  [ 0  0]]\n",
      "\n",
      " [[ 0  1]\n",
      "  [ 0  2]\n",
      "  [ 1  1]]\n",
      "\n",
      " [[ 0  0]\n",
      "  [ 0  0]\n",
      "  [-1 -1]]\n",
      "\n",
      " [[ 0 -1]\n",
      "  [ 0  1]\n",
      "  [ 0  2]]]\n",
      "\n",
      " [ 9.6 10.  29.   3.  67.  47.  50.   nan 10. ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in callset.values():  print(\"\\n\",i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The callset object returned by read_vcf() is a Python dictionary (dict). It contains several NumPy arrays, each of which can be accessed via a key. Here are the available keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['calldata/GT',\n",
       " 'samples',\n",
       " 'variants/ALT',\n",
       " 'variants/CHROM',\n",
       " 'variants/FILTER_PASS',\n",
       " 'variants/ID',\n",
       " 'variants/POS',\n",
       " 'variants/QUAL',\n",
       " 'variants/REF']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# The callset object returned by read_vcf() is a Python dictionary (dict). \n",
    "# It contains several NumPy arrays, each of which can be accessed via a key. \n",
    "# Here are the available keys:\n",
    "\n",
    "# keys:\n",
    "sorted(callset.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- calldata/GT\n",
      "- samples\n",
      "- variants/ALT\n",
      "- variants/CHROM\n",
      "- variants/FILTER_PASS\n",
      "- variants/ID\n",
      "- variants/POS\n",
      "- variants/QUAL\n",
      "- variants/REF\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in sorted(callset.keys()):  print(\"-\",i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NA00001', 'NA00002', 'NA00003'], dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# The ‘samples’ array contains sample identifiers extracted from the header line in the VCF file.\n",
    "\n",
    "callset['samples']\n",
    "\n",
    "# look to the far right:\n",
    "# #CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO\tFORMAT\tNA00001\tNA00002\tNA00003\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['19', '19', '20', '20', '20', '20', '20', '20', 'X'], dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# All arrays with keys beginning ‘variants/’ come from the fixed fields in the VCF file.\n",
    "# For example, here is the data from the ‘CHROM’ field:\n",
    "\n",
    "callset['variants/CHROM']\n",
    "\n",
    "# chromosomes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    111,     112,   14370,   17330, 1110696, 1230237, 1234567,\n",
       "       1235237,      10])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Here is the data from the ‘POS’ field:\n",
    "\n",
    "callset['variants/POS']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.6, 10. , 29. ,  3. , 67. , 47. , 50. ,  nan, 10. ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Here is the data from the ‘QUAL’ field:\n",
    "\n",
    "callset['variants/QUAL']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  0],\n",
       "        [ 0,  0],\n",
       "        [ 0,  1]],\n",
       "\n",
       "       [[ 0,  0],\n",
       "        [ 0,  0],\n",
       "        [ 0,  1]],\n",
       "\n",
       "       [[ 0,  0],\n",
       "        [ 1,  0],\n",
       "        [ 1,  1]],\n",
       "\n",
       "       [[ 0,  0],\n",
       "        [ 0,  1],\n",
       "        [ 0,  0]],\n",
       "\n",
       "       [[ 1,  2],\n",
       "        [ 2,  1],\n",
       "        [ 2,  2]],\n",
       "\n",
       "       [[ 0,  0],\n",
       "        [ 0,  0],\n",
       "        [ 0,  0]],\n",
       "\n",
       "       [[ 0,  1],\n",
       "        [ 0,  2],\n",
       "        [ 1,  1]],\n",
       "\n",
       "       [[ 0,  0],\n",
       "        [ 0,  0],\n",
       "        [-1, -1]],\n",
       "\n",
       "       [[ 0, -1],\n",
       "        [ 0,  1],\n",
       "        [ 0,  2]]], dtype=int8)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# All arrays with keys beginning ‘calldata/’ come from the sample fields in the VCF file. \n",
    "# For example, here are the actual genotype calls from the ‘GT’ field:\n",
    "\n",
    "callset['calldata/GT']\n",
    "\n",
    "# Note the -1 values for one of the genotype calls. By default scikit-allel uses \n",
    "# -1 to indicate a missing value for any array with a signed integer data type \n",
    "# d(although you can change this if you want).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"allel allel-DisplayAs2D\"><span>&lt;GenotypeArray shape=(9, 3, 2) dtype=int8&gt;</span><table><thead><tr><th></th><th style=\"text-align: center\">0</th><th style=\"text-align: center\">1</th><th style=\"text-align: center\">2</th></tr></thead><tbody><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">0</th><td style=\"text-align: center\">0/0</td><td style=\"text-align: center\">0/0</td><td style=\"text-align: center\">0/1</td></tr><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">1</th><td style=\"text-align: center\">0/0</td><td style=\"text-align: center\">0/0</td><td style=\"text-align: center\">0/1</td></tr><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">2</th><td style=\"text-align: center\">0/0</td><td style=\"text-align: center\">1/0</td><td style=\"text-align: center\">1/1</td></tr><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">...</th><td style=\"text-align: center\" colspan=\"4\">...</td></tr><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">6</th><td style=\"text-align: center\">0/1</td><td style=\"text-align: center\">0/2</td><td style=\"text-align: center\">1/1</td></tr><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">7</th><td style=\"text-align: center\">0/0</td><td style=\"text-align: center\">0/0</td><td style=\"text-align: center\">./.</td></tr><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">8</th><td style=\"text-align: center\">0/.</td><td style=\"text-align: center\">0/1</td><td style=\"text-align: center\">0/2</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "<GenotypeArray shape=(9, 3, 2) dtype=int8>\n",
       "0/0 0/0 0/1\n",
       "0/0 0/0 0/1\n",
       "0/0 1/0 1/1\n",
       "...\n",
       "0/1 0/2 1/1\n",
       "0/0 0/0 ./.\n",
       "0/. 0/1 0/2"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Aside: genotype arrays\n",
    "# Because working with genotype calls is a very common task, scikit-allel has \n",
    "# a GenotypeArray class which adds some convenient functionality to an array \n",
    "# of genotype calls. To use this class, pass the raw NumPy array into the GenotypeArray \n",
    "# class constructor, e.g.:\n",
    "\n",
    "\n",
    "gt = allel.GenotypeArray(callset['calldata/GT'])\n",
    "\n",
    "gt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the things that the GenotypeArray class does is provide a slightly more visually-appealing representation when used in a Jupyter notebook, as can be seen above. There are also methods for making various computations over the genotype calls. For example, the is_het() method locates all heterozygous genotype calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [ True,  True, False],\n",
       "       [False, False, False],\n",
       "       [ True,  True, False],\n",
       "       [False, False, False],\n",
       "       [False,  True,  True]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt.is_het()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 2, 0, 2, 0, 2])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# To give another example, the count_het() method will count heterozygous calls, summing over \n",
    "# variants (axis=0) or samples (axis=1) if requested.\n",
    "# E.g., to count the number of het calls per variant:\n",
    "    \n",
    "gt.count_het(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"allel allel-DisplayAs2D\"><span>&lt;AlleleCountsArray shape=(9, 3) dtype=int32&gt;</span><table><thead><tr><th></th><th style=\"text-align: center\">0</th><th style=\"text-align: center\">1</th><th style=\"text-align: center\">2</th></tr></thead><tbody><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">0</th><td style=\"text-align: center\">5</td><td style=\"text-align: center\">1</td><td style=\"text-align: center\">0</td></tr><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">1</th><td style=\"text-align: center\">5</td><td style=\"text-align: center\">1</td><td style=\"text-align: center\">0</td></tr><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">2</th><td style=\"text-align: center\">3</td><td style=\"text-align: center\">3</td><td style=\"text-align: center\">0</td></tr><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">...</th><td style=\"text-align: center\" colspan=\"4\">...</td></tr><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">6</th><td style=\"text-align: center\">2</td><td style=\"text-align: center\">3</td><td style=\"text-align: center\">1</td></tr><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">7</th><td style=\"text-align: center\">4</td><td style=\"text-align: center\">0</td><td style=\"text-align: center\">0</td></tr><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">8</th><td style=\"text-align: center\">3</td><td style=\"text-align: center\">1</td><td style=\"text-align: center\">1</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "<AlleleCountsArray shape=(9, 3) dtype=int32>\n",
       "5 1 0\n",
       "5 1 0\n",
       "3 3 0\n",
       "...\n",
       "2 3 1\n",
       "4 0 0\n",
       "3 1 1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# One more example, here is how to perform an allele count,\n",
    "# i.e., count the number times each allele (0=reference, 1=first alternate, \n",
    "# 2=second alternate, etc.) is observed for each variant:\n",
    "    \n",
    "ac = gt.count_alleles()\n",
    "ac \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fields: \n",
    "    \n",
    "VCF files can often contain many fields of data, and you may only need to extract \n",
    "some of them to perform a particular analysis. You can select which fields to extract by passing a list of strings as the fields parameter. For example, let’s extract the ‘DP’ field from within the ‘INFO’ field, and let’s also extract the ‘DP’ field from the genotype call data:\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['calldata/DP', 'variants/DP']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "callset = allel.read_vcf('C:/SPARK/sample.vcf', fields=['variants/DP', 'calldata/DP'])\n",
    "sorted(callset.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1, 14, 11, 10, 13,  9, -1, -1])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# here is the data we just extracted:\n",
    "callset['variants/DP']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1, -1, -1],\n",
       "       [-1, -1, -1],\n",
       "       [ 1,  8,  5],\n",
       "       [ 3,  5,  3],\n",
       "       [ 6,  0,  4],\n",
       "       [-1,  4,  2],\n",
       "       [ 4,  2,  3],\n",
       "       [-1, -1, -1],\n",
       "       [-1, -1, -1]], dtype=int16)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "callset['calldata/DP']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "I chose these two fields to illustrate the point that sometimes the same field name (e.g., ‘DP’) can be used both within the INFO field of a VCF and also within the genotype call data. When selecting fields, to make sure there is no ambiguity, you can include a prefix which is either ‘variants/’ or ‘calldata/’. For example, if you provide ‘variants/DP’, then the read_vcf() function will look for an INFO field named ‘DP’. If you provide ‘calldata/DP’ then read_vcf() will look for a FORMAT field named ‘DP’ within the call data.\n",
    "\n",
    "If you are feeling lazy, you can drop the ‘variants/’ and ‘calldata/’ prefixes, in which case read_vcf() will assume you mean ‘variants/’ if there is any ambiguity. E.g.:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['calldata/GT', 'variants/DP']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "callset = allel.read_vcf('C:/SPARK/sample.vcf', fields=['DP', 'GT'])\n",
    "sorted(callset.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to extract absolutely everything from a VCF file, then you can provide a special value '*' as the fields parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['calldata/DP',\n",
       " 'calldata/GQ',\n",
       " 'calldata/GT',\n",
       " 'calldata/HQ',\n",
       " 'samples',\n",
       " 'variants/AA',\n",
       " 'variants/AC',\n",
       " 'variants/AF',\n",
       " 'variants/ALT',\n",
       " 'variants/AN',\n",
       " 'variants/CHROM',\n",
       " 'variants/DB',\n",
       " 'variants/DP',\n",
       " 'variants/FILTER_PASS',\n",
       " 'variants/FILTER_q10',\n",
       " 'variants/FILTER_s50',\n",
       " 'variants/H2',\n",
       " 'variants/ID',\n",
       " 'variants/NS',\n",
       " 'variants/POS',\n",
       " 'variants/QUAL',\n",
       " 'variants/REF',\n",
       " 'variants/is_snp',\n",
       " 'variants/numalt',\n",
       " 'variants/svlen']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "callset = allel.read_vcf('C:/SPARK/sample.vcf', fields='*')\n",
    "sorted(callset.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also provide the special value 'variants/*' to request all variants fields (including all INFO), and the special value 'calldata/*' to request all call data fields.\n",
    "\n",
    "If you don’t specify the fields parameter, scikit-allel will default to extracting data from the fixed variants fields (but no INFO) and the GT genotype field if present (but no other call data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy arrays can have various data types, including signed integers (‘int8’, ‘int16’, ‘int32’, ‘int64’), unsigned integers (‘uint8’, ‘uint16’, ‘uint32’, ‘uint64’), floating point numbers (‘float32’, ‘float64’), variable length strings (‘object’) and fixed length strings (e.g., ‘S4’ for a 4-character ASCII string). scikit-allel will try to choose a sensible default data type for the fields you want to extract, based on the meta-information in the VCF file, but you can override these via the types parameter.\n",
    "\n",
    "For example, by default the ‘DP’ INFO field is loaded into a 32-bit integer array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1, 14, 11, 10, 13,  9, -1, -1])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "callset = allel.read_vcf('C:/SPARK/sample.vcf', fields=['DP'])\n",
    "callset['variants/DP']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fields containing textual data, there are two choices for data type. By default, scikit-allel will use an ‘object’ data type, which means that values are stored as an array of Python strings. E.g.:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bam !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['A', 'A', 'G', 'T', 'A', 'T', 'G', 'T', 'AC'], dtype=object)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "callset = allel.read_vcf('C:/SPARK/sample.vcf')\n",
    "callset['variants/REF']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'A', b'A', b'G', b'T', b'A', b'T', b'G', b'T', b'AC'], dtype='|S3')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# The advantage of using ‘object’ dtype is that strings can be of any length. \n",
    "# Alternatively, you can use a fixed-length string dtype, e.g.:\n",
    "\n",
    "callset = allel.read_vcf('C:/SPARK/sample.vcf', types={'REF': 'S3'})\n",
    "callset['variants/REF']\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Some fields like ‘ALT’, ‘AC’ and ‘AF’ can have a variable number of values. I.e., each variant may have a different number of data values for these fields. One trade-off you have to make when loading data into NumPy arrays is that you cannot have arrays with a variable number of items per row. Rather, you have to fix the maximum number of possible items. While you lose some flexibility, you gain speed of access.\n",
    "\n",
    "For fields like ‘ALT’, scikit-allel will choose a default number of expected values, which is set at 3. E.g., here is what you get by default:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['C', '', ''],\n",
       "       ['G', '', ''],\n",
       "       ['A', '', ''],\n",
       "       ['A', '', ''],\n",
       "       ['G', 'T', ''],\n",
       "       ['', '', ''],\n",
       "       ['GA', 'GAC', ''],\n",
       "       ['', '', ''],\n",
       "       ['A', 'ATG', '']], dtype=object)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "callset = allel.read_vcf('C:/SPARK/sample.vcf')\n",
    "callset['variants/ALT']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, 3 is more that we need, because no variant has more than 2 ALT values. However, some VCF files (especially those including INDELs) may have more than 3 ALT values.\n",
    "\n",
    "If you need to increase or decrease the expected number of values for any field, you can do this via the numbers parameter. E.g., increase the number of ALT values to 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['C', '', '', '', ''],\n",
       "       ['G', '', '', '', ''],\n",
       "       ['A', '', '', '', ''],\n",
       "       ['A', '', '', '', ''],\n",
       "       ['G', 'T', '', '', ''],\n",
       "       ['', '', '', '', ''],\n",
       "       ['GA', 'GAC', '', '', ''],\n",
       "       ['', '', '', '', ''],\n",
       "       ['A', 'ATG', '', '', '']], dtype=object)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "callset = allel.read_vcf('C:/SPARK/sample.vcf', numbers={'ALT': 5})\n",
    "callset['variants/ALT']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genotype ploidy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "By default, scikit-allel assumes you are working with a diploid organism, and so expects to parse out 2 alleles for each genotype call. If you are working with an organism with some other ploidy, you can change the expected number of alleles via the numbers parameter.\n",
    "\n",
    "For example, here is an example VCF with tetraploid genotype calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can extract data for only a specific chromosome or genome region via the region parameter. The value of the parameter should be a region string of the format ‘{chromosome}:{begin}-{end}’, just like you would give to tabix or samtools. E.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1110696, 1230237])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "callset = allel.read_vcf('C:/SPARK/sample.vcf', region='20:1000000-1231000')\n",
    "callset['variants/POS']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NA00001', 'NA00003'], dtype=object)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# You can extract data for only specific samples via the samples parameter. \n",
    "# e.g. extract data for samples ‘NA00001’ and ‘NA00003’:\n",
    "\n",
    "callset = allel.read_vcf('C:/SPARK/sample.vcf', samples=['NA00001', 'NA00003'])\n",
    "callset['samples']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"allel allel-DisplayAs2D\"><span>&lt;GenotypeArray shape=(9, 2, 2) dtype=int8&gt;</span><table><thead><tr><th></th><th style=\"text-align: center\">0</th><th style=\"text-align: center\">1</th></tr></thead><tbody><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">0</th><td style=\"text-align: center\">0/0</td><td style=\"text-align: center\">0/1</td></tr><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">1</th><td style=\"text-align: center\">0/0</td><td style=\"text-align: center\">0/1</td></tr><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">2</th><td style=\"text-align: center\">0/0</td><td style=\"text-align: center\">1/1</td></tr><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">...</th><td style=\"text-align: center\" colspan=\"3\">...</td></tr><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">6</th><td style=\"text-align: center\">0/1</td><td style=\"text-align: center\">1/1</td></tr><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">7</th><td style=\"text-align: center\">0/0</td><td style=\"text-align: center\">./.</td></tr><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">8</th><td style=\"text-align: center\">0/.</td><td style=\"text-align: center\">0/2</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "<GenotypeArray shape=(9, 2, 2) dtype=int8>\n",
       "0/0 0/1\n",
       "0/0 0/1\n",
       "0/0 1/1\n",
       "...\n",
       "0/1 1/1\n",
       "0/0 ./.\n",
       "0/. 0/2"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "allel.GenotypeArray(callset['calldata/GT'])\n",
    "\n",
    "# Note that the genotype array now only has two columns, corresponding to the two samples requested.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## we can also take the .vcf and store it as hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  but will store extracted data into an HDF5 file stored on disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vcf_to_hdf5()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For large datasets, the vcf_to_hdf5() function is available. This function again takes similar parameters to read_vcf(), but will store extracted data into an HDF5 file stored on disk. The extraction process works through the VCF file in chunks, and so the entire dataset is never loaded entirely into main memory. A bit further below I give worked examples with a large dataset, but for now here is a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# STOP:  if this file already exists, this will error out, so make sure it doesn't already exist\n",
    "# saving the file directly as hdf5 ! ! ! \n",
    "\n",
    "allel.vcf_to_hdf5('C:/SPARK/sample.vcf', 'C:/SPARK/sample2_hdf5.h5', fields='*', overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 file \"sample2_hdf5.h5\" (mode r)>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 file \"sample2_hdf5.h5\" (mode r)>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# now lets assume i just showed up and wanted to review this information (the hdf5 file):\n",
    "\n",
    "import h5py  # conda install this \n",
    "\n",
    "callset = h5py.File('C:/SPARK/sample2_hdf5.h5', mode='r')\n",
    "\n",
    "callset\n",
    "\n",
    "print(callset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"CHROM\": shape (9,), type \"|O\">"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "chrom = callset['variants/CHROM']\n",
    "chrom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"POS\": shape (9,), type \"<i4\">"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pos = callset['variants/POS']\n",
    "pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"GT\": shape (9, 3, 2), type \"|i1\">"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "gt = callset['calldata/GT']\n",
    "gt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['19', '20'], dtype=object)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# This dataset object is useful because you can then load all or only part of \n",
    "# the underlying data into main memory via slicing. e.g.\n",
    "\n",
    "# load second to fourth items into NumPy array\n",
    "chrom[1:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"allel allel-DisplayAs2D\"><span>&lt;GenotypeArray shape=(2, 3, 2) dtype=int8&gt;</span><table><thead><tr><th></th><th style=\"text-align: center\">0</th><th style=\"text-align: center\">1</th><th style=\"text-align: center\">2</th></tr></thead><tbody><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">0</th><td style=\"text-align: center\">0/0</td><td style=\"text-align: center\">0/0</td><td style=\"text-align: center\">0/1</td></tr><tr><th style=\"text-align: center; background-color: white; border-right: 1px solid black; \">1</th><td style=\"text-align: center\">0/0</td><td style=\"text-align: center\">1/0</td><td style=\"text-align: center\">1/1</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "<GenotypeArray shape=(2, 3, 2) dtype=int8>\n",
       "0/0 0/0 0/1\n",
       "0/0 1/0 1/1"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# load genotype calls into memory for second to fourth variants, all samples\n",
    "allel.GenotypeArray(gt[1:3, :])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assume you want all of this in a dataframe ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tbresee\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\allel\\io\\vcf_read.py:1569: FutureWarning: from_items is deprecated. Please use DataFrame.from_dict(dict(items), ...) instead. DataFrame.from_dict(OrderedDict(items)) may be used to preserve the key order.\n",
      "  df = pandas.DataFrame.from_items(items)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CHROM</th>\n",
       "      <th>POS</th>\n",
       "      <th>ID</th>\n",
       "      <th>REF</th>\n",
       "      <th>ALT_1</th>\n",
       "      <th>ALT_2</th>\n",
       "      <th>ALT_3</th>\n",
       "      <th>QUAL</th>\n",
       "      <th>FILTER_PASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>111</td>\n",
       "      <td>.</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>9.6</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>112</td>\n",
       "      <td>.</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>14370</td>\n",
       "      <td>rs6054257</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>29.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>17330</td>\n",
       "      <td>.</td>\n",
       "      <td>T</td>\n",
       "      <td>A</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>1110696</td>\n",
       "      <td>rs6040355</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>T</td>\n",
       "      <td></td>\n",
       "      <td>67.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>1230237</td>\n",
       "      <td>.</td>\n",
       "      <td>T</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>47.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20</td>\n",
       "      <td>1234567</td>\n",
       "      <td>microsat1</td>\n",
       "      <td>G</td>\n",
       "      <td>GA</td>\n",
       "      <td>GAC</td>\n",
       "      <td></td>\n",
       "      <td>50.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20</td>\n",
       "      <td>1235237</td>\n",
       "      <td>.</td>\n",
       "      <td>T</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>X</td>\n",
       "      <td>10</td>\n",
       "      <td>rsTest</td>\n",
       "      <td>AC</td>\n",
       "      <td>A</td>\n",
       "      <td>ATG</td>\n",
       "      <td></td>\n",
       "      <td>10.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CHROM      POS         ID REF ALT_1 ALT_2 ALT_3  QUAL  FILTER_PASS\n",
       "0    19      111          .   A     C               9.6        False\n",
       "1    19      112          .   A     G              10.0        False\n",
       "2    20    14370  rs6054257   G     A              29.0         True\n",
       "3    20    17330          .   T     A               3.0        False\n",
       "4    20  1110696  rs6040355   A     G     T        67.0         True\n",
       "5    20  1230237          .   T                    47.0         True\n",
       "6    20  1234567  microsat1   G    GA   GAC        50.0         True\n",
       "7    20  1235237          .   T                     NaN        False\n",
       "8     X       10     rsTest  AC     A   ATG        10.0         True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# For some analyses it can be useful to think of the data in a VCF file as a table or data frame, \n",
    "# especially if you are only analysing data from the fixed fields and don’t need the genotype \n",
    "# calls or any other call data. The vcf_to_dataframe() function extracts data from a VCF and \n",
    "# loads into a pandas DataFrame. E.g.:\n",
    "\n",
    "df = allel.vcf_to_dataframe('C:/SPARK/sample.vcf')\n",
    "\n",
    "df\n",
    "\n",
    "# extract my data from the vcf and put into a dataframe ! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CHROM      POS         ID REF ALT_1 ALT_2 ALT_3  QUAL  FILTER_PASS\n",
      "0    19      111          .   A     C               9.6        False\n",
      "1    19      112          .   A     G              10.0        False\n",
      "2    20    14370  rs6054257   G     A              29.0         True\n",
      "3    20    17330          .   T     A               3.0        False\n",
      "4    20  1110696  rs6040355   A     G     T        67.0         True\n",
      "5    20  1230237          .   T                    47.0         True\n",
      "6    20  1234567  microsat1   G    GA   GAC        50.0         True\n",
      "7    20  1235237          .   T                     NaN        False\n",
      "8     X       10     rsTest  AC     A   ATG        10.0         True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# so its just the original vcf data but put into a dataframe, which is cool:\n",
    "\n",
    "# #CHROM  POS    ID    REF    ALT    QUAL   \n",
    "\n",
    "# 19      111         .    A    C    9.6   \n",
    "# 19      112          .    A    G    10    \n",
    "# 20      14370        rs6054257    G    A    29    \n",
    "# 20      17330    .    T    A    3    q10       \n",
    "# 20      1110696    rs6040355    A    G,T    67   \n",
    "# 20      1230237    .    T    .    47      \n",
    "# 20      1234567    microsat1    G    GA,GAC    50   \n",
    "# 20      1235237    .    T    .    .    .    .   \n",
    "# X       10    rsTest    AC    A,ATG    10    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CHROM</th>\n",
       "      <th>POS</th>\n",
       "      <th>ID</th>\n",
       "      <th>REF</th>\n",
       "      <th>ALT_1</th>\n",
       "      <th>ALT_2</th>\n",
       "      <th>QUAL</th>\n",
       "      <th>DP</th>\n",
       "      <th>AA</th>\n",
       "      <th>NS</th>\n",
       "      <th>...</th>\n",
       "      <th>AC_1</th>\n",
       "      <th>AC_2</th>\n",
       "      <th>DB</th>\n",
       "      <th>FILTER_PASS</th>\n",
       "      <th>FILTER_q10</th>\n",
       "      <th>FILTER_s50</th>\n",
       "      <th>numalt</th>\n",
       "      <th>svlen_1</th>\n",
       "      <th>svlen_2</th>\n",
       "      <th>is_snp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>111</td>\n",
       "      <td>.</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td></td>\n",
       "      <td>9.6</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>112</td>\n",
       "      <td>.</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td></td>\n",
       "      <td>10.0</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>14370</td>\n",
       "      <td>rs6054257</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td></td>\n",
       "      <td>29.0</td>\n",
       "      <td>14</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>17330</td>\n",
       "      <td>.</td>\n",
       "      <td>T</td>\n",
       "      <td>A</td>\n",
       "      <td></td>\n",
       "      <td>3.0</td>\n",
       "      <td>11</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>1110696</td>\n",
       "      <td>rs6040355</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>T</td>\n",
       "      <td>67.0</td>\n",
       "      <td>10</td>\n",
       "      <td>T</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>1230237</td>\n",
       "      <td>.</td>\n",
       "      <td>T</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>47.0</td>\n",
       "      <td>13</td>\n",
       "      <td>T</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20</td>\n",
       "      <td>1234567</td>\n",
       "      <td>microsat1</td>\n",
       "      <td>G</td>\n",
       "      <td>GA</td>\n",
       "      <td>GAC</td>\n",
       "      <td>50.0</td>\n",
       "      <td>9</td>\n",
       "      <td>G</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20</td>\n",
       "      <td>1235237</td>\n",
       "      <td>.</td>\n",
       "      <td>T</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>X</td>\n",
       "      <td>10</td>\n",
       "      <td>rsTest</td>\n",
       "      <td>AC</td>\n",
       "      <td>A</td>\n",
       "      <td>ATG</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  CHROM      POS         ID REF ALT_1 ALT_2  QUAL  DP AA  NS   ...    AC_1  \\\n",
       "0    19      111          .   A     C         9.6  -1     -1   ...      -1   \n",
       "1    19      112          .   A     G        10.0  -1     -1   ...      -1   \n",
       "2    20    14370  rs6054257   G     A        29.0  14      3   ...      -1   \n",
       "3    20    17330          .   T     A         3.0  11      3   ...      -1   \n",
       "4    20  1110696  rs6040355   A     G     T  67.0  10  T   2   ...      -1   \n",
       "5    20  1230237          .   T              47.0  13  T   3   ...      -1   \n",
       "6    20  1234567  microsat1   G    GA   GAC  50.0   9  G   3   ...       3   \n",
       "7    20  1235237          .   T               NaN  -1     -1   ...      -1   \n",
       "8     X       10     rsTest  AC     A   ATG  10.0  -1     -1   ...      -1   \n",
       "\n",
       "   AC_2     DB  FILTER_PASS  FILTER_q10  FILTER_s50  numalt  svlen_1  svlen_2  \\\n",
       "0    -1  False        False       False       False       1        0        0   \n",
       "1    -1  False        False       False       False       1        0        0   \n",
       "2    -1   True         True       False       False       1        0        0   \n",
       "3    -1  False        False        True       False       1        0        0   \n",
       "4    -1   True         True       False       False       2        0        0   \n",
       "5    -1  False         True       False       False       0        0        0   \n",
       "6     1  False         True       False       False       2        1        2   \n",
       "7    -1  False        False       False       False       0        0        0   \n",
       "8    -1  False         True       False       False       2       -1        1   \n",
       "\n",
       "   is_snp  \n",
       "0    True  \n",
       "1    True  \n",
       "2    True  \n",
       "3    True  \n",
       "4    True  \n",
       "5   False  \n",
       "6   False  \n",
       "7   False  \n",
       "8   False  \n",
       "\n",
       "[9 rows x 24 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# some values (cols) on the right were missing, but if you want ALL of them:\n",
    "\n",
    "df = allel.vcf_to_dataframe('C:/SPARK/sample.vcf', fields='*', alt_number=2)\n",
    "df\n",
    "\n",
    "# pandas type reflection \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CHROM      POS         ID REF ALT_1 ALT_2  QUAL  DP AA  NS   ...    AC_1  \\\n",
      "0    19      111          .   A     C         9.6  -1     -1   ...      -1   \n",
      "1    19      112          .   A     G        10.0  -1     -1   ...      -1   \n",
      "2    20    14370  rs6054257   G     A        29.0  14      3   ...      -1   \n",
      "3    20    17330          .   T     A         3.0  11      3   ...      -1   \n",
      "4    20  1110696  rs6040355   A     G     T  67.0  10  T   2   ...      -1   \n",
      "5    20  1230237          .   T              47.0  13  T   3   ...      -1   \n",
      "6    20  1234567  microsat1   G    GA   GAC  50.0   9  G   3   ...       3   \n",
      "7    20  1235237          .   T               NaN  -1     -1   ...      -1   \n",
      "8     X       10     rsTest  AC     A   ATG  10.0  -1     -1   ...      -1   \n",
      "\n",
      "   AC_2     DB  FILTER_PASS  FILTER_q10  FILTER_s50  numalt  svlen_1  svlen_2  \\\n",
      "0    -1  False        False       False       False       1        0        0   \n",
      "1    -1  False        False       False       False       1        0        0   \n",
      "2    -1   True         True       False       False       1        0        0   \n",
      "3    -1  False        False        True       False       1        0        0   \n",
      "4    -1   True         True       False       False       2        0        0   \n",
      "5    -1  False         True       False       False       0        0        0   \n",
      "6     1  False         True       False       False       2        1        2   \n",
      "7    -1  False        False       False       False       0        0        0   \n",
      "8    -1  False         True       False       False       2       -1        1   \n",
      "\n",
      "   is_snp  \n",
      "0    True  \n",
      "1    True  \n",
      "2    True  \n",
      "3    True  \n",
      "4    True  \n",
      "5   False  \n",
      "6   False  \n",
      "7   False  \n",
      "8   False  \n",
      "\n",
      "[9 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(df)\n",
    "\n",
    "# bam ! \n",
    "\n",
    "# i actually think this format is clearer ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CHROM</th>\n",
       "      <th>POS</th>\n",
       "      <th>ID</th>\n",
       "      <th>REF</th>\n",
       "      <th>ALT_1</th>\n",
       "      <th>ALT_2</th>\n",
       "      <th>QUAL</th>\n",
       "      <th>DP</th>\n",
       "      <th>AA</th>\n",
       "      <th>NS</th>\n",
       "      <th>...</th>\n",
       "      <th>AC_1</th>\n",
       "      <th>AC_2</th>\n",
       "      <th>DB</th>\n",
       "      <th>FILTER_PASS</th>\n",
       "      <th>FILTER_q10</th>\n",
       "      <th>FILTER_s50</th>\n",
       "      <th>numalt</th>\n",
       "      <th>svlen_1</th>\n",
       "      <th>svlen_2</th>\n",
       "      <th>is_snp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>14370</td>\n",
       "      <td>rs6054257</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td></td>\n",
       "      <td>29.0</td>\n",
       "      <td>14</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>1230237</td>\n",
       "      <td>.</td>\n",
       "      <td>T</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>47.0</td>\n",
       "      <td>13</td>\n",
       "      <td>T</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  CHROM      POS         ID REF ALT_1 ALT_2  QUAL  DP AA  NS   ...    AC_1  \\\n",
       "2    20    14370  rs6054257   G     A        29.0  14      3   ...      -1   \n",
       "5    20  1230237          .   T              47.0  13  T   3   ...      -1   \n",
       "\n",
       "   AC_2     DB  FILTER_PASS  FILTER_q10  FILTER_s50  numalt  svlen_1  svlen_2  \\\n",
       "2    -1   True         True       False       False       1        0        0   \n",
       "5    -1  False         True       False       False       0        0        0   \n",
       "\n",
       "   is_snp  \n",
       "2    True  \n",
       "5   False  \n",
       "\n",
       "[2 rows x 24 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# wow, you can query with SparkSQL and bendify anything you want !\n",
    "\n",
    "df.query('DP > 10 and QUAL > 20')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHROM,POS,DP\n",
      "19,111,-1\n",
      "19,112,-1\n",
      "20,14370,14\n",
      "20,17330,11\n",
      "20,1110696,10\n",
      "20,1230237,13\n",
      "20,1234567,9\n",
      "20,1235237,-1\n",
      "X,10,-1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### export as .csv\n",
    "\n",
    "allel.vcf_to_csv('C:/SPARK/sample.vcf', 'C:/SPARK/example.csv', fields=['CHROM', 'POS', 'DP'])\n",
    "\n",
    "\n",
    "with open('C:/SPARK/example.csv', mode='r') as f:\n",
    "    print(f.read())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### so you have thme file saved within a dataframe, you can now manipulate it with SQL commands if you wanted to ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Now lets pull down a massive .vcf file and process it ! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* http://hgdownload.cse.ucsc.edu/gbdb/hg19/1000Genomes/phase3/?C=S;O=A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# downloaded a 1.2 GB file of VCF raw data\n",
    "\n",
    "vcf_path = 'C:/SPARK/ALL.chr1.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 TBresee mkpasswd 1.2G Jun  4 19:33 C:/SPARK/ALL.chr1.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!ls -lh {vcf_path}\n",
    "\n",
    "# bam, a 1.2G file \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[read_vcf] 65536 rows in 3.84s; chunk in 3.84s (17061 rows/s); 1\u0000:2308933\n",
      "[read_vcf] 131072 rows in 7.73s; chunk in 3.89s (16854 rows/s); 1\u0000:4177969\n",
      "[read_vcf] 196608 rows in 11.76s; chunk in 4.03s (16259 rows/s); 1\u0000:6022445\n",
      "[read_vcf] 262144 rows in 15.71s; chunk in 3.95s (16610 rows/s); 1\u0000:8078286\n",
      "[read_vcf] 327680 rows in 19.71s; chunk in 4.00s (16377 rows/s); 1\u0000:10246876\n",
      "[read_vcf] 393216 rows in 23.48s; chunk in 3.77s (17383 rows/s); 1\u0000:12313599\n",
      "[read_vcf] 458752 rows in 27.44s; chunk in 3.96s (16551 rows/s); 1\u0000:15033300\n",
      "[read_vcf] 524288 rows in 31.36s; chunk in 3.93s (16690 rows/s); 1\u0000:17226235\n",
      "[read_vcf] 589824 rows in 35.13s; chunk in 3.76s (17415 rows/s); 1\u0000:19176875\n",
      "[read_vcf] 655360 rows in 38.90s; chunk in 3.77s (17364 rows/s); 1\u0000:21331176\n",
      "[read_vcf] 720896 rows in 42.78s; chunk in 3.88s (16876 rows/s); 1\u0000:23514706\n",
      "[read_vcf] 786432 rows in 46.65s; chunk in 3.87s (16955 rows/s); 1\u0000:25882976\n",
      "[read_vcf] 851968 rows in 50.38s; chunk in 3.73s (17584 rows/s); 1\u0000:28279507\n",
      "[read_vcf] 917504 rows in 54.15s; chunk in 3.77s (17364 rows/s); 1\u0000:30713629\n",
      "[read_vcf] 983040 rows in 58.04s; chunk in 3.89s (16845 rows/s); 1\u0000:32885037\n",
      "[read_vcf] 1048576 rows in 61.88s; chunk in 3.84s (17074 rows/s); 1\u0000:35172507\n",
      "[read_vcf] 1114112 rows in 65.59s; chunk in 3.71s (17665 rows/s); 1\u0000:37478330\n",
      "[read_vcf] 1179648 rows in 69.38s; chunk in 3.79s (17300 rows/s); 1\u0000:39748103\n",
      "[read_vcf] 1245184 rows in 73.25s; chunk in 3.88s (16897 rows/s); 1\u0000:42086852\n",
      "[read_vcf] 1310720 rows in 77.26s; chunk in 4.00s (16381 rows/s); 1\u0000:44445975\n",
      "[read_vcf] 1376256 rows in 81.11s; chunk in 3.85s (17007 rows/s); 1\u0000:46867070\n",
      "[read_vcf] 1441792 rows in 84.95s; chunk in 3.84s (17061 rows/s); 1\u0000:49152193\n",
      "[read_vcf] 1507328 rows in 88.75s; chunk in 3.80s (17259 rows/s); 1\u0000:51791580\n",
      "[read_vcf] 1572864 rows in 92.63s; chunk in 3.88s (16876 rows/s); 1\u0000:54133705\n",
      "[read_vcf] 1638400 rows in 96.41s; chunk in 3.78s (17337 rows/s); 1\u0000:56272802\n",
      "[read_vcf] 1703936 rows in 100.17s; chunk in 3.76s (17416 rows/s); 1\u0000:58498999\n",
      "[read_vcf] 1769472 rows in 103.91s; chunk in 3.73s (17560 rows/s); 1\u0000:60743161\n",
      "[read_vcf] 1835008 rows in 107.81s; chunk in 3.90s (16793 rows/s); 1\u0000:63047401\n",
      "[read_vcf] 1900544 rows in 111.65s; chunk in 3.85s (17043 rows/s); 1\u0000:65448230\n",
      "[read_vcf] 1966080 rows in 115.63s; chunk in 3.98s (16480 rows/s); 1\u0000:67804819\n",
      "[read_vcf] 2031616 rows in 119.38s; chunk in 3.75s (17485 rows/s); 1\u0000:70086719\n",
      "[read_vcf] 2097152 rows in 123.30s; chunk in 3.92s (16712 rows/s); 1\u0000:72517936\n",
      "[read_vcf] 2162688 rows in 127.19s; chunk in 3.89s (16863 rows/s); 1\u0000:74833059\n",
      "[read_vcf] 2228224 rows in 131.04s; chunk in 3.86s (16985 rows/s); 1\u0000:77144733\n",
      "[read_vcf] 2293760 rows in 134.81s; chunk in 3.76s (17411 rows/s); 1\u0000:79495341\n",
      "[read_vcf] 2359296 rows in 138.75s; chunk in 3.94s (16635 rows/s); 1\u0000:81601478\n",
      "[read_vcf] 2424832 rows in 142.63s; chunk in 3.88s (16871 rows/s); 1\u0000:83811587\n",
      "[read_vcf] 2490368 rows in 146.60s; chunk in 3.97s (16501 rows/s); 1\u0000:86217241\n",
      "[read_vcf] 2555904 rows in 150.39s; chunk in 3.79s (17314 rows/s); 1\u0000:88536030\n",
      "[read_vcf] 2621440 rows in 154.23s; chunk in 3.84s (17079 rows/s); 1\u0000:90958154\n",
      "[read_vcf] 2686976 rows in 158.31s; chunk in 4.08s (16047 rows/s); 1\u0000:93388079\n",
      "[read_vcf] 2752512 rows in 162.04s; chunk in 3.73s (17546 rows/s); 1\u0000:95785935\n",
      "[read_vcf] 2818048 rows in 165.82s; chunk in 3.77s (17365 rows/s); 1\u0000:98123341\n",
      "[read_vcf] 2883584 rows in 169.71s; chunk in 3.90s (16819 rows/s); 1\u0000:100426497\n",
      "[read_vcf] 2949120 rows in 173.58s; chunk in 3.87s (16950 rows/s); 1\u0000:102706814\n",
      "[read_vcf] 3014656 rows in 177.46s; chunk in 3.88s (16898 rows/s); 1\u0000:105066606\n",
      "[read_vcf] 3080192 rows in 181.27s; chunk in 3.81s (17200 rows/s); 1\u0000:107035468\n",
      "[read_vcf] 3145728 rows in 185.01s; chunk in 3.74s (17513 rows/s); 1\u0000:109497713\n",
      "[read_vcf] 3211264 rows in 189.09s; chunk in 4.08s (16047 rows/s); 1\u0000:111823987\n",
      "[read_vcf] 3276800 rows in 192.97s; chunk in 3.87s (16919 rows/s); 1\u0000:114123013\n",
      "[read_vcf] 3342336 rows in 196.74s; chunk in 3.77s (17397 rows/s); 1\u0000:116449117\n",
      "[read_vcf] 3407872 rows in 200.55s; chunk in 3.82s (17178 rows/s); 1\u0000:118807176\n",
      "[read_vcf] 3473408 rows in 204.64s; chunk in 4.09s (16024 rows/s); 1\u0000:121361045\n",
      "[read_vcf] 3538944 rows in 208.38s; chunk in 3.74s (17523 rows/s); 1\u0000:146730296\n",
      "[read_vcf] 3604480 rows in 212.12s; chunk in 3.74s (17518 rows/s); 1\u0000:150430552\n",
      "[read_vcf] 3670016 rows in 215.91s; chunk in 3.79s (17282 rows/s); 1\u0000:152825060\n",
      "[read_vcf] 3735552 rows in 219.92s; chunk in 4.00s (16373 rows/s); 1\u0000:155138015\n",
      "[read_vcf] 3801088 rows in 223.67s; chunk in 3.75s (17476 rows/s); 1\u0000:157518826\n",
      "[read_vcf] 3866624 rows in 227.52s; chunk in 3.85s (17003 rows/s); 1\u0000:159647947\n",
      "[read_vcf] 3932160 rows in 231.25s; chunk in 3.73s (17565 rows/s); 1\u0000:161935207\n",
      "[read_vcf] 3997696 rows in 235.24s; chunk in 3.99s (16434 rows/s); 1\u0000:164144117\n",
      "[read_vcf] 4063232 rows in 239.09s; chunk in 3.86s (16994 rows/s); 1\u0000:166317740\n",
      "[read_vcf] 4128768 rows in 242.92s; chunk in 3.82s (17146 rows/s); 1\u0000:168527049\n",
      "[read_vcf] 4194304 rows in 246.66s; chunk in 3.75s (17485 rows/s); 1\u0000:170801668\n",
      "[read_vcf] 4259840 rows in 250.80s; chunk in 4.13s (15849 rows/s); 1\u0000:173165318\n",
      "[read_vcf] 4325376 rows in 254.62s; chunk in 3.82s (17159 rows/s); 1\u0000:175588900\n",
      "[read_vcf] 4390912 rows in 258.40s; chunk in 3.78s (17323 rows/s); 1\u0000:177907681\n",
      "[read_vcf] 4456448 rows in 262.18s; chunk in 3.78s (17360 rows/s); 1\u0000:180243702\n",
      "[read_vcf] 4521984 rows in 266.02s; chunk in 3.84s (17047 rows/s); 1\u0000:182517611\n",
      "[read_vcf] 4587520 rows in 269.98s; chunk in 3.96s (16555 rows/s); 1\u0000:184881497\n",
      "[read_vcf] 4653056 rows in 273.87s; chunk in 3.89s (16837 rows/s); 1\u0000:187198495\n",
      "[read_vcf] 4718592 rows in 277.68s; chunk in 3.80s (17232 rows/s); 1\u0000:189298208\n",
      "[read_vcf] 4784128 rows in 281.51s; chunk in 3.83s (17110 rows/s); 1\u0000:191463222\n",
      "[read_vcf] 4849664 rows in 285.39s; chunk in 3.88s (16889 rows/s); 1\u0000:193813991\n",
      "[read_vcf] 4915200 rows in 289.15s; chunk in 3.76s (17425 rows/s); 1\u0000:195952433\n",
      "[read_vcf] 4980736 rows in 292.87s; chunk in 3.73s (17579 rows/s); 1\u0000:198270907\n",
      "[read_vcf] 5046272 rows in 296.77s; chunk in 3.90s (16815 rows/s); 1\u0000:200656420\n",
      "[read_vcf] 5111808 rows in 300.69s; chunk in 3.92s (16738 rows/s); 1\u0000:202897382\n",
      "[read_vcf] 5177344 rows in 304.42s; chunk in 3.73s (17560 rows/s); 1\u0000:205098886\n",
      "[read_vcf] 5242880 rows in 308.18s; chunk in 3.76s (17439 rows/s); 1\u0000:207870968\n",
      "[read_vcf] 5308416 rows in 311.96s; chunk in 3.78s (17323 rows/s); 1\u0000:210058821\n",
      "[read_vcf] 5373952 rows in 315.98s; chunk in 4.02s (16311 rows/s); 1\u0000:212310880\n",
      "[read_vcf] 5439488 rows in 319.87s; chunk in 3.89s (16845 rows/s); 1\u0000:214630869\n",
      "[read_vcf] 5505024 rows in 323.67s; chunk in 3.80s (17241 rows/s); 1\u0000:216886865\n",
      "[read_vcf] 5570560 rows in 327.54s; chunk in 3.87s (16915 rows/s); 1\u0000:219052919\n",
      "[read_vcf] 5636096 rows in 331.45s; chunk in 3.91s (16763 rows/s); 1\u0000:221366760\n",
      "[read_vcf] 5701632 rows in 335.29s; chunk in 3.84s (17087 rows/s); 1\u0000:223680945\n",
      "[read_vcf] 5767168 rows in 339.07s; chunk in 3.79s (17309 rows/s); 1\u0000:226104697\n",
      "[read_vcf] 5832704 rows in 342.83s; chunk in 3.75s (17467 rows/s); 1\u0000:228405212\n",
      "[read_vcf] 5898240 rows in 346.60s; chunk in 3.78s (17351 rows/s); 1\u0000:230542914\n",
      "[read_vcf] 5963776 rows in 350.51s; chunk in 3.91s (16772 rows/s); 1\u0000:232742459\n",
      "[read_vcf] 6029312 rows in 354.42s; chunk in 3.91s (16742 rows/s); 1\u0000:234886959\n",
      "[read_vcf] 6094848 rows in 358.19s; chunk in 3.76s (17425 rows/s); 1\u0000:237093353\n",
      "[read_vcf] 6160384 rows in 362.04s; chunk in 3.86s (16990 rows/s); 1\u0000:239206947\n",
      "[read_vcf] 6225920 rows in 366.01s; chunk in 3.96s (16530 rows/s); 1\u0000:241305636\n",
      "[read_vcf] 6291456 rows in 369.87s; chunk in 3.86s (16981 rows/s); 1\u0000:243454558\n",
      "[read_vcf] 6356992 rows in 373.59s; chunk in 3.72s (17617 rows/s); 1\u0000:245642248\n",
      "[read_vcf] 6422528 rows in 377.45s; chunk in 3.86s (16963 rows/s); 1\u0000:247641903\n",
      "[read_vcf] 6468094 rows in 380.23s; chunk in 2.78s (16376 rows/s)\n",
      "[read_vcf] all done (17010 rows/s)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "callset = allel.read_vcf(vcf_path, fields=['numalt'], log=sys.stdout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "When processing larger VCF files it’s useful to get some feedback on how fast things are going. \n",
    "\n",
    "Ultimately I am going to extract all the data from this VCF file into a Zarr store. \n",
    "However, before I do that, I’m going to check how many alternate alleles I should expect. \n",
    "I’m going to do that by extracting just the ‘numalt’ field, which scikit-allel will compute \n",
    "from the number of values in the ‘ALT’ field:\n",
    "``` \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# let’s see what the largest number of alternate alleles is:\n",
    "\n",
    "numalt = callset['variants/numalt']\n",
    "\n",
    "np.max(numalt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([      0, 6437262,   29538,    1064,     165,      49,      10,\n",
       "             5,       0,       0,       0,       0,       1], dtype=int64)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Out of interest, how many variants are multi-allelic?\n",
    "count_numalt = np.bincount(numalt)\n",
    "count_numalt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30832"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "n_multiallelic = np.sum(count_numalt[2:])\n",
    "n_multiallelic\n",
    "\n",
    "# So there are only a very small number of multi-allelic variants (30,832), the vast majority (6,437,262) \n",
    "# have just one alternate allele.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
