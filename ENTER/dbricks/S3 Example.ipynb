{"cells":[{"cell_type":"markdown","source":["## Overview\n\nThis notebook shows you how to create and query a table or DataFrame loaded from data stored in AWS S3. There are two ways to establish access to S3: [IAM roles](https://docs.databricks.com/user-guide/cloud-configurations/aws/iam-roles.html) and access keys.\n\n*We recommend using IAM roles to specify which cluster can access which buckets. Keys can show up in logs and table metadata and are therefore fundamentally insecure.* If you do use keys, you'll have to escape the `/` in your keys with `%2F`.\n\nThis is a **Python** notebook so the default cell type is Python. However, you can use different languages by using the `%LANGUAGE` magic command. Python, Scala, SQL, and R are all supported."],"metadata":{}},{"cell_type":"code","source":["# File location and type\nfile_location = \"{{upload_location}}\"\nfile_type = \"{{file_type}}\"\n\n# CSV options\ninfer_schema = \"{{infer_schema}}\"\nfirst_row_is_header = \"{{first_row_is_header}}\"\ndelimiter = \"{{delimiter}}\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndf = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)\n\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["# Create a view or table\n\ntemp_table_name = \"{{file_name}}\"\n\ndf.createOrReplaceTempView(temp_table_name)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["%sql\n\n/* Query the created temp table in a SQL cell */\n\nselect * from `{{file_name}}`"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# Since this table is registered as a temp view, it will only be available to this notebook. If you'd like other users to be able to query this table, you can also create a table from the DataFrame.\n# Once saved, this table will persist across cluster restarts as well as allow various users across different notebooks to query this data.\n# To do so, choose your table name and uncomment the bottom line.\n\npermanent_table_name = \"{{table_name}}\"\n\n# df.write.format(\"{{table_import_type}}\").saveAsTable(permanent_table_name)"],"metadata":{},"outputs":[],"execution_count":5}],"metadata":{"name":"2019-06-11 - S3 Example","notebookId":3688659401164374},"nbformat":4,"nbformat_minor":0}
