# Databricks notebook source
# MAGIC %md
# MAGIC <div class="alert alert-info" role="alert">
# MAGIC   <h3 class="alert-heading">Welcome</h3>
# MAGIC   <p>My name is Tom Bresee.</p>
# MAGIC   <p>We are going to go step by step and dive into Databricks</p>
# MAGIC   <hr>
# MAGIC   <p class="mb-0">Let's get started...</p>
# MAGIC </div>

# COMMAND ----------

# MAGIC %md <br>

# COMMAND ----------

# MAGIC %md
# MAGIC This is a function i create at the top to be able to show the methods that a particular function/object has.  In other words, if I have an object and want to list out all of the potential methods you can run on it, you would execute this function to get a list of the .methods and thus you would get an understanding at the high level of 'what you could do' with the object.  I find it very helpful. 

# COMMAND ----------

# return all of the methods possible under this particular input object
def list_out_object_methods(your_object):
  for method in dir(your_object):
    if not method.startswith("_"):
      print(method)
      
# i.e. i create a spark object, but i want to see all the methods possible on 
# it, use this function.  It helps to list out ALL methods you can enter ! 

# COMMAND ----------

# MAGIC %md <br>

# COMMAND ----------

# MAGIC %md
# MAGIC 
# MAGIC ```
# MAGIC READ:
# MAGIC 
# MAGIC This is worth noting, because it trips people up all the time.
# MAGIC 
# MAGIC Normally when we run vanilla Apache Spark, we create something like this:
# MAGIC 
# MAGIC from pyspark.sql import SparkSession
# MAGIC spark = SparkSession \ 
# MAGIC     .builder \  
# MAGIC     .appName("Python Spark SQL basic example") \  
# MAGIC     .config("spark.some.config.option", "some-value") \  
# MAGIC     .getOrCreate()  
# MAGIC 
# MAGIC 
# MAGIC This has been done for you, so that when you open up your jupyter notebook, the spark session 
# MAGIC has *already* been created.  So you won't actually see that .builder type command anywhere...
# MAGIC 
# MAGIC ```

# COMMAND ----------

print(type(spark))  # see ?  its already present...

# COMMAND ----------

# --- lets examine the spark instance ---
spark
#  < - - go to the gui left side, click cluster, and you can click on the UI button on that page

# COMMAND ----------

list_out_object_methods(spark)

# COMMAND ----------

# So looking at the above:
# I can enter spark.stop to stop the instance
# I can enter spark.read to start reading in inputs
# I can enter spark.createDataFrame to create DFs
# I don't need to, but i could enter spark.Builder type commands if this was vanilla spark

# COMMAND ----------

# --- what is my apache spark version ? ---
spark.version

# COMMAND ----------

list_out_object_methods(spark.Builder)

# COMMAND ----------

list_out_object_methods(spark.catalog)

# COMMAND ----------

# --- list my databases ---
spark.catalog.listDatabases()

# COMMAND ----------

# MAGIC %md
# MAGIC <div class="alert alert-block alert-info">
# MAGIC Most people don't know that the differenes between Spark 1.0 and Spark 2.0 are pretty significant.  Including the actual spark instance (SparkSession)<div>

# COMMAND ----------

# --- libraries ---
from pyspark.sql.functions import *

# COMMAND ----------

# MAGIC %md
# MAGIC <br>

# COMMAND ----------

# MAGIC %md ## The File System

# COMMAND ----------

display(dbutils.fs.ls("dbfs:/"))

# COMMAND ----------

# MAGIC %md
# MAGIC <div class="alert alert-info" role="alert">
# MAGIC   <h4 class="alert-heading">DBFS Root</h4>
# MAGIC   <p>The /FileStore contains imported data files, generated plots, and uploaded libraries.  The /databricks-datasets contains sample public datasets.  The /databricks-results contains files generated by downloading the full results of a query.</p>
# MAGIC </div>

# COMMAND ----------

display(dbutils.fs.ls("dbfs:/FileStore"))
# FileStore stores uploaded data and jars etc 

# COMMAND ----------

display(dbutils.fs.ls("/databricks-datasets/samples/docs/"))

# COMMAND ----------

# --- databricks comes with some cool datasets ---
display(dbutils.fs.ls("dbfs:/databricks-datasets"))

# COMMAND ----------

display(dbutils.fs.ls("dbfs:/databricks-datasets/learning-spark-v2"))

# COMMAND ----------

display(dbutils.fs.ls("dbfs:/databricks-datasets/learning-spark/data-001"))

# COMMAND ----------

display(dbutils.fs.ls("dbfs:/databricks-datasets/"))

# COMMAND ----------

# --- The Definitive Guide is an O'Reilly book, very good ---
display(dbutils.fs.ls("dbfs:/databricks-datasets/definitive-guide/data"))

# COMMAND ----------

display(dbutils.fs.ls("/databricks-datasets/samples/data/mllib"))

# COMMAND ----------

# --- lets read a random README.md ---
with open("/dbfs/databricks-datasets/README.md") as f:
    x = ''.join(f.readlines())
print(x)

# COMMAND ----------

# dbutils.fs provides file-system-like commands to access files in DBFS
list_out_object_methods(dbutils.fs):
# you can mkdirs, put files, 'head', remove, 

# COMMAND ----------

#write a file to DBFS using Python I/O APIs
with open("/dbfs/tmp/test_dbfs.txt", 'w') as f:
  f.write("Apache Spark is awesome!\n")
  f.write("This is fun\n")
  f.write("End of example!")

# COMMAND ----------

# read the file
with open("/dbfs/tmp/test_dbfs.txt", "r") as f_read:
  for line in f_read:
    print (line)

# COMMAND ----------

# MAGIC %md
# MAGIC <br>

# COMMAND ----------

# MAGIC %md ## Quick Viz

# COMMAND ----------

# note that you can flip between python, R, Scala, and direct SQL

# COMMAND ----------

import seaborn as sns
sns.set(style="white")

df = sns.load_dataset("iris")
g = sns.PairGrid(df, diag_sharey=False)
g.map_lower(sns.kdeplot)
g.map_diag(sns.kdeplot, lw=3)

g.map_upper(sns.regplot)

display(g.fig)

# COMMAND ----------

# MAGIC %r
# MAGIC install.packages("DandEFA", repos = "http://cran.us.r-project.org")
# MAGIC library(DandEFA)
# MAGIC data(timss2011)
# MAGIC timss2011 <- na.omit(timss2011)
# MAGIC dandpal <- rev(rainbow(100, start = 0, end = 0.2))
# MAGIC facl <- factload(timss2011,nfac=5,method="prax",cormeth="spearman")
# MAGIC dandelion(facl,bound=0,mcex=c(1,1.2),palet=dandpal)
# MAGIC facl <- factload(timss2011,nfac=8,method="mle",cormeth="pearson")
# MAGIC dandelion(facl,bound=0,mcex=c(1,1.2),palet=dandpal)

# COMMAND ----------

# MAGIC %r
# MAGIC library(ggplot2)
# MAGIC ggplot(diamonds, aes(carat, price, color = color, group = 1)) + geom_point(alpha = 0.3) + stat_smooth()

# COMMAND ----------

#  if you register a spark DataFrame as a table, you can also query it with SQL to create SQL visualizations...

# COMMAND ----------

# MAGIC %md
# MAGIC <br>

# COMMAND ----------

# MAGIC %md
# MAGIC ## Interacting with Spark

# COMMAND ----------

display(dbutils.fs.ls("dbfs:/databricks-datasets/Rdatasets/data-001"))

# COMMAND ----------

# --- import data, and create dataframe ---
# use the standard 'diamonds.csv' dataset...

sparkDF = spark.read.format('csv').\
          options(header='true', inferSchema='true').\
          load('/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv')

# COMMAND ----------

# MAGIC %md
# MAGIC <div class="alert alert-block alert-warning">
# MAGIC <b>Note:</b> A lot just happened here. We imported (loaded) the dataset, inferred the schema it was using (such as strings or numbers/values), used its existing 'header', and 'read' in the data to a dataframe.  At this point, its 'queryable'...
# MAGIC </div>

# COMMAND ----------

# --- list out all the methods possible on this DF ---
list_out_object_methods(sparkDF)

# COMMAND ----------

print(type(sparkDF))
# this is in fact a spark DataFrame

# COMMAND ----------

display(sparkDF.describe())

# COMMAND ----------

# MAGIC %md
# MAGIC <div class="alert alert-block alert-info">
# MAGIC A Databricks database is a collection of tables. A Databricks table is a collection of structured data. Tables are equivalent to Apache Spark DataFrames. This means that you can cache, filter, and perform any operations supported by DataFrames on tables. You can query tables with Spark APIs and Spark SQL.
# MAGIC </div>

# COMMAND ----------

help(sparkDF.createOrReplaceTempView)

# COMMAND ----------

sparkDF.describe()

# COMMAND ----------

help(sparkDF.write.saveAsTable)

# COMMAND ----------

# MAGIC %sql
# MAGIC /* I can now access this Table via straight SQL */
# MAGIC SELECT * FROM diamonds LIMIT(6)

# COMMAND ----------

# MAGIC %md
# MAGIC <div class="alert alert-block alert-info">
# MAGIC <b>Important:</b> Once table is in a databricks Table, it can be queried with any language you want, and especially we enter the domain of SQL where you can use straight standard SQL commands to go nuts...
# MAGIC </div>

# COMMAND ----------

# MAGIC %sql
# MAGIC /* query  */
# MAGIC SELECT carat,cut,clarity,price FROM diamonds WHERE price>300 AND clarity='SI1' AND carat>.25 LIMIT(7)
# MAGIC /* this is a very powerful feature */

# COMMAND ----------

# We can also use spark.sql as well to query the DF
# The spark.sql(" - verbage - ") is treated as if its SQL command...
display(spark.sql("SELECT carat,cut,color,clarity FROM diamonds LIMIT(7)"))

# COMMAND ----------

# this has been a good intro, lets keep going in the next notebook...
