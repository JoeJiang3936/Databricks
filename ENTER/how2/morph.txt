



tbresee@TM0493322:~$ pyspark --version
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.4.3
      /_/

Using Scala version 2.11.12, OpenJDK 64-Bit Server VM, 1.8.0_222
Branch
Compiled by user  on 2019-05-01T05:08:38Z
Revision
Url
Type --help for more information.






import to know how to start:

tbresee@TM0493322:~$ pyspark --help
Usage: ./bin/pyspark [options]

Options:
  --master MASTER_URL         spark://host:port, mesos://host:port, yarn,
                              k8s://https://host:port, or local (Default: local[*]).
  --deploy-mode DEPLOY_MODE   Whether to launch the driver program locally ("client") or
                              on one of the worker machines inside the cluster ("cluster")
                              (Default: client).
  --class CLASS_NAME          Your application's main class (for Java / Scala apps).
  --name NAME                 A name of your application.
  --jars JARS                 Comma-separated list of jars to include on the driver
                              and executor classpaths.
  --packages                  Comma-separated list of maven coordinates of jars to include
                              on the driver and executor classpaths. Will search the local
                              maven repo, then maven central and any additional remote
                              repositories given by --repositories. The format for the
                              coordinates should be groupId:artifactId:version.
  --exclude-packages          Comma-separated list of groupId:artifactId, to exclude while
                              resolving the dependencies provided in --packages to avoid
                              dependency conflicts.
  --repositories              Comma-separated list of additional remote repositories to
                              search for the maven coordinates given with --packages.
  --py-files PY_FILES         Comma-separated list of .zip, .egg, or .py files to place
                              on the PYTHONPATH for Python apps.
  --files FILES               Comma-separated list of files to be placed in the working
                              directory of each executor. File paths of these files
                              in executors can be accessed via SparkFiles.get(fileName).

  --conf PROP=VALUE           Arbitrary Spark configuration property.
  --properties-file FILE      Path to a file from which to load extra properties. If not
                              specified, this will look for conf/spark-defaults.conf.

  --driver-memory MEM         Memory for driver (e.g. 1000M, 2G) (Default: 1024M).
  --driver-java-options       Extra Java options to pass to the driver.
  --driver-library-path       Extra library path entries to pass to the driver.
  --driver-class-path         Extra class path entries to pass to the driver. Note that
                              jars added with --jars are automatically included in the
                              classpath.

  --executor-memory MEM       Memory per executor (e.g. 1000M, 2G) (Default: 1G).

  --proxy-user NAME           User to impersonate when submitting the application.
                              This argument does not work with --principal / --keytab.

  --help, -h                  Show this help message and exit.
  --verbose, -v               Print additional debug output.
  --version,                  Print the version of current Spark.

 Cluster deploy mode only:
  --driver-cores NUM          Number of cores used by the driver, only in cluster mode
                              (Default: 1).

 Spark standalone or Mesos with cluster deploy mode only:
  --supervise                 If given, restarts the driver on failure.
  --kill SUBMISSION_ID        If given, kills the driver specified.
  --status SUBMISSION_ID      If given, requests the status of the driver specified.

 Spark standalone and Mesos only:
  --total-executor-cores NUM  Total cores for all executors.

 Spark standalone and YARN only:
  --executor-cores NUM        Number of cores per executor. (Default: 1 in YARN mode,
                              or all available cores on the worker in standalone mode)

 YARN-only:
  --queue QUEUE_NAME          The YARN queue to submit to (Default: "default").
  --num-executors NUM         Number of executors to launch (Default: 2).
                              If dynamic allocation is enabled, the initial number of
                              executors will be at least NUM.
  --archives ARCHIVES         Comma separated list of archives to be extracted into the
                              working directory of each executor.
  --principal PRINCIPAL       Principal to be used to login to KDC, while running on
                              secure HDFS.
  --keytab KEYTAB             The full path to the file that contains the keytab for the
                              principal specified above. This keytab will be copied to
                              the node running the Application Master via the Secure
                              Distributed Cache, for renewing the login tickets and the
                              delegation tokens periodically.












~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Ubuntu:

	hail is located within  ./.local/lib/python3.6/site-packages


The latest hail version in  my case (August 2019) is 

	0.2.19
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~










~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Reinstalling to show you locations and other information:


tbresee@TM0493322:~$ pip3 install --user hail
Requirement already satisfied: hail in ./.local/lib/python3.6/site-packages (0.2.19)
Requirement already satisfied: python-json-logger==0.1.11 in ./.local/lib/python3.6/site-packages (from hail) (0.1.11)
Requirement already satisfied: bokeh<1.3,>1.1 in ./.local/lib/python3.6/site-packages (from hail) (1.2.0)
Requirement already satisfied: ipykernel<5 in ./.local/lib/python3.6/site-packages (from hail) (4.10.1)
Requirement already satisfied: nest-asyncio in ./.local/lib/python3.6/site-packages (from hail) (1.0.0)
Requirement already satisfied: tabulate==0.8.3 in ./.local/lib/python3.6/site-packages (from hail) (0.8.3)
Requirement already satisfied: requests<2.21.1,>=2.21.0 in ./.local/lib/python3.6/site-packages (from hail) (2.21.0)
Collecting pyspark<2.4.2,>=2.4 (from hail)
Requirement already satisfied: parsimonious<0.9 in ./.local/lib/python3.6/site-packages (from hail) (0.8.1)
Requirement already satisfied: scipy<1.4,>1.2 in ./.local/lib/python3.6/site-packages (from hail) (1.3.0)
Requirement already satisfied: decorator<5 in ./.local/lib/python3.6/site-packages (from hail) (4.4.0)
Requirement already satisfied: hurry.filesize==0.9 in ./.local/lib/python3.6/site-packages (from hail) (0.9)
Requirement already satisfied: aiohttp in ./.local/lib/python3.6/site-packages (from hail) (3.5.4)
Requirement already satisfied: PyJWT in ./.local/lib/python3.6/site-packages (from hail) (1.7.1)
Requirement already satisfied: numpy<2 in ./.local/lib/python3.6/site-packages (from hail) (1.17.0)
Requirement already satisfied: pandas<0.24,>0.22 in ./.local/lib/python3.6/site-packages (from hail) (0.23.4)
Requirement already satisfied: gcsfs==0.2.1 in ./.local/lib/python3.6/site-packages (from hail) (0.2.1)
Requirement already satisfied: tornado>=4.3 in ./.local/lib/python3.6/site-packages (from bokeh<1.3,>1.1->hail) (6.0.3)
Requirement already satisfied: packaging>=16.8 in ./.local/lib/python3.6/site-packages (from bokeh<1.3,>1.1->hail) (19.1)
Requirement already satisfied: six>=1.5.2 in ./.local/lib/python3.6/site-packages (from bokeh<1.3,>1.1->hail) (1.12.0)
Requirement already satisfied: PyYAML>=3.10 in ./.local/lib/python3.6/site-packages (from bokeh<1.3,>1.1->hail) (5.1.2)
Requirement already satisfied: pillow>=4.0 in ./.local/lib/python3.6/site-packages (from bokeh<1.3,>1.1->hail) (6.1.0)
Requirement already satisfied: Jinja2>=2.7 in ./.local/lib/python3.6/site-packages (from bokeh<1.3,>1.1->hail) (2.10.1)
Requirement already satisfied: python-dateutil>=2.1 in ./.local/lib/python3.6/site-packages (from bokeh<1.3,>1.1->hail) (2.8.0)
Requirement already satisfied: ipython>=4.0.0 in ./.local/lib/python3.6/site-packages (from ipykernel<5->hail) (7.7.0)
Requirement already satisfied: jupyter-client in ./.local/lib/python3.6/site-packages (from ipykernel<5->hail) (5.3.1)
Requirement already satisfied: traitlets>=4.1.0 in ./.local/lib/python3.6/site-packages (from ipykernel<5->hail) (4.3.2)
Requirement already satisfied: chardet<3.1.0,>=3.0.2 in ./.local/lib/python3.6/site-packages (from requests<2.21.1,>=2.21.0->hail) (3.0.4)
Requirement already satisfied: certifi>=2017.4.17 in ./.local/lib/python3.6/site-packages (from requests<2.21.1,>=2.21.0->hail) (2019.6.16)
Requirement already satisfied: urllib3<1.25,>=1.21.1 in ./.local/lib/python3.6/site-packages (from requests<2.21.1,>=2.21.0->hail) (1.24.3)
Requirement already satisfied: idna<2.9,>=2.5 in ./.local/lib/python3.6/site-packages (from requests<2.21.1,>=2.21.0->hail) (2.8)
Requirement already satisfied: py4j==0.10.7 in ./.local/lib/python3.6/site-packages (from pyspark<2.4.2,>=2.4->hail) (0.10.7)
Requirement already satisfied: setuptools in ./.local/lib/python3.6/site-packages (from hurry.filesize==0.9->hail) (41.0.1)
Requirement already satisfied: yarl<2.0,>=1.0 in ./.local/lib/python3.6/site-packages (from aiohttp->hail) (1.3.0)
Requirement already satisfied: typing-extensions>=3.6.5; python_version < "3.7" in ./.local/lib/python3.6/site-packages (from aiohttp->hail) (3.7.4)
Requirement already satisfied: idna-ssl>=1.0; python_version < "3.7" in ./.local/lib/python3.6/site-packages (from aiohttp->hail) (1.1.0)
Requirement already satisfied: async-timeout<4.0,>=3.0 in ./.local/lib/python3.6/site-packages (from aiohttp->hail) (3.0.1)
Requirement already satisfied: attrs>=17.3.0 in ./.local/lib/python3.6/site-packages (from aiohttp->hail) (19.1.0)
Requirement already satisfied: multidict<5.0,>=4.0 in ./.local/lib/python3.6/site-packages (from aiohttp->hail) (4.5.2)
Requirement already satisfied: pytz>=2011k in ./.local/lib/python3.6/site-packages (from pandas<0.24,>0.22->hail) (2019.2)
Requirement already satisfied: google-auth>=1.2 in ./.local/lib/python3.6/site-packages (from gcsfs==0.2.1->hail) (1.6.3)
Requirement already satisfied: google-auth-oauthlib in ./.local/lib/python3.6/site-packages (from gcsfs==0.2.1->hail) (0.4.0)
Requirement already satisfied: pyparsing>=2.0.2 in ./.local/lib/python3.6/site-packages (from packaging>=16.8->bokeh<1.3,>1.1->hail) (2.4.2)
Requirement already satisfied: MarkupSafe>=0.23 in ./.local/lib/python3.6/site-packages (from Jinja2>=2.7->bokeh<1.3,>1.1->hail) (1.1.1)
Requirement already satisfied: pygments in ./.local/lib/python3.6/site-packages (from ipython>=4.0.0->ipykernel<5->hail) (2.4.2)
Requirement already satisfied: jedi>=0.10 in ./.local/lib/python3.6/site-packages (from ipython>=4.0.0->ipykernel<5->hail) (0.14.1)
Requirement already satisfied: pexpect; sys_platform != "win32" in ./.local/lib/python3.6/site-packages (from ipython>=4.0.0->ipykernel<5->hail) (4.7.0)
Requirement already satisfied: backcall in ./.local/lib/python3.6/site-packages (from ipython>=4.0.0->ipykernel<5->hail) (0.1.0)
Requirement already satisfied: pickleshare in ./.local/lib/python3.6/site-packages (from ipython>=4.0.0->ipykernel<5->hail) (0.7.5)
Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in ./.local/lib/python3.6/site-packages (from ipython>=4.0.0->ipykernel<5->hail) (2.0.9)
Requirement already satisfied: pyzmq>=13 in ./.local/lib/python3.6/site-packages (from jupyter-client->ipykernel<5->hail) (18.1.0)
Requirement already satisfied: jupyter-core in ./.local/lib/python3.6/site-packages (from jupyter-client->ipykernel<5->hail) (4.5.0)
Requirement already satisfied: ipython-genutils in ./.local/lib/python3.6/site-packages (from traitlets>=4.1.0->ipykernel<5->hail) (0.2.0)
Requirement already satisfied: pyasn1-modules>=0.2.1 in ./.local/lib/python3.6/site-packages (from google-auth>=1.2->gcsfs==0.2.1->hail) (0.2.6)
Requirement already satisfied: rsa>=3.1.4 in ./.local/lib/python3.6/site-packages (from google-auth>=1.2->gcsfs==0.2.1->hail) (4.0)
Requirement already satisfied: cachetools>=2.0.0 in ./.local/lib/python3.6/site-packages (from google-auth>=1.2->gcsfs==0.2.1->hail) (3.1.1)
Requirement already satisfied: requests-oauthlib>=0.7.0 in ./.local/lib/python3.6/site-packages (from google-auth-oauthlib->gcsfs==0.2.1->hail) (1.2.0)
Requirement already satisfied: parso>=0.5.0 in ./.local/lib/python3.6/site-packages (from jedi>=0.10->ipython>=4.0.0->ipykernel<5->hail) (0.5.1)
Requirement already satisfied: ptyprocess>=0.5 in ./.local/lib/python3.6/site-packages (from pexpect; sys_platform != "win32"->ipython>=4.0.0->ipykernel<5->hail) (0.6.0)
Requirement already satisfied: wcwidth in ./.local/lib/python3.6/site-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=4.0.0->ipykernel<5->hail) (0.1.7)
Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in ./.local/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs==0.2.1->hail) (0.4.6)
Requirement already satisfied: oauthlib>=3.0.0 in ./.local/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs==0.2.1->hail) (3.1.0)
Installing collected packages: pyspark
Successfully installed pyspark-2.4.3
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~









~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
looking around at the installation


tbresee@TM0493322:~$ cd ./.local/lib/python3.6/site-packages
tbresee@TM0493322:~/.local/lib/python3.6/site-packages$ ll
total 924
drwx------ 1 tbresee tbresee   4096 Aug 12 14:59 ./
drwx------ 1 tbresee tbresee   4096 Aug  7 10:23 ../
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:23 aiohttp/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 google_auth_oauthlib-0.4.0.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:24 hail/       < - - - - - - - 
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 hail-0.2.19.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:24 hailtop/




tbresee@TM0493322:~/.local/lib/python3.6/site-packages$ cd hail
tbresee@TM0493322:~/.local/lib/python3.6/site-packages/hail$ ll
total 32744
drwxrwxrwx 1 tbresee tbresee     4096 Aug  7 10:24 ./
drwx------ 1 tbresee tbresee     4096 Aug 12 14:59 ../
drwxrwxrwx 1 tbresee tbresee     4096 Aug  7 10:24 backend/
-rw-rw-rw- 1 tbresee tbresee     4972 Aug  8 11:34 conftest.py
-rw-rw-rw- 1 tbresee tbresee    14104 Aug  8 11:34 context.py
drwxrwxrwx 1 tbresee tbresee     4096 Aug  7 10:24 experimental/
drwxrwxrwx 1 tbresee tbresee     4096 Aug  7 10:24 expr/
drwxrwxrwx 1 tbresee tbresee     4096 Aug  7 10:24 fs/
drwxrwxrwx 1 tbresee tbresee     4096 Aug  7 10:24 genetics/
-rw-rw-rw- 1 tbresee tbresee 33229607 Aug  8 11:34 hail-all-spark.jar  < - - - - - - - ! 
-rw-rw-rw- 1 tbresee tbresee        6 Aug  8 11:34 hail_pip_version
-rw-rw-rw- 1 tbresee tbresee       19 Aug  8 11:34 hail_version
-rw-rw-rw- 1 tbresee tbresee     2453 Aug  8 11:34 __init__.py
drwxrwxrwx 1 tbresee tbresee     4096 Aug  7 10:24 ir/
drwxrwxrwx 1 tbresee tbresee     4096 Aug  7 10:24 linalg/
-rw-rw-rw- 1 tbresee tbresee   149224 Aug  8 11:34 matrixtable.py
drwxrwxrwx 1 tbresee tbresee     4096 Aug  7 10:24 methods/
drwxrwxrwx 1 tbresee tbresee     4096 Aug  7 10:24 plot/
drwxrwxrwx 1 tbresee tbresee     4096 Aug  7 10:24 __pycache__/
drwxrwxrwx 1 tbresee tbresee     4096 Aug  7 10:24 stats/
-rw-rw-rw- 1 tbresee tbresee   116557 Aug  8 11:34 table.py
drwxrwxrwx 1 tbresee tbresee     4096 Aug  7 10:24 typecheck/
drwxrwxrwx 1 tbresee tbresee     4096 Aug  7 10:24 utils/
tbresee@TM0493322:~/.local/lib/python3.6/site-packages/hail$






tbresee@TM0493322:~/.local/lib$ cd ~
tbresee@TM0493322:~$ pwd
/home/tbresee
tbresee@TM0493322:~$ cd hail
tbresee@TM0493322:~/hail$ ll
total 92
drwxrwxrwx 1 tbresee tbresee  4096 Aug  5 10:00 ./
drwxr-xr-x 1 tbresee tbresee  4096 Aug 12 15:05 ../
-rw-rw-rw- 1 tbresee tbresee    38 Aug  5 09:31 acknowledgements.txt
-rw-rw-rw- 1 tbresee tbresee   448 Aug  5 09:31 AUTHORS
drwxrwxrwx 1 tbresee tbresee  4096 Aug  5 10:00 build/
-rw-rw-rw- 1 tbresee tbresee 19095 Aug  5 09:31 build.gradle
-rw-rw-rw- 1 tbresee tbresee  7264 Aug  5 09:31 changes.md
-rw-rw-rw- 1 tbresee tbresee 17207 Aug  5 09:31 code_style.xml
-rw-rw-rw- 1 tbresee tbresee     6 Aug  5 09:31 deployed-spark-versions.txt
-rw-rw-rw- 1 tbresee tbresee   307 Aug  5 09:31 .dockerignore
-rw-rw-rw- 1 tbresee tbresee   393 Aug  5 09:31 generate-build-info.sh
-rw-rw-rw- 1 tbresee tbresee   631 Aug  5 09:31 generate-dist-links.sh
drwxrwxrwx 1 tbresee tbresee  4096 Aug  5 09:31 .git/
-rw-rw-rw- 1 tbresee tbresee   165 Aug  5 09:31 .gitignore
drwxrwxrwx 1 tbresee tbresee  4096 Aug  5 09:31 gradle/
drwxrwxrwx 1 tbresee tbresee  4096 Aug  5 09:59 .gradle/
-rwxrwxrwx 1 tbresee tbresee  5046 Aug  5 09:31 gradlew*
-rw-rw-rw- 1 tbresee tbresee  2404 Aug  5 09:31 gradlew.bat
-rw-rw-rw- 1 tbresee tbresee    79 Aug  5 09:31 hail-ci-build-image
-rw-rw-rw- 1 tbresee tbresee   937 Aug  5 09:31 hail-ci-build.sh
-rw-rw-rw- 1 tbresee tbresee  1842 Aug  5 09:31 hail-ci-deploy.sh
-rw-rw-rw- 1 tbresee tbresee   277 Aug  5 09:31 hail-docs-trampoline.sh
-rw-rw-rw- 1 tbresee tbresee  1102 Aug  5 09:31 LICENSE
-rw-rw-rw- 1 tbresee tbresee   972 Aug  5 09:31 Makefile
drwxrwxrwx 1 tbresee tbresee  4096 Aug  5 09:31 pr-builder/
drwxrwxrwx 1 tbresee tbresee  4096 Aug  5 09:31 python/
-rw-rw-rw- 1 tbresee tbresee  5855 Aug  5 09:31 README.md
drwxrwxrwx 1 tbresee tbresee  4096 Aug  5 09:31 scripts/
-rw-rw-rw- 1 tbresee tbresee    23 Aug  5 09:31 settings.gradle
drwxrwxrwx 1 tbresee tbresee  4096 Aug  5 09:31 src/
-rw-rw-rw- 1 tbresee tbresee  1862 Aug  5 09:31 style-guide.md
-rw-rw-rw- 1 tbresee tbresee   164 Aug  5 09:31 testng.xml
drwxrwxrwx 1 tbresee tbresee  4096 Aug  5 09:31 www/






You really need to make sure these are right

I may not have them perfect...

tbresee@TM0493322:~/hail$ printenv
LS_COLORS=rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:
HOSTTYPE=x86_64
LESSCLOSE=/usr/bin/lesspipe %s %s
HADOOP_HOME=/home/tbresee/hadoop/hadoop-3.2.0
LANG=en_US.UTF-8
JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64   < - - - 1.8 ! 
USER=tbresee
PWD=/home/tbresee/hail
HOME=/home/tbresee
XDG_DATA_DIRS=/usr/local/share:/usr/share:/var/lib/snapd/desktop
SPARK_HOME=/home/tbresee/hadoop/spark-2.4.3-bin-hadoop2.7   < - - - - - - - 
HAIL_HOME=/home/tbresee/hail  < - - - i think this is wrong ! 
SHELL=/bin/bash
TERM=xterm-256color
DOCKER_HOST=tcp://localhost:2375
SPARK_CLASSPATH=/home/tbresee/hail/build/libs/hail-all-spark.jar
PYSPARK_DRIVER_PYTHON=jupyter    < - - - - - - 
SHLVL=1
PYTHONPATH=/home/tbresee/hadoop/spark-2.4.3-bin-hadoop2.7/python:    < - - - - need more ??????
PYSPARK_DRIVER_PYTHON_OPTS=notebook   < - - - - - 
PYSPARK_PYTHON=ipython	  < - - - - - - 
PATH=/home/tbresee/bin:/home/tbresee/.local/bin:/home/tbresee/.local/bin:/home/tbresee/hadoop/spark-2.4.3-bin-hadoop2.7/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/mnt/c/Program Files (x86)/Common Files/Oracle/Java/javapath:/mnt/c/ProgramData/DockerDesktop/version-bin:/mnt/c/Program Files/Docker/Docker/resources/bin:/mnt/c/JAVA/bin:/mnt/c/Program Files (x86)/Microsoft SDKs/Azure/CLI2/wbin:/mnt/c/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0/bin:/mnt/c/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0/libnvvp:/mnt/c/Windows/System32:/mnt/c/Windows:/mnt/c/Windows/System32/wbem:/mnt/c/Windows/System32/WindowsPowerShell/v1.0:/mnt/c/Program Files (x86)/Symantec/VIP Access Client:/mnt/c/Program Files/Intel/WiFi/bin:/mnt/c/Program Files/Common Files/Intel/WirelessCommon:/mnt/c/Program Files/Microsoft VS Code/bin:/mnt/c/Program Files/Git/cmd:/mnt/c/Program Files (x86)/Pandoc:/mnt/c/Program Files (x86)/Graphviz2.38:/mnt/c/Program Files (x86)/Graphviz2.38/bin:/mnt/c/Program Files (x86)/Graphviz2.38/bin/dot.exe:/mnt/c/Program Files (x86)/NVIDIA Corporation/PhysX/Common:/mnt/c/Program Files/Amazon/AWSCLI/bin:/mnt/c/Program Files (x86)/Webex/Webex/Applications:/mnt/c/Program Files (x86)/Microsoft SQL Server/Client SDK/ODBC/130/Tools/Binn:/mnt/c/Program Files (x86)/Microsoft SQL Server/140/Tools/Binn:/mnt/c/Program Files (x86)/Microsoft SQL Server/140/DTS/Binn:/mnt/c/Program Files (x86)/Microsoft SQL Server/140/Tools/Binn/ManagementStudio:/mnt/c/Users/tbresee/Documents/chromedriver.exe:/mnt/c/Program Files/PuTTY:/mnt/c/Program Files/SASHome/SASFoundation/9.4/ets/sasexe:/mnt/c/Program Files/SASHome/Secure/ccme4:/mnt/c/Program Files/SASHome/x86/Secure/ccme4:/mnt/c/Program Files/MySQL/MySQL Utilities 1.6:/mnt/c/SPARK/bin:/mnt/c/Users/tbresee/Spark:/mnt/c/Program Files (x86)/scala/bin:/mnt/c/SPARK/hadoop:/mnt/c/HADOOP/hadoop-3.1.2/sbin:/mnt/c/SPARK/hadoop:/mnt/c/HADOOP/hadoop-2.7.1/bin:/mnt/c/SCALA/bin:/mnt/c/SPARK/bin:/mnt/c/MAVEN/bin:/mnt/c/SBT/bin:/mnt/c/Windows/System32:/mnt/c/Windows:/mnt/c/Windows/System32/wbem:/mnt/c/Windows/System32/WindowsPowerShell/v1.0:/mnt/c/Windows/System32/OpenSSH:/mnt/c/NODEJS:/mnt/c/Users/tbresee/AppData/Local/Programs/Python/Python37/Scripts:/mnt/c/Users/tbresee/AppData/Local/Programs/Python/Python37:/mnt/c/Users/tbresee/AppData/Local/Microsoft/WindowsApps:/mnt/c/Users/tbresee/AppData/Local/GitHubDesktop/bin:/mnt/c/Users/tbresee/AppData/Local/atom/bin:/mnt/c/Users/tbresee/AppData/Local/Continuum/anaconda3:/mnt/c/Users/tbresee/AppData/Local/Continuum/anaconda3/Scripts:/mnt/c/Users/tbresee/AppData/Local/Continuum/anaconda3/Library/bin:/mnt/c/Program Files/Amazon/AWSCLI:/mnt/c/Users/tbresee/AppData/Local/Programs/MiKTeX 2.9/miktex/bin/x64:/mnt/c/Program Files (x86)/Nmap:/mnt/c/HADOOP/hadoop-2.7.1:/mnt/c/HADOOP/hadoop-2.7.1:/mnt/c/Users/tbresee/AppData/Local/Microsoft/WindowsApps:/mnt/c/Users/tbresee/AppData/Roaming/npm:/mnt/c/Program Files (x86)/Nmap:/snap/bin
WSLENV=
LESSOPEN=| /usr/bin/lesspipe %s
_=/usr/bin/printenv





Remember:  If you are running Hail in local mode, you won't have access to the HDFS file scheme. (but cluster mode does)



would make cluster mode work:
PYSPARK_PYTHON="ipython" pyspark --conf spark.sql.files.openCostInBytes=1099511627776 --conf spark.sql.files.maxPartitionBytes=1099511627776 --conf spark.hadoop.parquet.block.size=1099511627776 --conf spark.serializer=org.apache.spark.serializer.KryoSerializer




SPARK_HOME=/home/tbresee/hadoop/spark-2.4.3-bin-hadoop2.7





export HAIL_HOME=/path/to/hail/

export PYTHONPATH="${PYTHONPATH:+$PYTHONPATH:}$HAIL_HOME/build/distributions/hail-python.zip"

export PYTHONPATH="$PYTHONPATH:$SPARK_HOME/python"

export PYTHONPATH="$PYTHONPATH:$SPARK_HOME/python/lib/py4j-*-src.zip"




export PYSPARK_SUBMIT_ARGS="\
  --jars $HAIL_HOME/build/libs/hail-all-spark.jar \
  --conf spark.driver.extraClassPath=\"$HAIL_HOME/build/libs/hail-all-spark.jar\" \
  --conf spark.executor.extraClassPath=./hail-all-spark.jar \
  --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
  --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator
  pyspark-shell"












make sure you resource this if you change it ...

tbresee@TM0493322:~$ cat ~/.bashrc
# ~/.bashrc: executed by bash(1) for non-login shells.
# see /usr/share/doc/bash/examples/startup-files (in the package bash-doc)
# for examples

# If not running interactively, don't do anything

export PYSPARK_PYTHON="ipython"
export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64
export HADOOP_HOME=/home/tbresee/hadoop/hadoop-3.2.0
export HAIL_HOME=/home/tbresee/hail               < - - - i think this is wrong !  

export SPARK_HOME=~/hadoop/spark-2.4.3-bin-hadoop2.7
export PATH=$SPARK_HOME/bin:$PATH

export PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH
export PYSPARK_DRIVER_PYTHON="jupyter"
export PYSPARK_DRIVER_PYTHON_OPTS="notebook"
#export PYSPARK_PYTHON=python3
export SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar

#
#
#


case $- in
    *i*) ;;
      *) return;;
esac

# don't put duplicate lines or lines starting with space in the history.
# See bash(1) for more options
HISTCONTROL=ignoreboth

# append to the history file, don't overwrite it
shopt -s histappend

# for setting history length see HISTSIZE and HISTFILESIZE in bash(1)
HISTSIZE=1000
HISTFILESIZE=2000

# check the window size after each command and, if necessary,
# update the values of LINES and COLUMNS.
shopt -s checkwinsize

# If set, the pattern "**" used in a pathname expansion context will
# match all files and zero or more directories and subdirectories.
#shopt -s globstar

# make less more friendly for non-text input files, see lesspipe(1)
[ -x /usr/bin/lesspipe ] && eval "$(SHELL=/bin/sh lesspipe)"

# set variable identifying the chroot you work in (used in the prompt below)
if [ -z "${debian_chroot:-}" ] && [ -r /etc/debian_chroot ]; then
    debian_chroot=$(cat /etc/debian_chroot)
fi

# set a fancy prompt (non-color, unless we know we "want" color)
case "$TERM" in
    xterm-color|*-256color) color_prompt=yes;;
esac

# uncomment for a colored prompt, if the terminal has the capability; turned
# off by default to not distract the user: the focus in a terminal window
# should be on the output of commands, not on the prompt
#force_color_prompt=yes

if [ -n "$force_color_prompt" ]; then
    if [ -x /usr/bin/tput ] && tput setaf 1 >&/dev/null; then
        # We have color support; assume it's compliant with Ecma-48
        # (ISO/IEC-6429). (Lack of such support is extremely rare, and such
        # a case would tend to support setf rather than setaf.)
        color_prompt=yes
    else
        color_prompt=
    fi
fi

if [ "$color_prompt" = yes ]; then
    PS1='${debian_chroot:+($debian_chroot)}\[\033[01;32m\]\u@\h\[\033[00m\]:\[\033[01;34m\]\w\[\033[00m\]\$ '
else
    PS1='${debian_chroot:+($debian_chroot)}\u@\h:\w\$ '
fi
unset color_prompt force_color_prompt

# If this is an xterm set the title to user@host:dir
case "$TERM" in
xterm*|rxvt*)
    PS1="\[\e]0;${debian_chroot:+($debian_chroot)}\u@\h: \w\a\]$PS1"
    ;;
*)
    ;;
esac

# enable color support of ls and also add handy aliases
if [ -x /usr/bin/dircolors ]; then
    test -r ~/.dircolors && eval "$(dircolors -b ~/.dircolors)" || eval "$(dircolors -b)"
    alias ls='ls --color=auto'
    #alias dir='dir --color=auto'
    #alias vdir='vdir --color=auto'

    alias grep='grep --color=auto'
    alias fgrep='fgrep --color=auto'
    alias egrep='egrep --color=auto'
fi

# colored GCC warnings and errors
#export GCC_COLORS='error=01;31:warning=01;35:note=01;36:caret=01;32:locus=01:quote=01'

# some more ls aliases
alias ll='ls -alF'
alias la='ls -A'
alias l='ls -CF'

# Add an "alert" alias for long running commands.  Use like so:
#   sleep 10; alert
alias alert='notify-send --urgency=low -i "$([ $? = 0 ] && echo terminal || echo error)" "$(history|tail -n1|sed -e '\''s/^\s*[0-9]\+\s*//;s/[;&|]\s*alert$//'\'')"'

# Alias definitions.
# You may want to put all your additions into a separate file like
# ~/.bash_aliases, instead of adding them here directly.
# See /usr/share/doc/bash-doc/examples in the bash-doc package.

if [ -f ~/.bash_aliases ]; then
    . ~/.bash_aliases
fi

alias python=python3
# enable programmable completion features (you don't need to enable
# this, if it's already enabled in /etc/bash.bashrc and /etc/profile
# sources /etc/bash.bashrc).
if ! shopt -oq posix; then
  if [ -f /usr/share/bash-completion/bash_completion ]; then
    . /usr/share/bash-completion/bash_completion
  elif [ -f /etc/bash_completion ]; then
    . /etc/bash_completion
  fi
fi
export DOCKER_HOST=tcp://localhost:2375
export DOCKER_HOST=tcp://localhost:2375
export DOCKER_HOST=tcp://localhost:2375
PATH=$HOME/.local/bin:$PATH






tbresee@TM0493322:~$ printenv | grep site-packages
tbresee@TM0493322:~$ python3
Python 3.6.7 (default, Oct 22 2018, 11:32:17)
[GCC 8.2.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> import sys
>>> site_packages = next(p for p in sys.path if 'site-packages' in p)
>>> print site_packages
  File "<stdin>", line 1
    print site_packages
                      ^
SyntaxError: Missing parentheses in call to 'print'. Did you mean print(site_packages)?
>>> import sys
>>> site_packages = next(p for p in sys.path if 'site-packages' in p)
>>> print(site_packages)
/home/tbresee/.local/lib/python3.6/site-packages
>>>






./.local/lib/python3.6/site-packages

/usr/local/lib/python3.6/site-packages




* i believe i should shift the HAIL_HOME to be exact 

	HAIL_HOME=/usr/local/lib/python3.6/site-packages/hail





./.local/lib/python3.6/site-packages






~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Redoing all of this to prove it is reproducible:




tbresee@--------:~$ pwd
/home/tbresee


Entered the following commands:
pyspark \
   --jars $HAIL_HOME/build/libs/hail-all-spark.jar \
   --conf spark.driver.extraClassPath=$HAIL_HOME/build/libs/hail-all-spark.jar \
   --conf spark.executor.extraClassPath=./hail-all-spark.jar \
   --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
   --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator



Output you saw:
tbresee@TM0493322:~$ pwd
/home/tbresee
tbresee@TM0493322:~$
tbresee@TM0493322:~$ pyspark \
>    --jars $HAIL_HOME/build/libs/hail-all-spark.jar \
>    --conf spark.driver.extraClassPath=$HAIL_HOME/build/libs/hail-all-spark.jar \
>    --conf spark.executor.extraClassPath=./hail-all-spark.jar \
>    --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
>    --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator
[I 15:50:25.986 NotebookApp] Serving notebooks from local directory: /home/tbresee
[I 15:50:25.986 NotebookApp] The Jupyter Notebook is running at:
[I 15:50:25.987 NotebookApp] http://localhost:8888/?token=9e87e967a70cc021227215fa781992c7311683555ba9725e
[I 15:50:25.988 NotebookApp]  or http://127.0.0.1:8888/?token=9e87e967a70cc021227215fa781992c7311683555ba9725e
[I 15:50:25.990 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[W 15:50:25.999 NotebookApp] No web browser found: could not locate runnable browser.
[C 15:50:26.000 NotebookApp]

    To access the notebook, open this file in a browser:
        file:///home/tbresee/.local/share/jupyter/runtime/nbserver-62-open.html
    Or copy and paste one of these URLs:
        http://localhost:8888/?token=9e87e967a70cc021227215fa781992c7311683555ba9725e
     or http://127.0.0.1:8888/?token=9e87e967a70cc021227215fa781992c7311683555ba9725e




I tried, but this failed:

tbresee@TM0493322:~$ pwd
/home/tbresee
tbresee@TM0493322:~$
tbresee@TM0493322:~$ pyspark \
>    --jars $HAIL_HOME/build/libs/hail-all-spark.jar \
>    --conf spark.driver.extraClassPath=$HAIL_HOME/build/libs/hail-all-spark.jar \
>    --conf spark.executor.extraClassPath=./hail-all-spark.jar \
>    --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
>    --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator
[I 15:50:25.986 NotebookApp] Serving notebooks from local directory: /home/tbresee
[I 15:50:25.986 NotebookApp] The Jupyter Notebook is running at:
[I 15:50:25.987 NotebookApp] http://localhost:8888/?token=9e87e967a70cc021227215fa781992c7311683555ba9725e
[I 15:50:25.988 NotebookApp]  or http://127.0.0.1:8888/?token=9e87e967a70cc021227215fa781992c7311683555ba9725e
[I 15:50:25.990 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[W 15:50:25.999 NotebookApp] No web browser found: could not locate runnable browser.
[C 15:50:26.000 NotebookApp]

    To access the notebook, open this file in a browser:
        file:///home/tbresee/.local/share/jupyter/runtime/nbserver-62-open.html
    Or copy and paste one of these URLs:
        http://localhost:8888/?token=9e87e967a70cc021227215fa781992c7311683555ba9725e
     or http://127.0.0.1:8888/?token=9e87e967a70cc021227215fa781992c7311683555ba9725e
[I 15:52:03.970 NotebookApp] 302 GET /?token=9e87e967a70cc021227215fa781992c7311683555ba9725e (127.0.0.1) 1.46ms
[E 15:52:04.051 NotebookApp] Could not open static file ''
[W 15:52:04.125 NotebookApp] 404 GET /static/components/react/react-dom.production.min.js (127.0.0.1) 16.79ms referer=http://127.0.0.1:8888/tree?token=9e87e967a70cc021227215fa781992c7311683555ba9725e
[W 15:52:04.212 NotebookApp] 404 GET /static/components/react/react-dom.production.min.js (127.0.0.1) 4.76ms referer=http://127.0.0.1:8888/tree?token=9e87e967a70cc021227215fa781992c7311683555ba9725e
[I 15:52:20.327 NotebookApp] Creating new notebook in
[W 15:52:20.881 NotebookApp] 404 GET /static/components/react/react-dom.production.min.js (127.0.0.1) 4.64ms referer=http://127.0.0.1:8888/notebooks/Untitled1.ipynb?kernel_name=python3
[W 15:52:20.963 NotebookApp] 404 GET /static/components/react/react-dom.production.min.js (127.0.0.1) 3.59ms referer=http://127.0.0.1:8888/notebooks/Untitled1.ipynb?kernel_name=python3
[I 15:52:21.533 NotebookApp] Kernel started: 2a156f78-aa9f-4c7a-aa66-81ff7aecb7e8
[W 15:52:21.545 NotebookApp] 404 GET /nbextensions/widgets/notebook/js/extension.js?v=20190812155025 (127.0.0.1) 7.87ms referer=http://127.0.0.1:8888/notebooks/Untitled1.ipynb?kernel_name=python3
19/08/12 20:52:24 WARN Utils: Your hostname, TM0493322 resolves to a loopback address: 127.0.1.1; using 10.0.75.1 instead (on interface eth3)
19/08/12 20:52:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
19/08/12 20:52:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
19/08/12 20:52:25 WARN DependencyUtils: Local jar /home/tbresee/hail/build/libs/hail-all-spark.jar does not exist, skipping.
19/08/12 20:52:26 INFO SparkContext: Running Spark version 2.4.3
19/08/12 20:52:26 INFO SparkContext: Submitted application: PySparkShell
19/08/12 20:52:26 INFO SecurityManager: Changing view acls to: tbresee
19/08/12 20:52:26 INFO SecurityManager: Changing modify acls to: tbresee
19/08/12 20:52:26 INFO SecurityManager: Changing view acls groups to:
19/08/12 20:52:26 INFO SecurityManager: Changing modify acls groups to:
19/08/12 20:52:26 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tbresee); groups with view permissions: Set(); users  with modify permissions: Set(tbresee); groups with modify permissions: Set()
19/08/12 20:52:26 INFO Utils: Successfully started service 'sparkDriver' on port 1317.
19/08/12 20:52:26 INFO SparkEnv: Registering MapOutputTracker
19/08/12 20:52:26 INFO SparkEnv: Registering BlockManagerMaster
19/08/12 20:52:26 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
19/08/12 20:52:26 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
19/08/12 20:52:26 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c7e0d0fa-bc35-4a03-a0ac-8b50bc96fbdf
19/08/12 20:52:26 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
19/08/12 20:52:26 INFO SparkEnv: Registering OutputCommitCoordinator
19/08/12 20:52:27 INFO Utils: Successfully started service 'SparkUI' on port 4040.
19/08/12 20:52:27 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.0.75.1:4040
19/08/12 20:52:27 ERROR SparkContext: Failed to add file:/home/tbresee/hail/build/libs/hail-all-spark.jar to Spark environment
java.io.FileNotFoundException: Jar /home/tbresee/hail/build/libs/hail-all-spark.jar not found
        at org.apache.spark.SparkContext.addJarFile$1(SparkContext.scala:1838)
        at org.apache.spark.SparkContext.addJar(SparkContext.scala:1868)
        at org.apache.spark.SparkContext$$anonfun$12.apply(SparkContext.scala:458)
        at org.apache.spark.SparkContext$$anonfun$12.apply(SparkContext.scala:458)
        at scala.collection.immutable.List.foreach(List.scala:392)
        at org.apache.spark.SparkContext.<init>(SparkContext.scala:458)
        at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
        at py4j.Gateway.invoke(Gateway.java:238)
        at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
        at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
        at py4j.GatewayConnection.run(GatewayConnection.java:238)
        at java.lang.Thread.run(Thread.java:748)
19/08/12 20:52:27 INFO Executor: Starting executor ID driver on host localhost
19/08/12 20:52:27 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 1319.
19/08/12 20:52:27 INFO NettyBlockTransferService: Server created on 10.0.75.1:1319
19/08/12 20:52:27 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
19/08/12 20:52:27 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.75.1, 1319, None)
19/08/12 20:52:27 INFO BlockManagerMasterEndpoint: Registering block manager 10.0.75.1:1319 with 366.3 MB RAM, BlockManagerId(driver, 10.0.75.1, 1319, None)
19/08/12 20:52:27 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.75.1, 1319, None)
19/08/12 20:52:27 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.75.1, 1319, None)
19/08/12 20:52:27 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/tbresee/spark-warehouse').
19/08/12 20:52:27 INFO SharedState: Warehouse path is 'file:/home/tbresee/spark-warehouse'.
19/08/12 20:52:28 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
[I 15:52:28.641 NotebookApp] Adapting from protocol version 5.1 (kernel 2a156f78-aa9f-4c7a-aa66-81ff7aecb7e8) to 5.3 (client).
[I 15:53:50.150 NotebookApp] Saving file at /Untitled1.ipynb
[I 15:54:21.440 NotebookApp] Saving file at /Untitled1.ipynb
^C[I 15:55:21.049 NotebookApp] interrupted
Serving notebooks from local directory: /home/tbresee
1 active kernel
The Jupyter Notebook is running at:
http://localhost:8888/?token=9e87e967a70cc021227215fa781992c7311683555ba9725e
 or http://127.0.0.1:8888/?token=9e87e967a70cc021227215fa781992c7311683555ba9725e
Shutdown this notebook server (y/[n])? ^C[C 15:55:21.168 NotebookApp] received signal 2, stopping
[I 15:55:21.174 NotebookApp] Shutting down 1 kernel
19/08/12 20:55:21 INFO SparkUI: Stopped Spark web UI at http://10.0.75.1:4040
19/08/12 20:55:21 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
19/08/12 20:55:21 INFO MemoryStore: MemoryStore cleared
19/08/12 20:55:21 INFO BlockManager: BlockManager stopped
19/08/12 20:55:21 INFO BlockManagerMaster: BlockManagerMaster stopped
19/08/12 20:55:21 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
19/08/12 20:55:21 INFO SparkContext: Successfully stopped SparkContext
19/08/12 20:55:21 INFO ShutdownHookManager: Shutdown hook called
19/08/12 20:55:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-43062682-f6b0-4877-8442-8bf0e65ad28b
19/08/12 20:55:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-6b7dac63-45c4-4f0b-8133-11ffec7cb1c5/pyspark-2704a284-04a3-4d21-9725-4ad9d5071024
19/08/12 20:55:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-6b7dac63-45c4-4f0b-8133-11ffec7cb1c5
[I 15:55:22.186 NotebookApp] Kernel shutdown: 2a156f78-aa9f-4c7a-aa66-81ff7aecb7e8
tbresee@TM0493322:~$


/home/tbresee/.local/lib/python3.6/site-packages/hail


It can't find my main .jar file, as i suspected ! 


./.local/lib/python3.6/site-packages/hail/
/home/tbresee/

hadoop/spark-2.4.3-bin-hadoop2.7





--- THIS IS A NEW WAY OF STARTING IT DIRECTLY VIA PYSPARK AND DIRECT POINTING ---

pyspark \
   --jars $HAIL_HOME/hail-all-spark.jar \
   --conf spark.driver.extraClassPath=$HAIL_HOME/hail-all-spark.jar \
   --conf spark.executor.extraClassPath=$HAIL_HOME/hail-all-spark.jar \
   --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
   --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator































tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$ pwd
/home/tbresee
tbresee@TM0493322:~$
tbresee@TM0493322:~$ echo $HAIL_HOME
/home/tbresee/hail
tbresee@TM0493322:~$ cd hail
tbresee@TM0493322:~/hail$ ll
total 92
drwxrwxrwx 1 tbresee tbresee  4096 Aug  5 10:00 ./
drwxr-xr-x 1 tbresee tbresee  4096 Aug 12 15:39 ../
-rw-rw-rw- 1 tbresee tbresee    38 Aug  5 09:31 acknowledgements.txt
-rw-rw-rw- 1 tbresee tbresee   448 Aug  5 09:31 AUTHORS
drwxrwxrwx 1 tbresee tbresee  4096 Aug  5 10:00 build/
-rw-rw-rw- 1 tbresee tbresee 19095 Aug  5 09:31 build.gradle
-rw-rw-rw- 1 tbresee tbresee  7264 Aug  5 09:31 changes.md
-rw-rw-rw- 1 tbresee tbresee 17207 Aug  5 09:31 code_style.xml
-rw-rw-rw- 1 tbresee tbresee     6 Aug  5 09:31 deployed-spark-versions.txt
-rw-rw-rw- 1 tbresee tbresee   307 Aug  5 09:31 .dockerignore
-rw-rw-rw- 1 tbresee tbresee   393 Aug  5 09:31 generate-build-info.sh
-rw-rw-rw- 1 tbresee tbresee   631 Aug  5 09:31 generate-dist-links.sh
drwxrwxrwx 1 tbresee tbresee  4096 Aug  5 09:31 .git/
-rw-rw-rw- 1 tbresee tbresee   165 Aug  5 09:31 .gitignore
drwxrwxrwx 1 tbresee tbresee  4096 Aug  5 09:31 gradle/
drwxrwxrwx 1 tbresee tbresee  4096 Aug  5 09:59 .gradle/
-rwxrwxrwx 1 tbresee tbresee  5046 Aug  5 09:31 gradlew*
-rw-rw-rw- 1 tbresee tbresee  2404 Aug  5 09:31 gradlew.bat
-rw-rw-rw- 1 tbresee tbresee    79 Aug  5 09:31 hail-ci-build-image
-rw-rw-rw- 1 tbresee tbresee   937 Aug  5 09:31 hail-ci-build.sh
-rw-rw-rw- 1 tbresee tbresee  1842 Aug  5 09:31 hail-ci-deploy.sh
-rw-rw-rw- 1 tbresee tbresee   277 Aug  5 09:31 hail-docs-trampoline.sh
-rw-rw-rw- 1 tbresee tbresee  1102 Aug  5 09:31 LICENSE
-rw-rw-rw- 1 tbresee tbresee   972 Aug  5 09:31 Makefile
drwxrwxrwx 1 tbresee tbresee  4096 Aug  5 09:31 pr-builder/
drwxrwxrwx 1 tbresee tbresee  4096 Aug  5 09:31 python/
-rw-rw-rw- 1 tbresee tbresee  5855 Aug  5 09:31 README.md
drwxrwxrwx 1 tbresee tbresee  4096 Aug  5 09:31 scripts/
-rw-rw-rw- 1 tbresee tbresee    23 Aug  5 09:31 settings.gradle
drwxrwxrwx 1 tbresee tbresee  4096 Aug  5 09:31 src/
-rw-rw-rw- 1 tbresee tbresee  1862 Aug  5 09:31 style-guide.md
-rw-rw-rw- 1 tbresee tbresee   164 Aug  5 09:31 testng.xml
drwxrwxrwx 1 tbresee tbresee  4096 Aug  5 09:31 www/
tbresee@TM0493322:~/hail$ cd build
tbresee@TM0493322:~/hail/build$ ll
total 0
drwxrwxrwx 1 tbresee tbresee 4096 Aug  5 10:00 ./
drwxrwxrwx 1 tbresee tbresee 4096 Aug  5 10:00 ../
drwxrwxrwx 1 tbresee tbresee 4096 Aug  5 10:00 classes/
drwxrwxrwx 1 tbresee tbresee 4096 Aug  5 10:00 tmp/
tbresee@TM0493322:~/hail/build$ ls -la
total 0
drwxrwxrwx 1 tbresee tbresee 4096 Aug  5 10:00 .
drwxrwxrwx 1 tbresee tbresee 4096 Aug  5 10:00 ..
drwxrwxrwx 1 tbresee tbresee 4096 Aug  5 10:00 classes
drwxrwxrwx 1 tbresee tbresee 4096 Aug  5 10:00 tmp
tbresee@TM0493322:~/hail/build$ cd ..
tbresee@TM0493322:~/hail$ cd ..
tbresee@TM0493322:~$ cd hail
tbresee@TM0493322:~/hail$ ll
total 92
drwxrwxrwx 1 tbresee tbresee  4096 Aug  5 10:00 ./
drwxr-xr-x 1 tbresee tbresee  4096 Aug 12 15:39 ../
-rw-rw-rw- 1 tbresee tbresee    38 Aug  5 09:31 acknowledgements.txt
-rw-rw-rw- 1 tbresee tbresee   448 Aug  5 09:31 AUTHORS
drwxrwxrwx 1 tbresee tbresee  4096 Aug  5 10:00 build/
-rw-rw-rw- 1 tbresee tbresee 19095 Aug  5 09:31 build.gradle
-rw-rw-rw- 1 tbresee tbresee  7264 Aug  5 09:31 changes.md
-rw-rw-rw- 1 tbresee tbresee 17207 Aug  5 09:31 code_style.xml
-rw-rw-rw- 1 tbresee tbresee     6 Aug  5 09:31 deployed-spark-versions.txt
-rw-rw-rw- 1 tbresee tbresee   307 Aug  5 09:31 .dockerignore
-rw-rw-rw- 1 tbresee tbresee   393 Aug  5 09:31 generate-build-info.sh
-rw-rw-rw- 1 tbresee tbresee   631 Aug  5 09:31 generate-dist-links.sh
drwxrwxrwx 1 tbresee tbresee  4096 Aug  5 09:31 .git/
-rw-rw-rw- 1 tbresee tbresee   165 Aug  5 09:31 .gitignore
drwxrwxrwx 1 tbresee tbresee  4096 Aug  5 09:31 gradle/
drwxrwxrwx 1 tbresee tbresee  4096 Aug  5 09:59 .gradle/
-rwxrwxrwx 1 tbresee tbresee  5046 Aug  5 09:31 gradlew*
-rw-rw-rw- 1 tbresee tbresee  2404 Aug  5 09:31 gradlew.bat
-rw-rw-rw- 1 tbresee tbresee    79 Aug  5 09:31 hail-ci-build-image
-rw-rw-rw- 1 tbresee tbresee   937 Aug  5 09:31 hail-ci-build.sh
-rw-rw-rw- 1 tbresee tbresee  1842 Aug  5 09:31 hail-ci-deploy.sh
-rw-rw-rw- 1 tbresee tbresee   277 Aug  5 09:31 hail-docs-trampoline.sh
-rw-rw-rw- 1 tbresee tbresee  1102 Aug  5 09:31 LICENSE
-rw-rw-rw- 1 tbresee tbresee   972 Aug  5 09:31 Makefile
drwxrwxrwx 1 tbresee tbresee  4096 Aug  5 09:31 pr-builder/
drwxrwxrwx 1 tbresee tbresee  4096 Aug  5 09:31 python/
-rw-rw-rw- 1 tbresee tbresee  5855 Aug  5 09:31 README.md
drwxrwxrwx 1 tbresee tbresee  4096 Aug  5 09:31 scripts/
-rw-rw-rw- 1 tbresee tbresee    23 Aug  5 09:31 settings.gradle
drwxrwxrwx 1 tbresee tbresee  4096 Aug  5 09:31 src/
-rw-rw-rw- 1 tbresee tbresee  1862 Aug  5 09:31 style-guide.md
-rw-rw-rw- 1 tbresee tbresee   164 Aug  5 09:31 testng.xml
drwxrwxrwx 1 tbresee tbresee  4096 Aug  5 09:31 www/
tbresee@TM0493322:~/hail$ cd build
tbresee@TM0493322:~/hail/build$ ll
total 0
drwxrwxrwx 1 tbresee tbresee 4096 Aug  5 10:00 ./
drwxrwxrwx 1 tbresee tbresee 4096 Aug  5 10:00 ../
drwxrwxrwx 1 tbresee tbresee 4096 Aug  5 10:00 classes/
drwxrwxrwx 1 tbresee tbresee 4096 Aug  5 10:00 tmp/
tbresee@TM0493322:~/hail/build$ cd ..
tbresee@TM0493322:~/hail$ cd ..
tbresee@TM0493322:~$ cd ~
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$ cd ~
tbresee@TM0493322:~$ pwd
/home/tbresee
tbresee@TM0493322:~$ cd /usr/local/lib/python3.6/site-packages/hail
-bash: cd: /usr/local/lib/python3.6/site-packages/hail: No such file or directory
tbresee@TM0493322:~$ cd ./.local/lib/python3.6/site-packages
tbresee@TM0493322:~/.local/lib/python3.6/site-packages$ ll
total 924
drwx------ 1 tbresee tbresee   4096 Aug 12 14:59 ./
drwx------ 1 tbresee tbresee   4096 Aug  7 10:23 ../
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:23 aiohttp/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 aiohttp-3.5.4.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:23 async_timeout/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 async_timeout-3.0.1.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:23 attr/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 attrs-19.1.0.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:24 backcall/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 backcall-0.1.0.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:39 bleach/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:39 bleach-3.1.0.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:23 bokeh/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 bokeh-1.2.0.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:23 cachetools/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 cachetools-3.1.1.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:23 certifi/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 certifi-2019.6.16.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:23 chardet/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 chardet-3.0.4.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 11:09 cycler-0.10.0.dist-info/
-rw-rw-rw- 1 tbresee tbresee  15959 Aug  7 11:09 cycler.py
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:23 dateutil/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 decorator-4.4.0.dist-info/
-rw-rw-rw- 1 tbresee tbresee  17244 Aug  8 11:34 decorator.py
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:39 defusedxml/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:39 defusedxml-0.6.0.dist-info/
-rw-rw-rw- 1 tbresee tbresee    126 Aug  8 11:35 easy_install.py
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:39 entrypoints-0.3.dist-info/
-rw-rw-rw- 1 tbresee tbresee   8262 Aug  7 10:38 entrypoints.py
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 12:22 findspark-1.3.0.dist-info/
-rw-rw-rw- 1 tbresee tbresee   5594 Aug  7 12:22 findspark.py
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:23 gcsfs/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 gcsfs-0.2.1.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:23 google/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 google_auth-1.6.3.dist-info/
-rw-rw-rw- 1 tbresee tbresee    539 Aug  8 11:35 google_auth-1.6.3-py3.5-nspkg.pth
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:23 google_auth_oauthlib/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 google_auth_oauthlib-0.4.0.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:24 hail/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 hail-0.2.19.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:24 hailtop/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:23 hurry/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 hurry.filesize-0.9.dist-info/
-rw-rw-rw- 1 tbresee tbresee    534 Aug  8 11:34 hurry.filesize-0.9-nspkg.pth
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:23 idna/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 idna-2.8.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 idna_ssl-1.1.0.dist-info/
-rw-rw-rw- 1 tbresee tbresee    779 Aug  8 11:35 idna_ssl.py
drwxrwxrwx 1 tbresee tbresee   4096 Aug 12 14:59 ipykernel/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 ipykernel-4.10.0.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug 12 14:59 ipykernel-4.10.1.dist-info/
-rw-rw-rw- 1 tbresee tbresee    451 Aug 12 14:59 ipykernel_launcher.py
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:24 IPython/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 ipython-7.7.0.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:23 ipython_genutils/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 ipython_genutils-0.2.0.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:39 ipywidgets/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:39 ipywidgets-7.5.1.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:24 jedi/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 jedi-0.14.1.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:23 jinja2/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 Jinja2-2.10.1.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:39 jsonschema/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:39 jsonschema-3.0.2.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:39 jupyter-1.0.0.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:24 jupyter_client/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 jupyter_client-5.3.1.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:39 jupyter_console/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:39 jupyter_console-6.0.0.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:23 jupyter_core/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 jupyter_core-4.5.0.dist-info/
-rw-rw-rw- 1 tbresee tbresee    118 Aug  8 11:35 jupyter.py
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:23 jwt/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 11:09 kiwisolver-1.1.0.dist-info/
-rwxrwxrwx 1 tbresee tbresee 255336 Aug  7 11:09 kiwisolver.cpython-36m-x86_64-linux-gnu.so*
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:23 markupsafe/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 MarkupSafe-1.1.1.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 11:08 matplotlib/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 11:09 matplotlib-3.1.1.dist-info/
-rw-rw-rw- 1 tbresee tbresee    569 Aug  7 11:09 matplotlib-3.1.1-py3.6-nspkg.pth
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:39 mistune-0.8.4.dist-info/
-rw-rw-rw- 1 tbresee tbresee  36516 Aug  7 10:38 mistune.py
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 11:08 mpl_toolkits/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:23 multidict/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 multidict-4.5.2.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:39 nbconvert/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:39 nbconvert-5.5.0.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:39 nbformat/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:39 nbformat-4.4.0.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 nest_asyncio-1.0.0.dist-info/
-rw-rw-rw- 1 tbresee tbresee   5501 Aug  8 11:34 nest_asyncio.py
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:39 notebook/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:39 notebook-6.0.0.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:23 numpy/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 numpy-1.17.0.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:23 oauthlib/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 oauthlib-3.1.0.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:23 packaging/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 packaging-19.1.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug 12 14:59 pandas/
drwxrwxrwx 1 tbresee tbresee   4096 Aug 12 14:59 pandas-0.23.4.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:39 pandocfilters-1.4.2.dist-info/
-rw-rw-rw- 1 tbresee tbresee   8271 Aug  7 10:39 pandocfilters.py
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:23 parsimonious/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 parsimonious-0.8.1.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:24 parso/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 parso-0.5.1.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:24 pexpect/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 pexpect-4.7.0.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 pickleshare-0.7.5.dist-info/
-rw-rw-rw- 1 tbresee tbresee   9942 Aug  8 11:35 pickleshare.py
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:23 PIL/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 Pillow-6.1.0.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:44 pip/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:44 pip-19.2.1.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:23 pkg_resources/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:39 prometheus_client/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:39 prometheus_client-0.7.1.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:24 prompt_toolkit/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 prompt_toolkit-2.0.9.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:24 ptyprocess/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 ptyprocess-0.6.0.dist-info/
-rwxrwxrwx 1 tbresee tbresee 105736 Aug  7 10:39 pvectorc.cpython-36m-x86_64-linux-gnu.so*
drwxrwxrwx 1 tbresee tbresee   4096 Aug 12 14:59 py4j/
drwxrwxrwx 1 tbresee tbresee   4096 Aug 12 14:59 py4j-0.10.7.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:23 pyasn1/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 pyasn1-0.4.6.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:23 pyasn1_modules/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 pyasn1_modules-0.2.6.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug 12 14:59 __pycache__/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:24 pygments/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 Pygments-2.4.2.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 PyJWT-1.7.1.dist-info/
-rw-rw-rw- 1 tbresee tbresee     90 Aug  7 11:09 pylab.py
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 pyparsing-2.4.2.dist-info/
-rw-rw-rw- 1 tbresee tbresee 263439 Aug  8 11:35 pyparsing.py
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:39 pyrsistent/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:39 pyrsistent-0.15.4.dist-info/
-rw-rw-rw- 1 tbresee tbresee     23 Aug  7 10:39 _pyrsistent_version.py
drwxrwxrwx 1 tbresee tbresee   4096 Aug 12 15:15 pyspark/
drwxrwxrwx 1 tbresee tbresee   4096 Aug 12 15:15 pyspark-2.4.1.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 12:00 pyspark-2.4.3.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 python_dateutil-2.8.0.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:24 pythonjsonlogger/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 python_json_logger-0.1.11.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:23 pytz/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 pytz-2019.2.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 00:14 PyVCF-0.6.8.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 PyYAML-5.1.2.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:39 pyzmq-18.0.2.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 pyzmq-18.1.0.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:39 qtconsole/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:39 qtconsole-4.5.2.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:23 requests/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 requests-2.21.0.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:23 requests_oauthlib/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 requests_oauthlib-1.2.0.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:23 rsa/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 rsa-4.0.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:24 scipy/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 scipy-1.3.0.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 11:10 seaborn/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 11:10 seaborn-0.9.0.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:39 send2trash/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:39 Send2Trash-1.5.0.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:23 setuptools/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 setuptools-41.0.1.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 six-1.12.0.dist-info/
-rw-rw-rw- 1 tbresee tbresee  32452 Aug  8 11:35 six.py
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 tabulate-0.8.3.dist-info/
-rw-rw-rw- 1 tbresee tbresee  59377 Aug  8 11:34 tabulate.py
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:39 terminado/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:39 terminado-0.8.2.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:39 testpath/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:39 testpath-0.4.2.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:23 tornado/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 tornado-6.0.3.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:23 traitlets/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 traitlets-4.3.2.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 typing_extensions-3.7.4.dist-info/
-rw-rw-rw- 1 tbresee tbresee  74362 Aug  8 11:35 typing_extensions.py
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:23 urllib3/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 urllib3-1.24.3.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 00:14 vcf/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:24 wcwidth/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 wcwidth-0.1.7.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:39 webencodings/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:39 webencodings-0.5.1.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:39 widgetsnbextension/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:39 widgetsnbextension-3.5.1.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:23 yaml/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:23 yarl/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  8 11:35 yarl-1.3.0.dist-info/
drwxrwxrwx 1 tbresee tbresee   4096 Aug  7 10:23 zmq/
tbresee@TM0493322:~/.local/lib/python3.6/site-packages$ cd hail
tbresee@TM0493322:~/.local/lib/python3.6/site-packages/hail$ ll
total 32744
drwxrwxrwx 1 tbresee tbresee     4096 Aug  7 10:24 ./
drwx------ 1 tbresee tbresee     4096 Aug 12 14:59 ../
drwxrwxrwx 1 tbresee tbresee     4096 Aug  7 10:24 backend/
-rw-rw-rw- 1 tbresee tbresee     4972 Aug  8 11:34 conftest.py
-rw-rw-rw- 1 tbresee tbresee    14104 Aug  8 11:34 context.py
drwxrwxrwx 1 tbresee tbresee     4096 Aug  7 10:24 experimental/
drwxrwxrwx 1 tbresee tbresee     4096 Aug  7 10:24 expr/
drwxrwxrwx 1 tbresee tbresee     4096 Aug  7 10:24 fs/
drwxrwxrwx 1 tbresee tbresee     4096 Aug  7 10:24 genetics/
-rw-rw-rw- 1 tbresee tbresee 33229607 Aug  8 11:34 hail-all-spark.jar
-rw-rw-rw- 1 tbresee tbresee        6 Aug  8 11:34 hail_pip_version
-rw-rw-rw- 1 tbresee tbresee       19 Aug  8 11:34 hail_version
-rw-rw-rw- 1 tbresee tbresee     2453 Aug  8 11:34 __init__.py
drwxrwxrwx 1 tbresee tbresee     4096 Aug  7 10:24 ir/
drwxrwxrwx 1 tbresee tbresee     4096 Aug  7 10:24 linalg/
-rw-rw-rw- 1 tbresee tbresee   149224 Aug  8 11:34 matrixtable.py
drwxrwxrwx 1 tbresee tbresee     4096 Aug  7 10:24 methods/
drwxrwxrwx 1 tbresee tbresee     4096 Aug  7 10:24 plot/
drwxrwxrwx 1 tbresee tbresee     4096 Aug  7 10:24 __pycache__/
drwxrwxrwx 1 tbresee tbresee     4096 Aug  7 10:24 stats/
-rw-rw-rw- 1 tbresee tbresee   116557 Aug  8 11:34 table.py
drwxrwxrwx 1 tbresee tbresee     4096 Aug  7 10:24 typecheck/
drwxrwxrwx 1 tbresee tbresee     4096 Aug  7 10:24 utils/
tbresee@TM0493322:~/.local/lib/python3.6/site-packages/hail$ hail_version
hail_version: command not found
tbresee@TM0493322:~/.local/lib/python3.6/site-packages/hail$ .hail_version
.hail_version: command not found
tbresee@TM0493322:~/.local/lib/python3.6/site-packages/hail$ ./hail_version
-bash: ./hail_version: Permission denied
tbresee@TM0493322:~/.local/lib/python3.6/site-packages/hail$ sudo ./hail_version
[sudo] password for tbresee:
sudo: ./hail_version: command not found
tbresee@TM0493322:~/.local/lib/python3.6/site-packages/hail$ ll
total 32744
drwxrwxrwx 1 tbresee tbresee     4096 Aug  7 10:24 ./
drwx------ 1 tbresee tbresee     4096 Aug 12 14:59 ../
drwxrwxrwx 1 tbresee tbresee     4096 Aug  7 10:24 backend/
-rw-rw-rw- 1 tbresee tbresee     4972 Aug  8 11:34 conftest.py
-rw-rw-rw- 1 tbresee tbresee    14104 Aug  8 11:34 context.py
drwxrwxrwx 1 tbresee tbresee     4096 Aug  7 10:24 experimental/
drwxrwxrwx 1 tbresee tbresee     4096 Aug  7 10:24 expr/
drwxrwxrwx 1 tbresee tbresee     4096 Aug  7 10:24 fs/
drwxrwxrwx 1 tbresee tbresee     4096 Aug  7 10:24 genetics/
-rw-rw-rw- 1 tbresee tbresee 33229607 Aug  8 11:34 hail-all-spark.jar
-rw-rw-rw- 1 tbresee tbresee        6 Aug  8 11:34 hail_pip_version
-rw-rw-rw- 1 tbresee tbresee       19 Aug  8 11:34 hail_version
-rw-rw-rw- 1 tbresee tbresee     2453 Aug  8 11:34 __init__.py
drwxrwxrwx 1 tbresee tbresee     4096 Aug  7 10:24 ir/
drwxrwxrwx 1 tbresee tbresee     4096 Aug  7 10:24 linalg/
-rw-rw-rw- 1 tbresee tbresee   149224 Aug  8 11:34 matrixtable.py
drwxrwxrwx 1 tbresee tbresee     4096 Aug  7 10:24 methods/
drwxrwxrwx 1 tbresee tbresee     4096 Aug  7 10:24 plot/
drwxrwxrwx 1 tbresee tbresee     4096 Aug  7 10:24 __pycache__/
drwxrwxrwx 1 tbresee tbresee     4096 Aug  7 10:24 stats/
-rw-rw-rw- 1 tbresee tbresee   116557 Aug  8 11:34 table.py
drwxrwxrwx 1 tbresee tbresee     4096 Aug  7 10:24 typecheck/
drwxrwxrwx 1 tbresee tbresee     4096 Aug  7 10:24 utils/
tbresee@TM0493322:~/.local/lib/python3.6/site-packages/hail$
tbresee@TM0493322:~/.local/lib/python3.6/site-packages/hail$
tbresee@TM0493322:~/.local/lib/python3.6/site-packages/hail$
tbresee@TM0493322:~/.local/lib/python3.6/site-packages/hail$
tbresee@TM0493322:~/.local/lib/python3.6/site-packages/hail$
tbresee@TM0493322:~/.local/lib/python3.6/site-packages/hail$
tbresee@TM0493322:~/.local/lib/python3.6/site-packages/hail$
tbresee@TM0493322:~/.local/lib/python3.6/site-packages/hail$ pwd
/home/tbresee/.local/lib/python3.6/site-packages/hail
tbresee@TM0493322:~/.local/lib/python3.6/site-packages/hail$
tbresee@TM0493322:~/.local/lib/python3.6/site-packages/hail$
tbresee@TM0493322:~/.local/lib/python3.6/site-packages/hail$
tbresee@TM0493322:~/.local/lib/python3.6/site-packages/hail$
tbresee@TM0493322:~/.local/lib/python3.6/site-packages/hail$
tbresee@TM0493322:~/.local/lib/python3.6/site-packages/hail$ cd ~
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$ cd hail
tbresee@TM0493322:~/hail$ ll
total 92
drwxrwxrwx 1 tbresee tbresee  4096 Aug  5 10:00 ./
drwxr-xr-x 1 tbresee tbresee  4096 Aug 12 15:39 ../
-rw-rw-rw- 1 tbresee tbresee    38 Aug  5 09:31 acknowledgements.txt
-rw-rw-rw- 1 tbresee tbresee   448 Aug  5 09:31 AUTHORS
drwxrwxrwx 1 tbresee tbresee  4096 Aug  5 10:00 build/
-rw-rw-rw- 1 tbresee tbresee 19095 Aug  5 09:31 build.gradle
-rw-rw-rw- 1 tbresee tbresee  7264 Aug  5 09:31 changes.md
-rw-rw-rw- 1 tbresee tbresee 17207 Aug  5 09:31 code_style.xml
-rw-rw-rw- 1 tbresee tbresee     6 Aug  5 09:31 deployed-spark-versions.txt
-rw-rw-rw- 1 tbresee tbresee   307 Aug  5 09:31 .dockerignore
-rw-rw-rw- 1 tbresee tbresee   393 Aug  5 09:31 generate-build-info.sh
-rw-rw-rw- 1 tbresee tbresee   631 Aug  5 09:31 generate-dist-links.sh
drwxrwxrwx 1 tbresee tbresee  4096 Aug  5 09:31 .git/
-rw-rw-rw- 1 tbresee tbresee   165 Aug  5 09:31 .gitignore
drwxrwxrwx 1 tbresee tbresee  4096 Aug  5 09:31 gradle/
drwxrwxrwx 1 tbresee tbresee  4096 Aug  5 09:59 .gradle/
-rwxrwxrwx 1 tbresee tbresee  5046 Aug  5 09:31 gradlew*
-rw-rw-rw- 1 tbresee tbresee  2404 Aug  5 09:31 gradlew.bat
-rw-rw-rw- 1 tbresee tbresee    79 Aug  5 09:31 hail-ci-build-image
-rw-rw-rw- 1 tbresee tbresee   937 Aug  5 09:31 hail-ci-build.sh
-rw-rw-rw- 1 tbresee tbresee  1842 Aug  5 09:31 hail-ci-deploy.sh
-rw-rw-rw- 1 tbresee tbresee   277 Aug  5 09:31 hail-docs-trampoline.sh
-rw-rw-rw- 1 tbresee tbresee  1102 Aug  5 09:31 LICENSE
-rw-rw-rw- 1 tbresee tbresee   972 Aug  5 09:31 Makefile
drwxrwxrwx 1 tbresee tbresee  4096 Aug  5 09:31 pr-builder/
drwxrwxrwx 1 tbresee tbresee  4096 Aug  5 09:31 python/
-rw-rw-rw- 1 tbresee tbresee  5855 Aug  5 09:31 README.md
drwxrwxrwx 1 tbresee tbresee  4096 Aug  5 09:31 scripts/
-rw-rw-rw- 1 tbresee tbresee    23 Aug  5 09:31 settings.gradle
drwxrwxrwx 1 tbresee tbresee  4096 Aug  5 09:31 src/
-rw-rw-rw- 1 tbresee tbresee  1862 Aug  5 09:31 style-guide.md
-rw-rw-rw- 1 tbresee tbresee   164 Aug  5 09:31 testng.xml
drwxrwxrwx 1 tbresee tbresee  4096 Aug  5 09:31 www/
tbresee@TM0493322:~/hail$ cd ..
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$ pwd
/home/tbresee
tbresee@TM0493322:~$ tbresee@TM0493322:~$ pyspark \
>    --jars $HAIL_HOME/build/libs/hail-all-spark.jar \
>    --conf spark.driver.extraClassPath=$HAIL_HOME/build/libs/hail-all-spark.jar \
>    --conf spark.executor.extraClassPath=./hail-all-spark.jar \
>    --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
>    --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator
tbresee@TM0493322:~$: command not found
tbresee@TM0493322:~$ pwd
/home/tbresee
tbresee@TM0493322:~$ cd hail
tbresee@TM0493322:~/hail$ ll
total 92
drwxrwxrwx 1 tbresee tbresee  4096 Aug  5 10:00 ./
drwxr-xr-x 1 tbresee tbresee  4096 Aug 12 15:39 ../
-rw-rw-rw- 1 tbresee tbresee    38 Aug  5 09:31 acknowledgements.txt
-rw-rw-rw- 1 tbresee tbresee   448 Aug  5 09:31 AUTHORS
drwxrwxrwx 1 tbresee tbresee  4096 Aug  5 10:00 build/
-rw-rw-rw- 1 tbresee tbresee 19095 Aug  5 09:31 build.gradle
-rw-rw-rw- 1 tbresee tbresee  7264 Aug  5 09:31 changes.md
-rw-rw-rw- 1 tbresee tbresee 17207 Aug  5 09:31 code_style.xml
-rw-rw-rw- 1 tbresee tbresee     6 Aug  5 09:31 deployed-spark-versions.txt
-rw-rw-rw- 1 tbresee tbresee   307 Aug  5 09:31 .dockerignore
-rw-rw-rw- 1 tbresee tbresee   393 Aug  5 09:31 generate-build-info.sh
-rw-rw-rw- 1 tbresee tbresee   631 Aug  5 09:31 generate-dist-links.sh
drwxrwxrwx 1 tbresee tbresee  4096 Aug  5 09:31 .git/
-rw-rw-rw- 1 tbresee tbresee   165 Aug  5 09:31 .gitignore
drwxrwxrwx 1 tbresee tbresee  4096 Aug  5 09:31 gradle/
drwxrwxrwx 1 tbresee tbresee  4096 Aug  5 09:59 .gradle/
-rwxrwxrwx 1 tbresee tbresee  5046 Aug  5 09:31 gradlew*
-rw-rw-rw- 1 tbresee tbresee  2404 Aug  5 09:31 gradlew.bat
-rw-rw-rw- 1 tbresee tbresee    79 Aug  5 09:31 hail-ci-build-image
-rw-rw-rw- 1 tbresee tbresee   937 Aug  5 09:31 hail-ci-build.sh
-rw-rw-rw- 1 tbresee tbresee  1842 Aug  5 09:31 hail-ci-deploy.sh
-rw-rw-rw- 1 tbresee tbresee   277 Aug  5 09:31 hail-docs-trampoline.sh
-rw-rw-rw- 1 tbresee tbresee  1102 Aug  5 09:31 LICENSE
-rw-rw-rw- 1 tbresee tbresee   972 Aug  5 09:31 Makefile
drwxrwxrwx 1 tbresee tbresee  4096 Aug  5 09:31 pr-builder/
drwxrwxrwx 1 tbresee tbresee  4096 Aug  5 09:31 python/
-rw-rw-rw- 1 tbresee tbresee  5855 Aug  5 09:31 README.md
drwxrwxrwx 1 tbresee tbresee  4096 Aug  5 09:31 scripts/
-rw-rw-rw- 1 tbresee tbresee    23 Aug  5 09:31 settings.gradle
drwxrwxrwx 1 tbresee tbresee  4096 Aug  5 09:31 src/
-rw-rw-rw- 1 tbresee tbresee  1862 Aug  5 09:31 style-guide.md
-rw-rw-rw- 1 tbresee tbresee   164 Aug  5 09:31 testng.xml
drwxrwxrwx 1 tbresee tbresee  4096 Aug  5 09:31 www/
tbresee@TM0493322:~/hail$
tbresee@TM0493322:~/hail$
tbresee@TM0493322:~/hail$
tbresee@TM0493322:~/hail$
tbresee@TM0493322:~/hail$
tbresee@TM0493322:~/hail$
tbresee@TM0493322:~/hail$
tbresee@TM0493322:~/hail$
tbresee@TM0493322:~/hail$ cd ..
tbresee@TM0493322:~$ pwd
/home/tbresee
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$ pwd
/home/tbresee
tbresee@TM0493322:~$
tbresee@TM0493322:~$ pyspark \
>    --jars $HAIL_HOME/build/libs/hail-all-spark.jar \
>    --conf spark.driver.extraClassPath=$HAIL_HOME/build/libs/hail-all-spark.jar \
>    --conf spark.executor.extraClassPath=./hail-all-spark.jar \
>    --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
>    --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator
[I 15:50:25.986 NotebookApp] Serving notebooks from local directory: /home/tbresee
[I 15:50:25.986 NotebookApp] The Jupyter Notebook is running at:
[I 15:50:25.987 NotebookApp] http://localhost:8888/?token=9e87e967a70cc021227215fa781992c7311683555ba9725e
[I 15:50:25.988 NotebookApp]  or http://127.0.0.1:8888/?token=9e87e967a70cc021227215fa781992c7311683555ba9725e
[I 15:50:25.990 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[W 15:50:25.999 NotebookApp] No web browser found: could not locate runnable browser.
[C 15:50:26.000 NotebookApp]

    To access the notebook, open this file in a browser:
        file:///home/tbresee/.local/share/jupyter/runtime/nbserver-62-open.html
    Or copy and paste one of these URLs:
        http://localhost:8888/?token=9e87e967a70cc021227215fa781992c7311683555ba9725e
     or http://127.0.0.1:8888/?token=9e87e967a70cc021227215fa781992c7311683555ba9725e
[I 15:52:03.970 NotebookApp] 302 GET /?token=9e87e967a70cc021227215fa781992c7311683555ba9725e (127.0.0.1) 1.46ms
[E 15:52:04.051 NotebookApp] Could not open static file ''
[W 15:52:04.125 NotebookApp] 404 GET /static/components/react/react-dom.production.min.js (127.0.0.1) 16.79ms referer=http://127.0.0.1:8888/tree?token=9e87e967a70cc021227215fa781992c7311683555ba9725e
[W 15:52:04.212 NotebookApp] 404 GET /static/components/react/react-dom.production.min.js (127.0.0.1) 4.76ms referer=http://127.0.0.1:8888/tree?token=9e87e967a70cc021227215fa781992c7311683555ba9725e
[I 15:52:20.327 NotebookApp] Creating new notebook in
[W 15:52:20.881 NotebookApp] 404 GET /static/components/react/react-dom.production.min.js (127.0.0.1) 4.64ms referer=http://127.0.0.1:8888/notebooks/Untitled1.ipynb?kernel_name=python3
[W 15:52:20.963 NotebookApp] 404 GET /static/components/react/react-dom.production.min.js (127.0.0.1) 3.59ms referer=http://127.0.0.1:8888/notebooks/Untitled1.ipynb?kernel_name=python3
[I 15:52:21.533 NotebookApp] Kernel started: 2a156f78-aa9f-4c7a-aa66-81ff7aecb7e8
[W 15:52:21.545 NotebookApp] 404 GET /nbextensions/widgets/notebook/js/extension.js?v=20190812155025 (127.0.0.1) 7.87ms referer=http://127.0.0.1:8888/notebooks/Untitled1.ipynb?kernel_name=python3
19/08/12 20:52:24 WARN Utils: Your hostname, TM0493322 resolves to a loopback address: 127.0.1.1; using 10.0.75.1 instead (on interface eth3)
19/08/12 20:52:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
19/08/12 20:52:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
19/08/12 20:52:25 WARN DependencyUtils: Local jar /home/tbresee/hail/build/libs/hail-all-spark.jar does not exist, skipping.
19/08/12 20:52:26 INFO SparkContext: Running Spark version 2.4.3
19/08/12 20:52:26 INFO SparkContext: Submitted application: PySparkShell
19/08/12 20:52:26 INFO SecurityManager: Changing view acls to: tbresee
19/08/12 20:52:26 INFO SecurityManager: Changing modify acls to: tbresee
19/08/12 20:52:26 INFO SecurityManager: Changing view acls groups to:
19/08/12 20:52:26 INFO SecurityManager: Changing modify acls groups to:
19/08/12 20:52:26 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tbresee); groups with view permissions: Set(); users  with modify permissions: Set(tbresee); groups with modify permissions: Set()
19/08/12 20:52:26 INFO Utils: Successfully started service 'sparkDriver' on port 1317.
19/08/12 20:52:26 INFO SparkEnv: Registering MapOutputTracker
19/08/12 20:52:26 INFO SparkEnv: Registering BlockManagerMaster
19/08/12 20:52:26 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
19/08/12 20:52:26 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
19/08/12 20:52:26 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c7e0d0fa-bc35-4a03-a0ac-8b50bc96fbdf
19/08/12 20:52:26 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
19/08/12 20:52:26 INFO SparkEnv: Registering OutputCommitCoordinator
19/08/12 20:52:27 INFO Utils: Successfully started service 'SparkUI' on port 4040.
19/08/12 20:52:27 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.0.75.1:4040
19/08/12 20:52:27 ERROR SparkContext: Failed to add file:/home/tbresee/hail/build/libs/hail-all-spark.jar to Spark environment
java.io.FileNotFoundException: Jar /home/tbresee/hail/build/libs/hail-all-spark.jar not found
        at org.apache.spark.SparkContext.addJarFile$1(SparkContext.scala:1838)
        at org.apache.spark.SparkContext.addJar(SparkContext.scala:1868)
        at org.apache.spark.SparkContext$$anonfun$12.apply(SparkContext.scala:458)
        at org.apache.spark.SparkContext$$anonfun$12.apply(SparkContext.scala:458)
        at scala.collection.immutable.List.foreach(List.scala:392)
        at org.apache.spark.SparkContext.<init>(SparkContext.scala:458)
        at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
        at py4j.Gateway.invoke(Gateway.java:238)
        at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
        at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
        at py4j.GatewayConnection.run(GatewayConnection.java:238)
        at java.lang.Thread.run(Thread.java:748)
19/08/12 20:52:27 INFO Executor: Starting executor ID driver on host localhost
19/08/12 20:52:27 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 1319.
19/08/12 20:52:27 INFO NettyBlockTransferService: Server created on 10.0.75.1:1319
19/08/12 20:52:27 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
19/08/12 20:52:27 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.75.1, 1319, None)
19/08/12 20:52:27 INFO BlockManagerMasterEndpoint: Registering block manager 10.0.75.1:1319 with 366.3 MB RAM, BlockManagerId(driver, 10.0.75.1, 1319, None)
19/08/12 20:52:27 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.75.1, 1319, None)
19/08/12 20:52:27 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.75.1, 1319, None)
19/08/12 20:52:27 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/tbresee/spark-warehouse').
19/08/12 20:52:27 INFO SharedState: Warehouse path is 'file:/home/tbresee/spark-warehouse'.
19/08/12 20:52:28 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
[I 15:52:28.641 NotebookApp] Adapting from protocol version 5.1 (kernel 2a156f78-aa9f-4c7a-aa66-81ff7aecb7e8) to 5.3 (client).
[I 15:53:50.150 NotebookApp] Saving file at /Untitled1.ipynb
[I 15:54:21.440 NotebookApp] Saving file at /Untitled1.ipynb
^C[I 15:55:21.049 NotebookApp] interrupted
Serving notebooks from local directory: /home/tbresee
1 active kernel
The Jupyter Notebook is running at:
http://localhost:8888/?token=9e87e967a70cc021227215fa781992c7311683555ba9725e
 or http://127.0.0.1:8888/?token=9e87e967a70cc021227215fa781992c7311683555ba9725e
Shutdown this notebook server (y/[n])? ^C[C 15:55:21.168 NotebookApp] received signal 2, stopping
[I 15:55:21.174 NotebookApp] Shutting down 1 kernel
19/08/12 20:55:21 INFO SparkUI: Stopped Spark web UI at http://10.0.75.1:4040
19/08/12 20:55:21 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
19/08/12 20:55:21 INFO MemoryStore: MemoryStore cleared
19/08/12 20:55:21 INFO BlockManager: BlockManager stopped
19/08/12 20:55:21 INFO BlockManagerMaster: BlockManagerMaster stopped
19/08/12 20:55:21 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
19/08/12 20:55:21 INFO SparkContext: Successfully stopped SparkContext
19/08/12 20:55:21 INFO ShutdownHookManager: Shutdown hook called
19/08/12 20:55:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-43062682-f6b0-4877-8442-8bf0e65ad28b
19/08/12 20:55:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-6b7dac63-45c4-4f0b-8133-11ffec7cb1c5/pyspark-2704a284-04a3-4d21-9725-4ad9d5071024
19/08/12 20:55:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-6b7dac63-45c4-4f0b-8133-11ffec7cb1c5
[I 15:55:22.186 NotebookApp] Kernel shutdown: 2a156f78-aa9f-4c7a-aa66-81ff7aecb7e8
tbresee@TM0493322:~$ pwd
/home/tbresee
tbresee@TM0493322:~$ vi ./bashrc
tbresee@TM0493322:~$ pwd
/home/tbresee
tbresee@TM0493322:~$ ls -la
total 1016580
drwxr-xr-x 1 tbresee tbresee      4096 Aug 12 15:56  .
drwxr-xr-x 1 root    root         4096 Sep  5  2018  ..
drwxrwxrwx 1 tbresee tbresee      4096 Sep  5  2018  1
drwx------ 1 tbresee tbresee      4096 Sep  7  2018  .ansible
-rw------- 1 tbresee tbresee     12394 Aug 12 15:43  .bash_history
-rw-r--r-- 1 tbresee tbresee       220 Sep  5  2018  .bash_logout
-rw-r--r-- 1 tbresee tbresee      4510 Aug  8 11:49  .bashrc
drwx------ 1 tbresee tbresee      4096 Aug  7 11:30  .cache
drwxrwxrwx 1 tbresee tbresee      4096 Aug  9 15:28  .cassandra
-rw-rw-rw- 1 tbresee tbresee         0 Aug  8 10:29  --conf
drwxrwxrwx 1 tbresee tbresee      4096 Aug  7 11:09  .config
drwxrwxrwx 1 tbresee tbresee      4096 Aug  7 13:33  data
drwxrwxrwx 1 tbresee tbresee      4096 Jan 27  2019  docker-curriculum
drwxrwxrwx 1 tbresee tbresee      4096 Aug  5 09:57  .gradle
drwxrwxrwx 1 tbresee tbresee      4096 Aug  7 12:28  hadoop
-rw-rw-rw- 1 tbresee tbresee 345625475 Jan 21  2019  hadoop-3.2.0.tar.gz
drwxrwxrwx 1 tbresee tbresee      4096 Aug  5 10:00  hail
-rw-rw-rw- 1 tbresee tbresee    169250 Aug  7 11:13  hail-20190807-1054-0.2.19-c6ec8b76eb26.log
-rw-rw-rw- 1 tbresee tbresee   6838004 Aug  8 00:38  hail-20190807-1257-0.2.19-c6ec8b76eb26.log
-rw-rw-rw- 1 tbresee tbresee      8414 Aug 12 15:05  hail-20190812-1500-0.2.19-c6ec8b76eb26.log
-rw-rw-rw- 1 tbresee tbresee     15393 Aug  7 12:13 'HAIL 3.0.ipynb'
-rw-rw-rw- 1 tbresee tbresee     16338 Aug  7 12:53 'HAIL 4.0.ipynb'
-rw-rw-rw- 1 tbresee tbresee   1187904 Aug  8 11:50 'HAIL v74.ipynb'
-rw-rw-rw- 1 tbresee tbresee   1195446 Aug  8 10:51 'HAIL v75.ipynb'
-rw-rw-rw- 1 tbresee tbresee        43 Sep  7  2018  hosts.ini
-rw-rw-rw- 1 tbresee tbresee        24 Sep  5  2018  inventory
drwxrwxrwx 1 tbresee tbresee      4096 Aug 12 15:52  .ipynb_checkpoints
drwxr-xr-x 1 tbresee tbresee      4096 Aug  7 10:40  .ipython
-rw-rw-rw- 1 tbresee tbresee         0 Aug  8 10:29  --jars
drwxrwxrwx 1 tbresee tbresee      4096 Aug  7 11:06  .jupyter
drwx------ 1 tbresee tbresee      4096 Aug  7 10:39  .local
-rw-rw-rw- 1 tbresee tbresee         8 Sep  7  2018  main.retry
-rw-rw-rw- 1 tbresee tbresee       168 Sep  7  2018  main.yml
-rw-rw-rw- 1 tbresee tbresee       128 Sep  5  2018  new_nokia_new_service.yml
-rw-rw-rw- 1 tbresee tbresee       234 Sep  5  2018  play-1.yml
-rw-rw-rw- 1 tbresee tbresee       388 Sep  5  2018  playbook-ansible-1.yml
-rw-rw-rw- 1 tbresee tbresee       247 Sep  5  2018  playbook-ansible-2.yml
-rw-r--r-- 1 tbresee tbresee       655 Sep  5  2018  .profile
-rw------- 1 tbresee tbresee       456 Aug 12 15:39  .python_history
-rw------- 1 tbresee tbresee        60 Jan 22  2019  .rediscli_history
-rw------- 1 root    root        24576 Sep 20  2018  .sample.sql.swp
-rw------- 1 tbresee tbresee       795 Aug  7 12:30  .scala_history
drwxrwxrwx 1 tbresee tbresee      4096 Oct  2  2018  scripts
-rw-rw-rw- 1 tbresee tbresee 227893062 Oct 29  2018  spark-2.4.0-bin-hadoop2.7.tgz
-rw-rw-rw- 1 tbresee tbresee 227893062 Oct 29  2018  spark-2.4.0-bin-hadoop2.7.tgz.1
-rw-rw-rw- 1 tbresee tbresee     22215 Jan 22  2019  spark-2.4.0-bin-hadoop2.7.tgz.2
-rw-rw-rw- 1 tbresee tbresee 229988313 May  1 00:57  spark-2.4.3-bin-hadoop2.7.tgz
drwxrwxrwx 1 tbresee tbresee      4096 Sep  7  2018  srx
drwx------ 1 tbresee tbresee      4096 Aug  7 11:29  .ssh
-rw-r--r-- 1 tbresee tbresee         0 Sep  5  2018  .sudo_as_admin_successful
-rw-rw-rw- 1 tbresee tbresee         0 Sep  5  2018  test
-rw-rw-rw- 1 tbresee tbresee       235 Sep  5  2018  test1.yml
-rw-rw-rw- 1 tbresee tbresee       115 Sep  5  2018  testplaybook1.yml
-rw-rw-rw- 1 tbresee tbresee        12 Sep  5  2018  testplaybook.retry
-rw-rw-rw- 1 tbresee tbresee       125 Sep  5  2018  testplaybook.yml
-rw-rw-rw- 1 tbresee tbresee      9001 Aug 12 15:54  Untitled1.ipynb
-rw-rw-rw- 1 tbresee tbresee     12207 Aug  7 10:42  Untitled.ipynb
-rw------- 1 tbresee tbresee      7510 Aug 12 15:56  .viminfo
tbresee@TM0493322:~$ vi ./bashrc
tbresee@TM0493322:~$ # vi .bashrc
tbresee@TM0493322:~$ vi .bashrc
tbresee@TM0493322:~$ pwd
/home/tbresee
tbresee@TM0493322:~$ cd ./.local/lib/python3.6/site-packages
tbresee@TM0493322:~/.local/lib/python3.6/site-packages$ pwd
/home/tbresee/.local/lib/python3.6/site-packages
tbresee@TM0493322:~/.local/lib/python3.6/site-packages$ vi .bashrc
tbresee@TM0493322:~/.local/lib/python3.6/site-packages$ cd ~
tbresee@TM0493322:~$ vi .bashrc
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$ cd ~
tbresee@TM0493322:~$ pwd
/home/tbresee
tbresee@TM0493322:~$ # printed wd
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$ pyspark \
>    --jars $HAIL_HOME/hail-all-spark.jar \
>    --conf spark.driver.extraClassPath=$HAIL_HOME/hail-all-spark.jar \
>    --conf spark.executor.extraClassPath=$HAIL_HOME/hail-all-spark.jar \
>    --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
>    --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator
[I 16:02:48.523 NotebookApp] Serving notebooks from local directory: /home/tbresee
[I 16:02:48.523 NotebookApp] The Jupyter Notebook is running at:
[I 16:02:48.524 NotebookApp] http://localhost:8888/?token=f53f29d980aaf9dcc337a8186b8d5302ddb30932f8fb1121
[I 16:02:48.524 NotebookApp]  or http://127.0.0.1:8888/?token=f53f29d980aaf9dcc337a8186b8d5302ddb30932f8fb1121
[I 16:02:48.524 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[W 16:02:48.533 NotebookApp] No web browser found: could not locate runnable browser.
[C 16:02:48.534 NotebookApp]

    To access the notebook, open this file in a browser:
        file:///home/tbresee/.local/share/jupyter/runtime/nbserver-244-open.html
    Or copy and paste one of these URLs:
        http://localhost:8888/?token=f53f29d980aaf9dcc337a8186b8d5302ddb30932f8fb1121
     or http://127.0.0.1:8888/?token=f53f29d980aaf9dcc337a8186b8d5302ddb30932f8fb1121
[I 16:03:04.026 NotebookApp] 302 GET /?token=f53f29d980aaf9dcc337a8186b8d5302ddb30932f8fb1121 (127.0.0.1) 0.92ms
[E 16:03:04.085 NotebookApp] Could not open static file ''
[W 16:03:04.143 NotebookApp] 404 GET /static/components/react/react-dom.production.min.js (127.0.0.1) 15.91ms referer=http://127.0.0.1:8888/tree?token=f53f29d980aaf9dcc337a8186b8d5302ddb30932f8fb1121
[W 16:03:04.222 NotebookApp] 404 GET /static/components/react/react-dom.production.min.js (127.0.0.1) 4.28ms referer=http://127.0.0.1:8888/tree?token=f53f29d980aaf9dcc337a8186b8d5302ddb30932f8fb1121
[I 16:03:11.117 NotebookApp] Creating new notebook in
[W 16:03:11.566 NotebookApp] 404 GET /static/components/react/react-dom.production.min.js (127.0.0.1) 4.93ms referer=http://127.0.0.1:8888/notebooks/Untitled2.ipynb?kernel_name=python3
[W 16:03:11.624 NotebookApp] 404 GET /static/components/react/react-dom.production.min.js (127.0.0.1) 4.30ms referer=http://127.0.0.1:8888/notebooks/Untitled2.ipynb?kernel_name=python3
[I 16:03:12.109 NotebookApp] Kernel started: 1909cc59-e232-476b-8e6d-14f87a595206
[W 16:03:12.124 NotebookApp] 404 GET /nbextensions/widgets/notebook/js/extension.js?v=20190812160248 (127.0.0.1) 10.21ms referer=http://127.0.0.1:8888/notebooks/Untitled2.ipynb?kernel_name=python3
19/08/12 21:03:14 WARN Utils: Your hostname, TM0493322 resolves to a loopback address: 127.0.1.1; using 10.0.75.1 instead (on interface eth3)
19/08/12 21:03:14 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
19/08/12 21:03:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
19/08/12 21:03:15 WARN DependencyUtils: Local jar /home/tbresee/hail/hail-all-spark.jar does not exist, skipping.
19/08/12 21:03:16 INFO SparkContext: Running Spark version 2.4.3
19/08/12 21:03:16 INFO SparkContext: Submitted application: PySparkShell
19/08/12 21:03:16 INFO SecurityManager: Changing view acls to: tbresee
19/08/12 21:03:16 INFO SecurityManager: Changing modify acls to: tbresee
19/08/12 21:03:16 INFO SecurityManager: Changing view acls groups to:
19/08/12 21:03:16 INFO SecurityManager: Changing modify acls groups to:
19/08/12 21:03:16 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tbresee); groups with view permissions: Set(); users  with modify permissions: Set(tbresee); groups with modify permissions: Set()
19/08/12 21:03:16 INFO Utils: Successfully started service 'sparkDriver' on port 1334.
19/08/12 21:03:16 INFO SparkEnv: Registering MapOutputTracker
19/08/12 21:03:16 INFO SparkEnv: Registering BlockManagerMaster
19/08/12 21:03:16 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
19/08/12 21:03:16 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
19/08/12 21:03:16 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-3c8044af-eb0b-48b6-9f0d-e461894ce5a6
19/08/12 21:03:16 INFO MemoryStore: MemoryStore started with capacity 366.3 MB
19/08/12 21:03:16 INFO SparkEnv: Registering OutputCommitCoordinator
19/08/12 21:03:17 INFO Utils: Successfully started service 'SparkUI' on port 4040.
19/08/12 21:03:17 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.0.75.1:4040
19/08/12 21:03:17 ERROR SparkContext: Failed to add file:/home/tbresee/hail/hail-all-spark.jar to Spark environment
java.io.FileNotFoundException: Jar /home/tbresee/hail/hail-all-spark.jar not found
        at org.apache.spark.SparkContext.addJarFile$1(SparkContext.scala:1838)
        at org.apache.spark.SparkContext.addJar(SparkContext.scala:1868)
        at org.apache.spark.SparkContext$$anonfun$12.apply(SparkContext.scala:458)
        at org.apache.spark.SparkContext$$anonfun$12.apply(SparkContext.scala:458)
        at scala.collection.immutable.List.foreach(List.scala:392)
        at org.apache.spark.SparkContext.<init>(SparkContext.scala:458)
        at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
        at py4j.Gateway.invoke(Gateway.java:238)
        at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
        at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
        at py4j.GatewayConnection.run(GatewayConnection.java:238)
        at java.lang.Thread.run(Thread.java:748)
19/08/12 21:03:17 INFO Executor: Starting executor ID driver on host localhost
19/08/12 21:03:17 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 1335.
19/08/12 21:03:17 INFO NettyBlockTransferService: Server created on 10.0.75.1:1335
19/08/12 21:03:17 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
19/08/12 21:03:17 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.75.1, 1335, None)
19/08/12 21:03:17 INFO BlockManagerMasterEndpoint: Registering block manager 10.0.75.1:1335 with 366.3 MB RAM, BlockManagerId(driver, 10.0.75.1, 1335, None)
19/08/12 21:03:17 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.75.1, 1335, None)
19/08/12 21:03:17 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.75.1, 1335, None)
19/08/12 21:03:17 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/home/tbresee/spark-warehouse').
19/08/12 21:03:17 INFO SharedState: Warehouse path is 'file:/home/tbresee/spark-warehouse'.
19/08/12 21:03:18 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
[I 16:03:18.613 NotebookApp] Adapting from protocol version 5.1 (kernel 1909cc59-e232-476b-8e6d-14f87a595206) to 5.3 (client).
^C[I 16:03:47.211 NotebookApp] interrupted
Serving notebooks from local directory: /home/tbresee
1 active kernel
The Jupyter Notebook is running at:
http://localhost:8888/?token=f53f29d980aaf9dcc337a8186b8d5302ddb30932f8fb1121
 or http://127.0.0.1:8888/?token=f53f29d980aaf9dcc337a8186b8d5302ddb30932f8fb1121
Shutdown this notebook server (y/[n])? ^C[C 16:03:47.344 NotebookApp] received signal 2, stopping
[I 16:03:47.348 NotebookApp] Shutting down 1 kernel
19/08/12 21:03:47 INFO SparkUI: Stopped Spark web UI at http://10.0.75.1:4040
19/08/12 21:03:47 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
19/08/12 21:03:47 INFO MemoryStore: MemoryStore cleared
19/08/12 21:03:47 INFO BlockManager: BlockManager stopped
19/08/12 21:03:47 INFO BlockManagerMaster: BlockManagerMaster stopped
19/08/12 21:03:47 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
19/08/12 21:03:47 INFO SparkContext: Successfully stopped SparkContext
19/08/12 21:03:47 INFO ShutdownHookManager: Shutdown hook called
19/08/12 21:03:47 INFO ShutdownHookManager: Deleting directory /tmp/spark-fd01d34c-619d-4faa-8cbd-0e7072a9a5e0
19/08/12 21:03:47 INFO ShutdownHookManager: Deleting directory /tmp/spark-fd01d34c-619d-4faa-8cbd-0e7072a9a5e0/pyspark-dac82d5c-bcf7-4998-b391-e2debfd6ba30
19/08/12 21:03:47 INFO ShutdownHookManager: Deleting directory /tmp/spark-5e944a37-da1a-4883-a759-b612fb272c4a
[I 16:03:47.856 NotebookApp] Kernel shutdown: 1909cc59-e232-476b-8e6d-14f87a595206
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$ source .bashrc
tbresee@TM0493322:~$ source .bashr
-bash: .bashr: No such file or directory
tbresee@TM0493322:~$ source .bashrc
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$ pwd
/home/tbresee
tbresee@TM0493322:~$ type .bashrc
-bash: type: .bashrc: not found
tbresee@TM0493322:~$ cat .bashrc
# ~/.bashrc: executed by bash(1) for non-login shells.
# see /usr/share/doc/bash/examples/startup-files (in the package bash-doc)
# for examples

# If not running interactively, don't do anything





export HAIL_HOME=/home/tbresee/.local/lib/python3.6/site-packages/hail


export PYSPARK_PYTHON="ipython"
export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64
export HADOOP_HOME=/home/tbresee/hadoop/hadoop-3.2.0
#   export HAIL_HOME=/home/tbresee/hail

export SPARK_HOME=~/hadoop/spark-2.4.3-bin-hadoop2.7
export PATH=$SPARK_HOME/bin:$PATH

export PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH
export PYSPARK_DRIVER_PYTHON="jupyter"
export PYSPARK_DRIVER_PYTHON_OPTS="notebook"
#export PYSPARK_PYTHON=python3
export SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar

#
#
#


case $- in
    *i*) ;;
      *) return;;
esac

# don't put duplicate lines or lines starting with space in the history.
# See bash(1) for more options
HISTCONTROL=ignoreboth

# append to the history file, don't overwrite it
shopt -s histappend

# for setting history length see HISTSIZE and HISTFILESIZE in bash(1)
HISTSIZE=1000
HISTFILESIZE=2000

# check the window size after each command and, if necessary,
# update the values of LINES and COLUMNS.
shopt -s checkwinsize

# If set, the pattern "**" used in a pathname expansion context will
# match all files and zero or more directories and subdirectories.
#shopt -s globstar

# make less more friendly for non-text input files, see lesspipe(1)
[ -x /usr/bin/lesspipe ] && eval "$(SHELL=/bin/sh lesspipe)"

# set variable identifying the chroot you work in (used in the prompt below)
if [ -z "${debian_chroot:-}" ] && [ -r /etc/debian_chroot ]; then
    debian_chroot=$(cat /etc/debian_chroot)
fi

# set a fancy prompt (non-color, unless we know we "want" color)
case "$TERM" in
    xterm-color|*-256color) color_prompt=yes;;
esac

# uncomment for a colored prompt, if the terminal has the capability; turned
# off by default to not distract the user: the focus in a terminal window
# should be on the output of commands, not on the prompt
#force_color_prompt=yes

if [ -n "$force_color_prompt" ]; then
    if [ -x /usr/bin/tput ] && tput setaf 1 >&/dev/null; then
        # We have color support; assume it's compliant with Ecma-48
        # (ISO/IEC-6429). (Lack of such support is extremely rare, and such
        # a case would tend to support setf rather than setaf.)
        color_prompt=yes
    else
        color_prompt=
    fi
fi

if [ "$color_prompt" = yes ]; then
    PS1='${debian_chroot:+($debian_chroot)}\[\033[01;32m\]\u@\h\[\033[00m\]:\[\033[01;34m\]\w\[\033[00m\]\$ '
else
    PS1='${debian_chroot:+($debian_chroot)}\u@\h:\w\$ '
fi
unset color_prompt force_color_prompt

# If this is an xterm set the title to user@host:dir
case "$TERM" in
xterm*|rxvt*)
    PS1="\[\e]0;${debian_chroot:+($debian_chroot)}\u@\h: \w\a\]$PS1"
    ;;
*)
    ;;
esac

# enable color support of ls and also add handy aliases
if [ -x /usr/bin/dircolors ]; then
    test -r ~/.dircolors && eval "$(dircolors -b ~/.dircolors)" || eval "$(dircolors -b)"
    alias ls='ls --color=auto'
    #alias dir='dir --color=auto'
    #alias vdir='vdir --color=auto'

    alias grep='grep --color=auto'
    alias fgrep='fgrep --color=auto'
    alias egrep='egrep --color=auto'
fi

# colored GCC warnings and errors
#export GCC_COLORS='error=01;31:warning=01;35:note=01;36:caret=01;32:locus=01:quote=01'

# some more ls aliases
alias ll='ls -alF'
alias la='ls -A'
alias l='ls -CF'

# Add an "alert" alias for long running commands.  Use like so:
#   sleep 10; alert
alias alert='notify-send --urgency=low -i "$([ $? = 0 ] && echo terminal || echo error)" "$(history|tail -n1|sed -e '\''s/^\s*[0-9]\+\s*//;s/[;&|]\s*alert$//'\'')"'

# Alias definitions.
# You may want to put all your additions into a separate file like
# ~/.bash_aliases, instead of adding them here directly.
# See /usr/share/doc/bash-doc/examples in the bash-doc package.

if [ -f ~/.bash_aliases ]; then
    . ~/.bash_aliases
fi

alias python=python3
# enable programmable completion features (you don't need to enable
# this, if it's already enabled in /etc/bash.bashrc and /etc/profile
# sources /etc/bash.bashrc).
if ! shopt -oq posix; then
  if [ -f /usr/share/bash-completion/bash_completion ]; then
    . /usr/share/bash-completion/bash_completion
  elif [ -f /etc/bash_completion ]; then
    . /etc/bash_completion
  fi
fi
export DOCKER_HOST=tcp://localhost:2375
export DOCKER_HOST=tcp://localhost:2375
export DOCKER_HOST=tcp://localhost:2375
PATH=$HOME/.local/bin:$PATH
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$ pwd
/home/tbresee
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$ pyspark \
>    --jars $HAIL_HOME/hail-all-spark.jar \
>    --conf spark.driver.extraClassPath=$HAIL_HOME/hail-all-spark.jar \
>    --conf spark.executor.extraClassPath=$HAIL_HOME/hail-all-spark.jar \
>    --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
>    --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator
[I 16:04:39.008 NotebookApp] Serving notebooks from local directory: /home/tbresee
[I 16:04:39.009 NotebookApp] The Jupyter Notebook is running at:
[I 16:04:39.010 NotebookApp] http://localhost:8888/?token=b8589bbb7ae774db32d953b586d2025b90eb550cd6e743ff
[I 16:04:39.010 NotebookApp]  or http://127.0.0.1:8888/?token=b8589bbb7ae774db32d953b586d2025b90eb550cd6e743ff
[I 16:04:39.011 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[W 16:04:39.020 NotebookApp] No web browser found: could not locate runnable browser.
[C 16:04:39.020 NotebookApp]

    To access the notebook, open this file in a browser:
        file:///home/tbresee/.local/share/jupyter/runtime/nbserver-444-open.html
    Or copy and paste one of these URLs:
        http://localhost:8888/?token=b8589bbb7ae774db32d953b586d2025b90eb550cd6e743ff
     or http://127.0.0.1:8888/?token=b8589bbb7ae774db32d953b586d2025b90eb550cd6e743ff
[I 16:04:53.622 NotebookApp] 302 GET /?token=b8589bbb7ae774db32d953b586d2025b90eb550cd6e743ff (127.0.0.1) 2.11ms
[E 16:04:53.676 NotebookApp] Could not open static file ''
[W 16:04:53.738 NotebookApp] 404 GET /static/components/react/react-dom.production.min.js (127.0.0.1) 17.28ms referer=http://127.0.0.1:8888/tree?token=b8589bbb7ae774db32d953b586d2025b90eb550cd6e743ff
[W 16:04:53.827 NotebookApp] 404 GET /static/components/react/react-dom.production.min.js (127.0.0.1) 2.34ms referer=http://127.0.0.1:8888/tree?token=b8589bbb7ae774db32d953b586d2025b90eb550cd6e743ff
[I 16:04:57.109 NotebookApp] Creating new notebook in
[W 16:04:57.533 NotebookApp] 404 GET /static/components/react/react-dom.production.min.js (127.0.0.1) 4.70ms referer=http://127.0.0.1:8888/notebooks/Untitled2.ipynb?kernel_name=python3
[W 16:04:57.593 NotebookApp] 404 GET /static/components/react/react-dom.production.min.js (127.0.0.1) 2.86ms referer=http://127.0.0.1:8888/notebooks/Untitled2.ipynb?kernel_name=python3
[I 16:04:57.996 NotebookApp] Kernel started: a7b36357-c9bb-49ac-8dd9-a2f53a55d069
[W 16:04:58.007 NotebookApp] 404 GET /nbextensions/widgets/notebook/js/extension.js?v=20190812160438 (127.0.0.1) 7.38ms referer=http://127.0.0.1:8888/notebooks/Untitled2.ipynb?kernel_name=python3
19/08/12 21:05:00 WARN Utils: Your hostname, TM0493322 resolves to a loopback address: 127.0.1.1; using 10.0.75.1 instead (on interface eth3)
19/08/12 21:05:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
19/08/12 21:05:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[I 16:05:04.371 NotebookApp] Adapting from protocol version 5.1 (kernel a7b36357-c9bb-49ac-8dd9-a2f53a55d069) to 5.3 (client).
[I 16:06:58.715 NotebookApp] Saving file at /REPRODUCIBLE_HAIL_0.2.ipynb
[I 16:08:57.915 NotebookApp] Saving file at /REPRODUCIBLE_HAIL_0.2.ipynb
^C[I 16:09:03.561 NotebookApp] interrupted
Serving notebooks from local directory: /home/tbresee
1 active kernel
The Jupyter Notebook is running at:
http://localhost:8888/?token=b8589bbb7ae774db32d953b586d2025b90eb550cd6e743ff
 or http://127.0.0.1:8888/?token=b8589bbb7ae774db32d953b586d2025b90eb550cd6e743ff
Shutdown this notebook server (y/[n])? ^C[C 16:09:03.705 NotebookApp] received signal 2, stopping
[I 16:09:03.715 NotebookApp] Shutting down 1 kernel
[I 16:09:04.728 NotebookApp] Kernel shutdown: a7b36357-c9bb-49ac-8dd9-a2f53a55d069
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$ pyspark
[I 16:09:26.400 NotebookApp] Serving notebooks from local directory: /home/tbresee
[I 16:09:26.401 NotebookApp] The Jupyter Notebook is running at:
[I 16:09:26.402 NotebookApp] http://localhost:8888/?token=518a027d074bf65d7e18b797ba1c143564f31146fbf5c5ff
[I 16:09:26.403 NotebookApp]  or http://127.0.0.1:8888/?token=518a027d074bf65d7e18b797ba1c143564f31146fbf5c5ff
[I 16:09:26.404 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[W 16:09:26.413 NotebookApp] No web browser found: could not locate runnable browser.
[C 16:09:26.414 NotebookApp]

    To access the notebook, open this file in a browser:
        file:///home/tbresee/.local/share/jupyter/runtime/nbserver-624-open.html
    Or copy and paste one of these URLs:
        http://localhost:8888/?token=518a027d074bf65d7e18b797ba1c143564f31146fbf5c5ff
     or http://127.0.0.1:8888/?token=518a027d074bf65d7e18b797ba1c143564f31146fbf5c5ff
^C[I 16:09:28.242 NotebookApp] interrupted
Serving notebooks from local directory: /home/tbresee
0 active kernels
The Jupyter Notebook is running at:
http://localhost:8888/?token=518a027d074bf65d7e18b797ba1c143564f31146fbf5c5ff
 or http://127.0.0.1:8888/?token=518a027d074bf65d7e18b797ba1c143564f31146fbf5c5ff
Shutdown this notebook server (y/[n])? ^C[C 16:09:28.360 NotebookApp] received signal 2, stopping
[I 16:09:28.365 NotebookApp] Shutting down 0 kernels
tbresee@TM0493322:~$ python3
Python 3.6.7 (default, Oct 22 2018, 11:32:17)
[GCC 8.2.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> sc
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'sc' is not defined
>>> import pyspark
>>> sc = pyspark.SparkContext()
19/08/12 21:10:27 WARN Utils: Your hostname, TM0493322 resolves to a loopback address: 127.0.1.1; using 10.0.75.1 instead (on interface eth3)
19/08/12 21:10:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
19/08/12 21:10:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
>>> sc
<SparkContext master=local[*] appName=pyspark-shell>
>>> sc.stop()
>>> quit
Use quit() or Ctrl-D (i.e. EOF) to exit
>>> quit()
tbresee@TM0493322:~$






























---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-6-1fdeb34ea907> in <module>
----> 1 mt = hl.balding_nichols_model(n_populations=3, n_samples=50, n_variants=100)

</home/tbresee/.local/lib/python3.6/site-packages/decorator.py:decorator-gen-1471> in balding_nichols_model(n_populations, n_samples, n_variants, n_partitions, pop_dist, fst, af_dist, reference_genome, mixture)

~/.local/lib/python3.6/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs)
    582     @decorator
    583     def wrapper(__original_func, *args, **kwargs):
--> 584         args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method)
    585         return __original_func(*args_, **kwargs_)
    586 

~/.local/lib/python3.6/site-packages/hail/typecheck/check.py in check_all(f, args, kwargs, checks, is_method)
    510                     if i < len(args):
    511                         arg = args[i]
--> 512                         args_.append(checker.check(arg, name, arg_name))
    513                     # passed as keyword
    514                     else:

~/.local/lib/python3.6/site-packages/hail/typecheck/check.py in check(self, x, caller, param)
     54         for tc in self.checkers:
     55             try:
---> 56                 return tc.check(x, caller, param)
     57             except TypecheckFailure:
     58                 pass

~/.local/lib/python3.6/site-packages/hail/typecheck/check.py in check(self, x, caller, param)
    301         for tc, f in self.fs:
    302             try:
--> 303                 return f(tc.check(x, caller, param))
    304             except TypecheckFailure:
    305                 pass

~/.local/lib/python3.6/site-packages/hail/genetics/reference_genome.py in <lambda>(x)
      8 
      9 rg_type = lazy()
---> 10 reference_genome_type = oneof(transformed((str, lambda x: hl.get_reference(x))), rg_type)
     11 
     12 

~/.local/lib/python3.6/site-packages/hail/context.py in get_reference(name)
    356     :class:`.ReferenceGenome`
    357     """
--> 358     Env.hc()
    359     if name == 'default':
    360         return default_reference()

~/.local/lib/python3.6/site-packages/hail/utils/java.py in hc()
     61             import sys
     62             sys.stderr.write("Initializing Spark and Hail with default parameters...\n")
---> 63             init()
     64             assert Env._hc is not None
     65         return Env._hc

</home/tbresee/.local/lib/python3.6/site-packages/decorator.py:decorator-gen-1189> in init(sc, app_name, master, local, log, quiet, append, min_block_size, branching_factor, tmp_dir, default_reference, idempotent, global_seed, _optimizer_iterations, _backend)

~/.local/lib/python3.6/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs)
    583     def wrapper(__original_func, *args, **kwargs):
    584         args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method)
--> 585         return __original_func(*args_, **kwargs_)
    586 
    587     return wrapper

~/.local/lib/python3.6/site-packages/hail/context.py in init(sc, app_name, master, local, log, quiet, append, min_block_size, branching_factor, tmp_dir, default_reference, idempotent, global_seed, _optimizer_iterations, _backend)
    262                 min_block_size, branching_factor, tmp_dir,
    263                 default_reference, idempotent, global_seed,
--> 264                 _optimizer_iterations,_backend)
    265 
    266 

</home/tbresee/.local/lib/python3.6/site-packages/decorator.py:decorator-gen-1187> in __init__(self, sc, app_name, master, local, log, quiet, append, min_block_size, branching_factor, tmp_dir, default_reference, idempotent, global_seed, optimizer_iterations, _backend)

~/.local/lib/python3.6/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs)
    583     def wrapper(__original_func, *args, **kwargs):
    584         args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method)
--> 585         return __original_func(*args_, **kwargs_)
    586 
    587     return wrapper

~/.local/lib/python3.6/site-packages/hail/context.py in __init__(self, sc, app_name, master, local, log, quiet, append, min_block_size, branching_factor, tmp_dir, default_reference, idempotent, global_seed, optimizer_iterations, _backend)
     97             self._jhc = self._hail.HailContext.apply(
     98                 jsc, app_name, joption(master), local, log, True, append,
---> 99                 min_block_size, branching_factor, tmp_dir, optimizer_iterations)
    100 
    101         self._jsc = self._jhc.sc()

~/hadoop/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py in __call__(self, *args)
   1255         answer = self.gateway_client.send_command(command)
   1256         return_value = get_return_value(
-> 1257             answer, self.gateway_client, self.target_id, self.name)
   1258 
   1259         for temp_arg in temp_args:

~/hadoop/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/utils.py in deco(*a, **kw)
     61     def deco(*a, **kw):
     62         try:
---> 63             return f(*a, **kw)
     64         except py4j.protocol.Py4JJavaError as e:
     65             s = e.java_exception.toString()

~/hadoop/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    326                 raise Py4JJavaError(
    327                     "An error occurred while calling {0}{1}{2}.\n".
--> 328                     format(target_id, ".", name), value)
    329             else:
    330                 raise Py4JError(

Py4JJavaError: An error occurred while calling z:is.hail.HailContext.apply.
: org.apache.spark.SparkException: Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
java.lang.reflect.Constructor.newInstance(Constructor.java:423)
py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
py4j.Gateway.invoke(Gateway.java:238)
py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
py4j.GatewayConnection.run(GatewayConnection.java:238)
java.lang.Thread.run(Thread.java:748)
	at org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(SparkContext.scala:2483)
	at org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(SparkContext.scala:2479)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2479)
	at org.apache.spark.SparkContext$.markPartiallyConstructed(SparkContext.scala:2568)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:85)
	at is.hail.HailContext$.configureAndCreateSparkContext(HailContext.scala:137)
	at is.hail.HailContext$.apply(HailContext.scala:273)
	at is.hail.HailContext.apply(HailContext.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)












spark.driver.allowMultipleContexts = true



pyspark \
   --jars $HAIL_HOME/hail-all-spark.jar \
   --conf spark.driver.extraClassPath=$HAIL_HOME/hail-all-spark.jar \
   --conf spark.executor.extraClassPath=$HAIL_HOME/hail-all-spark.jar \
   --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
   --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator \
   --conf spark.driver.allowMultipleContexts=TRUE






tbresee@TM0493322:~$ pwd
/home/tbresee
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$ ls -la
total 1016604
drwxr-xr-x 1 tbresee tbresee      4096 Aug 12 16:10  .
drwxr-xr-x 1 root    root         4096 Sep  5  2018  ..
drwxrwxrwx 1 tbresee tbresee      4096 Sep  5  2018  1
drwx------ 1 tbresee tbresee      4096 Sep  7  2018  .ansible
-rw------- 1 tbresee tbresee     14313 Aug 12 16:11  .bash_history
-rw-r--r-- 1 tbresee tbresee       220 Sep  5  2018  .bash_logout
-rw-r--r-- 1 tbresee tbresee      4591 Aug 12 16:00  .bashrc
drwx------ 1 tbresee tbresee      4096 Aug  7 11:30  .cache
drwxrwxrwx 1 tbresee tbresee      4096 Aug  9 15:28  .cassandra
-rw-rw-rw- 1 tbresee tbresee         0 Aug  8 10:29  --conf
drwxrwxrwx 1 tbresee tbresee      4096 Aug  7 11:09  .config
drwxrwxrwx 1 tbresee tbresee      4096 Aug  7 13:33  data
drwxrwxrwx 1 tbresee tbresee      4096 Jan 27  2019  docker-curriculum
drwxrwxrwx 1 tbresee tbresee      4096 Aug  5 09:57  .gradle
drwxrwxrwx 1 tbresee tbresee      4096 Aug  7 12:28  hadoop
-rw-rw-rw- 1 tbresee tbresee 345625475 Jan 21  2019  hadoop-3.2.0.tar.gz
drwxrwxrwx 1 tbresee tbresee      4096 Aug  5 10:00  hail
-rw-rw-rw- 1 tbresee tbresee    169250 Aug  7 11:13  hail-20190807-1054-0.2.19-c6ec8b76eb26.log
-rw-rw-rw- 1 tbresee tbresee   6838004 Aug  8 00:38  hail-20190807-1257-0.2.19-c6ec8b76eb26.log
-rw-rw-rw- 1 tbresee tbresee      8414 Aug 12 15:05  hail-20190812-1500-0.2.19-c6ec8b76eb26.log
-rw-rw-rw- 1 tbresee tbresee      1257 Aug 12 16:09  hail-20190812-1607-0.2.19-c6ec8b76eb26.log
-rw-rw-rw- 1 tbresee tbresee     15393 Aug  7 12:13 'HAIL 3.0.ipynb'
-rw-rw-rw- 1 tbresee tbresee     16338 Aug  7 12:53 'HAIL 4.0.ipynb'
-rw-rw-rw- 1 tbresee tbresee   1187904 Aug  8 11:50 'HAIL v74.ipynb'
-rw-rw-rw- 1 tbresee tbresee   1195446 Aug  8 10:51 'HAIL v75.ipynb'
-rw-rw-rw- 1 tbresee tbresee        43 Sep  7  2018  hosts.ini
-rw-rw-rw- 1 tbresee tbresee        24 Sep  5  2018  inventory
drwxrwxrwx 1 tbresee tbresee      4096 Aug 12 16:05  .ipynb_checkpoints
drwxr-xr-x 1 tbresee tbresee      4096 Aug  7 10:40  .ipython
-rw-rw-rw- 1 tbresee tbresee         0 Aug  8 10:29  --jars
drwxrwxrwx 1 tbresee tbresee      4096 Aug  7 11:06  .jupyter
drwx------ 1 tbresee tbresee      4096 Aug  7 10:39  .local
-rw-rw-rw- 1 tbresee tbresee         8 Sep  7  2018  main.retry
-rw-rw-rw- 1 tbresee tbresee       168 Sep  7  2018  main.yml
-rw-rw-rw- 1 tbresee tbresee       128 Sep  5  2018  new_nokia_new_service.yml
-rw-rw-rw- 1 tbresee tbresee       234 Sep  5  2018  play-1.yml
-rw-rw-rw- 1 tbresee tbresee       388 Sep  5  2018  playbook-ansible-1.yml
-rw-rw-rw- 1 tbresee tbresee       247 Sep  5  2018  playbook-ansible-2.yml
-rw-r--r-- 1 tbresee tbresee       655 Sep  5  2018  .profile
-rw------- 1 tbresee tbresee       527 Aug 12 16:10  .python_history
-rw------- 1 tbresee tbresee        60 Jan 22  2019  .rediscli_history
-rw-rw-rw- 1 tbresee tbresee     28126 Aug 12 16:08  REPRODUCIBLE_HAIL_0.2.ipynb
-rw-rw-rw- 1 tbresee tbresee        72 Aug 12 16:03  REPRODUCIBLE_HAIL.ipynb
-rw------- 1 root    root        24576 Sep 20  2018  .sample.sql.swp
-rw------- 1 tbresee tbresee       795 Aug  7 12:30  .scala_history
drwxrwxrwx 1 tbresee tbresee      4096 Oct  2  2018  scripts
-rw-rw-rw- 1 tbresee tbresee 227893062 Oct 29  2018  spark-2.4.0-bin-hadoop2.7.tgz
-rw-rw-rw- 1 tbresee tbresee 227893062 Oct 29  2018  spark-2.4.0-bin-hadoop2.7.tgz.1
-rw-rw-rw- 1 tbresee tbresee     22215 Jan 22  2019  spark-2.4.0-bin-hadoop2.7.tgz.2
-rw-rw-rw- 1 tbresee tbresee 229988313 May  1 00:57  spark-2.4.3-bin-hadoop2.7.tgz
drwxrwxrwx 1 tbresee tbresee      4096 Sep  7  2018  srx
drwx------ 1 tbresee tbresee      4096 Aug  7 11:29  .ssh
-rw-r--r-- 1 tbresee tbresee         0 Sep  5  2018  .sudo_as_admin_successful
-rw-rw-rw- 1 tbresee tbresee         0 Sep  5  2018  test
-rw-rw-rw- 1 tbresee tbresee       235 Sep  5  2018  test1.yml
-rw-rw-rw- 1 tbresee tbresee       115 Sep  5  2018  testplaybook1.yml
-rw-rw-rw- 1 tbresee tbresee        12 Sep  5  2018  testplaybook.retry
-rw-rw-rw- 1 tbresee tbresee       125 Sep  5  2018  testplaybook.yml
-rw-rw-rw- 1 tbresee tbresee      9001 Aug 12 15:54  Untitled1.ipynb
-rw-rw-rw- 1 tbresee tbresee     12207 Aug  7 10:42  Untitled.ipynb
-rw------- 1 tbresee tbresee      7627 Aug 12 16:00  .viminfo
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$ vi .bashrc
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$ # TAKE 2
tbresee@TM0493322:~$
tbresee@TM0493322:~$ # --- THIS IS A NEW WAY OF STARTING IT DIRECTLY VIA PYSPARK AND DIRECT POINTING ---
tbresee@TM0493322:~$ pyspark \
>    --jars $HAIL_HOME/hail-all-spark.jar \
>    --conf spark.driver.extraClassPath=$HAIL_HOME/hail-all-spark.jar \
>    --conf spark.executor.extraClassPath=$HAIL_HOME/hail-all-spark.jar \
>    --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
>    --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator
[I 16:13:12.376 NotebookApp] Serving notebooks from local directory: /home/tbresee
[I 16:13:12.377 NotebookApp] The Jupyter Notebook is running at:
[I 16:13:12.378 NotebookApp] http://localhost:8888/?token=586ee4e03b6b1db9df1f8a7c32507a71952200e88c73d85c
[I 16:13:12.379 NotebookApp]  or http://127.0.0.1:8888/?token=586ee4e03b6b1db9df1f8a7c32507a71952200e88c73d85c
[I 16:13:12.380 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[W 16:13:12.389 NotebookApp] No web browser found: could not locate runnable browser.
[C 16:13:12.389 NotebookApp]

    To access the notebook, open this file in a browser:
        file:///home/tbresee/.local/share/jupyter/runtime/nbserver-28-open.html
    Or copy and paste one of these URLs:
        http://localhost:8888/?token=586ee4e03b6b1db9df1f8a7c32507a71952200e88c73d85c
     or http://127.0.0.1:8888/?token=586ee4e03b6b1db9df1f8a7c32507a71952200e88c73d85c
[I 16:13:22.665 NotebookApp] 302 GET /?token=586ee4e03b6b1db9df1f8a7c32507a71952200e88c73d85c (127.0.0.1) 0.98ms
[E 16:13:22.735 NotebookApp] Could not open static file ''
[W 16:13:22.799 NotebookApp] 404 GET /static/components/react/react-dom.production.min.js (127.0.0.1) 15.24ms referer=http://127.0.0.1:8888/tree?token=586ee4e03b6b1db9df1f8a7c32507a71952200e88c73d85c
[W 16:13:22.885 NotebookApp] 404 GET /static/components/react/react-dom.production.min.js (127.0.0.1) 2.53ms referer=http://127.0.0.1:8888/tree?token=586ee4e03b6b1db9df1f8a7c32507a71952200e88c73d85c
[W 16:13:36.581 NotebookApp] 404 GET /static/components/react/react-dom.production.min.js (127.0.0.1) 4.40ms referer=http://127.0.0.1:8888/notebooks/REPRODUCIBLE_HAIL_0.2.ipynb
[W 16:13:36.641 NotebookApp] 404 GET /static/components/react/react-dom.production.min.js (127.0.0.1) 3.28ms referer=http://127.0.0.1:8888/notebooks/REPRODUCIBLE_HAIL_0.2.ipynb
[W 16:13:36.921 NotebookApp] 404 GET /nbextensions/widgets/notebook/js/extension.js?v=20190812161311 (127.0.0.1) 6.40ms referer=http://127.0.0.1:8888/notebooks/REPRODUCIBLE_HAIL_0.2.ipynb
[I 16:13:37.158 NotebookApp] Kernel started: 5b3c1cde-2499-4f15-98fb-af5ddf811e35
19/08/12 21:13:40 WARN Utils: Your hostname, TM0493322 resolves to a loopback address: 127.0.1.1; using 10.0.75.1 instead (on interface eth3)
19/08/12 21:13:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
19/08/12 21:13:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[I 16:13:43.809 NotebookApp] Adapting from protocol version 5.1 (kernel 5b3c1cde-2499-4f15-98fb-af5ddf811e35) to 5.3 (client).
^C[I 16:14:36.122 NotebookApp] interrupted
Serving notebooks from local directory: /home/tbresee
1 active kernel
The Jupyter Notebook is running at:
http://localhost:8888/?token=586ee4e03b6b1db9df1f8a7c32507a71952200e88c73d85c
 or http://127.0.0.1:8888/?token=586ee4e03b6b1db9df1f8a7c32507a71952200e88c73d85c
Shutdown this notebook server (y/[n])? y
[C 16:14:38.976 NotebookApp] Shutdown confirmed
[I 16:14:38.981 NotebookApp] Shutting down 1 kernel
[I 16:14:39.889 NotebookApp] Kernel shutdown: 5b3c1cde-2499-4f15-98fb-af5ddf811e35
tbresee@TM0493322:~$ pyspark
1/                                          .local/
.ansible/                                   main.retry
.bash_history                               main.yml
.bash_logout                                new_nokia_new_service.yml
.bashrc                                     play-1.yml
.cache/                                     playbook-ansible-1.yml
.cassandra/                                 playbook-ansible-2.yml
--conf                                      .profile
.config/                                    .python_history
data/                                       .rediscli_history
docker-curriculum/                          REPRODUCIBLE_HAIL_0.2.ipynb
.gradle/                                    REPRODUCIBLE_HAIL.ipynb
hadoop/                                     .sample.sql.swp
hadoop-3.2.0.tar.gz                         .scala_history
hail/                                       scripts/
hail-20190807-1054-0.2.19-c6ec8b76eb26.log  spark-2.4.0-bin-hadoop2.7.tgz
hail-20190807-1257-0.2.19-c6ec8b76eb26.log  spark-2.4.0-bin-hadoop2.7.tgz.1
hail-20190812-1500-0.2.19-c6ec8b76eb26.log  spark-2.4.0-bin-hadoop2.7.tgz.2
hail-20190812-1607-0.2.19-c6ec8b76eb26.log  spark-2.4.3-bin-hadoop2.7.tgz
hail-20190812-1613-0.2.19-c6ec8b76eb26.log  srx/
HAIL 3.0.ipynb                              .ssh/
HAIL 4.0.ipynb                              .sudo_as_admin_successful
HAIL v74.ipynb                              test
HAIL v75.ipynb                              test1.yml
hosts.ini                                   testplaybook1.yml
inventory                                   testplaybook.retry
.ipynb_checkpoints/                         testplaybook.yml
.ipython/                                   Untitled1.ipynb
--jars                                      Untitled.ipynb
.jupyter/                                   .viminfo
tbresee@TM0493322:~$ help(pyspark)
-bash: syntax error near unexpected token `pyspark'
tbresee@TM0493322:~$ pyspark --version
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.4.3
      /_/

Using Scala version 2.11.12, OpenJDK 64-Bit Server VM, 1.8.0_222
Branch
Compiled by user  on 2019-05-01T05:08:38Z
Revision
Url
Type --help for more information.
tbresee@TM0493322:~$ pyspark --help
Usage: ./bin/pyspark [options]

Options:
  --master MASTER_URL         spark://host:port, mesos://host:port, yarn,
                              k8s://https://host:port, or local (Default: local[*]).
  --deploy-mode DEPLOY_MODE   Whether to launch the driver program locally ("client") or
                              on one of the worker machines inside the cluster ("cluster")
                              (Default: client).
  --class CLASS_NAME          Your application's main class (for Java / Scala apps).
  --name NAME                 A name of your application.
  --jars JARS                 Comma-separated list of jars to include on the driver
                              and executor classpaths.
  --packages                  Comma-separated list of maven coordinates of jars to include
                              on the driver and executor classpaths. Will search the local
                              maven repo, then maven central and any additional remote
                              repositories given by --repositories. The format for the
                              coordinates should be groupId:artifactId:version.
  --exclude-packages          Comma-separated list of groupId:artifactId, to exclude while
                              resolving the dependencies provided in --packages to avoid
                              dependency conflicts.
  --repositories              Comma-separated list of additional remote repositories to
                              search for the maven coordinates given with --packages.
  --py-files PY_FILES         Comma-separated list of .zip, .egg, or .py files to place
                              on the PYTHONPATH for Python apps.
  --files FILES               Comma-separated list of files to be placed in the working
                              directory of each executor. File paths of these files
                              in executors can be accessed via SparkFiles.get(fileName).

  --conf PROP=VALUE           Arbitrary Spark configuration property.
  --properties-file FILE      Path to a file from which to load extra properties. If not
                              specified, this will look for conf/spark-defaults.conf.

  --driver-memory MEM         Memory for driver (e.g. 1000M, 2G) (Default: 1024M).
  --driver-java-options       Extra Java options to pass to the driver.
  --driver-library-path       Extra library path entries to pass to the driver.
  --driver-class-path         Extra class path entries to pass to the driver. Note that
                              jars added with --jars are automatically included in the
                              classpath.

  --executor-memory MEM       Memory per executor (e.g. 1000M, 2G) (Default: 1G).

  --proxy-user NAME           User to impersonate when submitting the application.
                              This argument does not work with --principal / --keytab.

  --help, -h                  Show this help message and exit.
  --verbose, -v               Print additional debug output.
  --version,                  Print the version of current Spark.

 Cluster deploy mode only:
  --driver-cores NUM          Number of cores used by the driver, only in cluster mode
                              (Default: 1).

 Spark standalone or Mesos with cluster deploy mode only:
  --supervise                 If given, restarts the driver on failure.
  --kill SUBMISSION_ID        If given, kills the driver specified.
  --status SUBMISSION_ID      If given, requests the status of the driver specified.

 Spark standalone and Mesos only:
  --total-executor-cores NUM  Total cores for all executors.

 Spark standalone and YARN only:
  --executor-cores NUM        Number of cores per executor. (Default: 1 in YARN mode,
                              or all available cores on the worker in standalone mode)

 YARN-only:
  --queue QUEUE_NAME          The YARN queue to submit to (Default: "default").
  --num-executors NUM         Number of executors to launch (Default: 2).
                              If dynamic allocation is enabled, the initial number of
                              executors will be at least NUM.
  --archives ARCHIVES         Comma separated list of archives to be extracted into the
                              working directory of each executor.
  --principal PRINCIPAL       Principal to be used to login to KDC, while running on
                              secure HDFS.
  --keytab KEYTAB             The full path to the file that contains the keytab for the
                              principal specified above. This keytab will be copied to
                              the node running the Application Master via the Secure
                              Distributed Cache, for renewing the login tickets and the
                              delegation tokens periodically.

tbresee@TM0493322:~$
tbresee@TM0493322:~$
tbresee@TM0493322:~$ pyspark \
>    --jars $HAIL_HOME/hail-all-spark.jar \
>    --conf spark.driver.extraClassPath=$HAIL_HOME/hail-all-spark.jar \
>    --conf spark.executor.extraClassPath=$HAIL_HOME/hail-all-spark.jar \
>    --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
>    --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator \
>    --conf spark.driver.allowMultipleContexts=TRUE
[I 16:21:20.526 NotebookApp] Serving notebooks from local directory: /home/tbresee
[I 16:21:20.527 NotebookApp] The Jupyter Notebook is running at:
[I 16:21:20.528 NotebookApp] http://localhost:8888/?token=108229d019433d8b9a83935088af4d30011509b00f65aba6
[I 16:21:20.529 NotebookApp]  or http://127.0.0.1:8888/?token=108229d019433d8b9a83935088af4d30011509b00f65aba6
[I 16:21:20.530 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[W 16:21:20.537 NotebookApp] No web browser found: could not locate runnable browser.
[C 16:21:20.538 NotebookApp]

    To access the notebook, open this file in a browser:
        file:///home/tbresee/.local/share/jupyter/runtime/nbserver-299-open.html
    Or copy and paste one of these URLs:
        http://localhost:8888/?token=108229d019433d8b9a83935088af4d30011509b00f65aba6
     or http://127.0.0.1:8888/?token=108229d019433d8b9a83935088af4d30011509b00f65aba6
[I 16:21:30.712 NotebookApp] 302 GET /?token=108229d019433d8b9a83935088af4d30011509b00f65aba6 (127.0.0.1) 1.68ms
[E 16:21:30.794 NotebookApp] Could not open static file ''
[W 16:21:30.853 NotebookApp] 404 GET /static/components/react/react-dom.production.min.js (127.0.0.1) 20.17ms referer=http://127.0.0.1:8888/tree?token=108229d019433d8b9a83935088af4d30011509b00f65aba6
[W 16:21:30.937 NotebookApp] 404 GET /static/components/react/react-dom.production.min.js (127.0.0.1) 2.46ms referer=http://127.0.0.1:8888/tree?token=108229d019433d8b9a83935088af4d30011509b00f65aba6
[W 16:21:36.374 NotebookApp] 404 GET /static/components/react/react-dom.production.min.js (127.0.0.1) 5.92ms referer=http://127.0.0.1:8888/notebooks/REPRODUCIBLE_HAIL_0.2.ipynb
[W 16:21:36.436 NotebookApp] 404 GET /static/components/react/react-dom.production.min.js (127.0.0.1) 2.74ms referer=http://127.0.0.1:8888/notebooks/REPRODUCIBLE_HAIL_0.2.ipynb
[W 16:21:36.699 NotebookApp] 404 GET /nbextensions/widgets/notebook/js/extension.js?v=20190812162120 (127.0.0.1) 4.28ms referer=http://127.0.0.1:8888/notebooks/REPRODUCIBLE_HAIL_0.2.ipynb
[I 16:21:36.936 NotebookApp] Kernel started: 61119cc7-2264-47d1-861c-ebc4f2b1c349
[I 16:21:37.713 NotebookApp] Saving file at /REPRODUCIBLE_HAIL_0.2.ipynb
19/08/12 21:21:39 WARN Utils: Your hostname, TM0493322 resolves to a loopback address: 127.0.1.1; using 10.0.75.1 instead (on interface eth3)
19/08/12 21:21:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
19/08/12 21:21:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[I 16:21:43.091 NotebookApp] Adapting from protocol version 5.1 (kernel 61119cc7-2264-47d1-861c-ebc4f2b1c349) to 5.3 (client).
[I 16:23:47.116 NotebookApp] Saving file at /REPRODUCIBLE_HAIL_0.2.ipynb
[I 16:26:49.522 NotebookApp] Starting buffering for 61119cc7-2264-47d1-861c-ebc4f2b1c349:8ca0e9f264cc4c93800b5d27639ce8c2
^C[I 16:27:05.234 NotebookApp] interrupted
Serving notebooks from local directory: /home/tbresee
1 active kernel
The Jupyter Notebook is running at:
http://localhost:8888/?token=108229d019433d8b9a83935088af4d30011509b00f65aba6
 or http://127.0.0.1:8888/?token=108229d019433d8b9a83935088af4d30011509b00f65aba6
Shutdown this notebook server (y/[n])? ^C[C 16:27:05.352 NotebookApp] received signal 2, stopping
[I 16:27:05.360 NotebookApp] Shutting down 1 kernel
No answer for 5s: resuming operation...
[I 16:27:10.421 NotebookApp] Kernel shutdown: 61119cc7-2264-47d1-861c-ebc4f2b1c349





