


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    Databricks Quick Commands
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~














~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
--- download/copy/move/use   ---
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


download files:

%sh
wget https://github.com/evodify/genomic-analyses_in_apache-spark/raw/master/vcf_filtering_tutorial/Cbp31_SNPs_test0.01.vcf.gz # download the test VCF file
wget https://github.com/evodify/genomic-analyses_in_apache-spark/raw/master/vcf_filtering_tutorial/Cbp31_annot.csv # download the test annotation file
gunzip Cbp31_SNPs_test0.01.vcf.gz # uncomress the gzip file.



see:
%sh pwd && ls -l
/databricks/driver
total 104208
-rw-r--r-- 1 root root       712 Oct  3 13:29 Cbp31_annot.csv
-rw-r--r-- 1 root root 106677117 Oct  3 13:29 Cbp31_SNPs_test0.01.vcf
drwxr-xr-x 2 root root      4096 Mar 24  2017 conf
-rw-r--r-- 1 root root       718 Oct  3 13:28 derby.log
drwxr-xr-x 3 root root      4096 Oct  3 13:28 eventlogs
-rw-r--r-- 1 root root      6181 Oct  3 13:29 hail.log
drwxr-xr-x 2 root root      4096 Oct  3 13:28 logs


move:

dbutils.fs.mkdirs("dbfs:/FileStore/tables/Cbp") # create a new directory for our data files
Out[5]: True

dbutils.fs.mv("file:/databricks/driver/Cbp31_SNPs_test0.01.vcf", "dbfs:/FileStore/tables/Cbp") # move
dbutils.fs.cp("file:/databricks/driver/Cbp31_annot.csv", "dbfs:/FileStore/tables/Cbp") # copy because we will need non-dbfs file for R later


dbutils.fs.ls("dbfs:/FileStore/tables/Cbp/")
Out[7]: 
[FileInfo(path=u'dbfs:/FileStore/tables/Cbp/Cbp31_SNPs_test0.01.vcf', name=u'Cbp31_SNPs_test0.01.vcf', size=106677117L),
 FileInfo(path=u'dbfs:/FileStore/tables/Cbp/Cbp31_annot.csv', name=u'Cbp31_annot.csv', size=712L)]














~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
--- download a file and save ---
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

import urllib.request
# retrieve 
urllib.request.urlretrieve("http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz", "/tmp/kddcup_data.gz")
# move 
dbutils.fs.mv("file:/tmp/kddcup_data.gz", "dbfs:/kdd/kddcup_data.gz")
# show 
display(dbutils.fs.ls("dbfs:/kdd"))





















~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
--- list out files from dbfs ---
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

%fs
ls  dbfs:/FileStore/jars












~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
---   install library        ---
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

dbutils.library.installPyPI("pypipackage", version="version", repo="repo", extras="extras")
dbutils.library.list()

dbutils.library.installPyPI("azureml-sdk", version="1.0.8", extras="databricks")
dbutils.library.restartPython()  # Removes Python state, but some libraries might not work without calling this function













~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
---      .bz2                ---
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

urllib.urlretrieve("https://s3-us-west-1.amazonaws.com/variant-spark-pub/datasets/hipsterIndex/hipster.vcf.bz2", "/tmp/hipster.vcf.bz2")
urllib.urlretrieve("https://s3-us-west-1.amazonaws.com/variant-spark-pub/datasets/hipsterIndex/hipster_labels.txt", "/tmp/hipster_labels.txt")

dbutils.fs.mv("file:/tmp/hipster.vcf.bz2", "dbfs:/vs-datasets/hipsterIndex/hipster.vcf.bz2")
dbutils.fs.mv("file:/tmp/hipster_labels.txt", "dbfs:/vs-datasets/hipsterIndex/hipster_labels.txt")

display(dbutils.fs.ls("dbfs:/vs-datasets/hipsterIndex"))















~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
---      .vcf                ---
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

get this to work:
	https://docs.databricks.com/_static/notebooks/genomics/vcf2delta.html


https://docs.azuredatabricks.net/_static/notebooks/genomics/vcf2delta.html



Predicting Geographic Population using Genome Variants and K-Means
	https://databricks.com/blog/2016/05/24/predicting-geographic-population-using-genome-variants-and-k-means.html


hail_all_spark.jar
×
Status
Installed
Source
dbfs:/FileStore/jars/5698d3bc_4eae_4017_adc7_a960409bcd16-hail_all_spark-71440.jar
Messages
No Messages




== Parsed Logical Plan ==
AddJarCommand /local_disk0/tmp/addedFile52657766961105597795698d3bc_4eae_4017_adc7_a960409bcd16_hail_all_spark_71440-df353.jar

== Analyzed Logical Plan ==
result: int
AddJarCommand /local_disk0/tmp/addedFile52657766961105597795698d3bc_4eae_4017_adc7_a960409bcd16_hail_all_spark_71440-df353.jar

== Optimized Logical Plan ==
AddJarCommand /local_disk0/tmp/addedFile52657766961105597795698d3bc_4eae_4017_adc7_a960409bcd16_hail_all_spark_71440-df353.jar

== Physical Plan ==
Execute AddJarCommand
   +- AddJarCommand /local_disk0/tmp/addedFile52657766961105597795698d3bc_4eae_4017_adc7_a960409bcd16_hail_all_spark_71440-df353.jar
















~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
---   setting up genomics    ---
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


env 
	SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar

https://docs.databricks.com/_static/notebooks/genomics/vcf2delta.html

env
	SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar


SPARK_CLASSPATH=dbfs:/FileStore/jars/5698d3bc_4eae_4017_adc7_a960409bcd16-hail_all_spark-71440.jar

PYSPARK_PYTHON=python3
SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar
println(sys.env.get("SPARK_HOME"))
import os
print(os.environ['SPARK_HOME'])


https://www.blue-granite.com/blog/scaling-your-genomics-pipeline-to-the-cloud-with-azure-databricks



hail_all_spark.jar
×
Status
Installed
Source
dbfs:/FileStore/jars/5698d3bc_4eae_4017_adc7_a960409bcd16-hail_all_spark-71440.jar
Messages
No Messages











~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
--- things ive tried to get it to work  ---
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


SPARK_CLASSPATH=dbfs:/FileStore/jars/5698d3bc_4eae_4017_adc7_a960409bcd16-hail_all_spark-71440.jar
PYSPARK_PYTHON=/databricks/python3/bin/python3
ENABLE_HAIL=true
HAIL_HOME=dbfs:/FileStore/jars/5698d3bc_4eae_4017_adc7_a960409bcd16-hail_all_spark-71440.jar




https://forums.databricks.com/questions/11391/can-i-run-hail-on-databricks.html  ? worth trying again ? 



SPARK_CLASSPATH=dbfs:/FileStore/jars/5698d3bc_4eae_4017_adc7_a960409bcd16-hail_all_spark-71440.jar
                


export HAIL_HOME=/path/to/hail/


HAIL_HOME=dbfs:/FileStore/jars/5698d3bc_4eae_4017_adc7_a960409bcd16-hail_all_spark-71440.jar



  --jars $HAIL_HOME/build/libs/hail-all-spark.jar \
  --conf spark.driver.extraClassPath=\"$HAIL_HOME/build/libs/hail-all-spark.jar\" \
  --conf spark.executor.extraClassPath=./hail-all-spark.jar \
  --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
  --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator
  pyspark-shell"


























~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
--- compression type error when loading .bgz ---
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~




	at org.apache.spark.io.LZ4CompressionCodec.compressedInputStream(CompressionCodec.scala:122)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$7.apply(TorrentBroadcast.scala:324)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$7.apply(TorrentBroadcast.scala:324)
	at scala.Option.map(Option.scala:146)
	at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:324)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1$$anonfun$apply$2.apply(TorrentBroadcast.scala:255)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:231)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1461)
	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:227)
	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:74)
	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:74)
	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:108)
	at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:78)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:84)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:139)
	at org.apache.spark.scheduler.Task.run(Task.scala:112)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1495)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:503)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2355)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2343)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2342)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2342)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1096)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1096)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1096)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2574)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2510)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:893)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2243)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2341)
	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1114)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:379)
	at org.apache.spark.rdd.RDD.fold(RDD.scala:1108)
	at is.hail.utils.richUtils.RichRDD$.exists$extension(RichRDD.scala:29)
	at is.hail.utils.richUtils.RichRDD$.forall$extension(RichRDD.scala:25)
	at is.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:286)
	at is.hail.HailContext.importVCFs(HailContext.scala:556)
	at is.hail.HailContext.importVCF(HailContext.scala:531)
	at linef382fd524229485aad6cdfe0d266921d35.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(command-1410785419595570:1)
	at linef382fd524229485aad6cdfe0d266921d35.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(command-1410785419595570:49)
	at linef382fd524229485aad6cdfe0d266921d35.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(command-1410785419595570:51)
	at linef382fd524229485aad6cdfe0d266921d35.$read$$iw$$iw$$iw$$iw$$iw.<init>(command-1410785419595570:53)
	at linef382fd524229485aad6cdfe0d266921d35.$read$$iw$$iw$$iw$$iw.<init>(command-1410785419595570:55)
	at linef382fd524229485aad6cdfe0d266921d35.$read$$iw$$iw$$iw.<init>(command-1410785419595570:57)
	at linef382fd524229485aad6cdfe0d266921d35.$read$$iw$$iw.<init>(command-1410785419595570:59)
	at linef382fd524229485aad6cdfe0d266921d35.$read$$iw.<init>(command-1410785419595570:61)
	at linef382fd524229485aad6cdfe0d266921d35.$read.<init>(command-1410785419595570:63)
	at linef382fd524229485aad6cdfe0d266921d35.$read$.<init>(command-1410785419595570:67)
	at linef382fd524229485aad6cdfe0d266921d35.$read$.<clinit>(command-1410785419595570)
	at linef382fd524229485aad6cdfe0d266921d35.$eval$.$print$lzycompute(<notebook>:7)
	at linef382fd524229485aad6cdfe0d266921d35.$eval$.$print(<notebook>:6)
	at linef382fd524229485aad6cdfe0d266921d35.$eval.$print(<notebook>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:793)
	at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1054)
	at scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:645)
	at scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:644)
	at scala.reflect.internal.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:31)
	at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:19)
	at scala.tools.nsc.interpreter.IMain$WrappedRequest.loadAndRunReq(IMain.scala:644)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:576)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:572)
	at com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:215)
	at com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply$mcV$sp(ScalaDriverLocal.scala:197)
	at com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply(ScalaDriverLocal.scala:197)
	at com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply(ScalaDriverLocal.scala:197)
	at com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:653)
	at com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:606)
	at com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:197)
	at com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$8.apply(DriverLocal.scala:342)
	at com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$8.apply(DriverLocal.scala:319)
	at com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:238)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:233)
	at com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:47)
	at com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:271)
	at com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:47)
	at com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:319)
	at com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:644)
	at com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:644)
	at scala.util.Try$.apply(Try.scala:192)
	at com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:639)
	at com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:485)
	at com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:597)
	at com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:390)
	at com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:337)
	at com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:219)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.NoSuchMethodError: net.jpountz.lz4.LZ4BlockInputStream.<init>(Ljava/io/InputStream;Z)V
	at org.apache.spark.io.LZ4CompressionCodec.compressedInputStream(CompressionCodec.scala:122)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$7.apply(TorrentBroadcast.scala:324)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$7.apply(TorrentBroadcast.scala:324)
	at scala.Option.map(Option.scala:146)
	at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:324)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1$$anonfun$apply$2.apply(TorrentBroadcast.scala:255)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:231)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1461)
	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:227)
	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:74)
	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:74)
	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:108)
	at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:78)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:84)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:139)
	at org.apache.spark.scheduler.Task.run(Task.scala:112)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1495)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:503)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)


