

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
   windows ubuntu location
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


location

	tbresee@TM0493322:~/.local/lib/python3.6/site-packages/hail$










spark.driver.extraClassPath or it's alias --driver-class-path to set extra classpaths on the node running the driver.


https://community.cloud.databricks.com/files/beer_data.csv?o=7001951515152566



my-file.txt



?o=7001951515152566#notebook/2385188326668946/command/2385188326668969



https://tance>/files/my-stuff/my-file.txt?o=7001951515152566



beer_data.csv




https://community.cloud.databricks.com/?o=7001951515152566#notebook/2385188326668946/command/2385188326668969

However, if there is ?o= in the deployment URL, for example, https://<databricks-instance>/?o=6280049833385130, replace https://<databricks-instance>/files/my-stuff/my-file.txt with 


https://<databricks-instance>/files/my-stuff/my-file.txt?o=###### where the number after o= is the same as in your URL.



https://community.cloud.databricks.com/?o=7001951515152566#notebook/2385188326668946/command/2385188326668969



Databricks Workspace id


A Databricks Workspace is where the Databricks platform runs and where you can create Spark clusters and schedule workloads.

Some types of workspaces have a unique Workspace ID. If there is o= in the deployment URL, for example, https://<databricks-instance>/?o=6280049833385130, the random number after o= is the Databricks Workspace ID. Here the Workspace ID is 6280049833385130. If there is no o= in the deployment URL, the Workspace ID is 0.




ClassPath:
ClassPath is affected depending on what you provide. There are a couple of ways to set something on the classpath:

spark.driver.extraClassPath or it's alias --driver-class-path to set extra classpaths on the node running the driver.
spark.executor.extraClassPath to set extra class path on the Worker nodes.










HAIL HAS TO HAVE THE CORE HAIL JAR FILE WITHIN DATABRICKS OR NOTHING WORKS AT ALL

I need to see my core .jar file for Hail within the Databricks /jars folder here:

%fs ls dbfs:/FileStore/jars

If you dont, nothing at all will work properly, in any form.  It is your starting point ! 












COMMUNITY EDITION DATABRICKS SEEMS TO LOAD HAIL INCORRECTLY

when you enter this command:  ENABLE_HAIL=true, within your environment variables, like this website says to do:
https://docs.databricks.com/applications/genomics/hail.html, and then you look for your jar fails, you DON't see
jar files that are right, you see like these tutorials:

%fs ls dbfs:/FileStore/jars command outputs:
dbfs:/FileStore/jars/35bb0c74_d723_4381_bc5c_5bb9d4ea6326-hail_tutorial_databricks-e84f3.jar	35bb0c74_d723_4381_bc5c_5bb9d4ea6326-hail_tutorial_databricks-e84f3.jar	27876587

dbfs:/FileStore/jars/a3245b97_8a2b_4bce_902a_cf2cec710769-hail_tutorial_databricks-e84f3.jar	a3245b97_8a2b_4bce_902a_cf2cec710769-hail_tutorial_databricks-e84f3.jar	27876587

dbfs:/FileStore/jars/b9008399_4234_4453_837b_0c918d34f13a-hail_tutorial_databricks-e84f3.jar	b9008399_4234_4453_837b_0c918d34f13a-hail_tutorial_databricks-e84f3.jar	27876587

dbfs:/FileStore/jars/c7eaedcd_8828_4b98_8cc6_90caba462aa4-hail_devel_py2_7_databricks-20ed0.egg	c7eaedcd_8828_4b98_8cc6_90caba462aa4-hail_devel_py2_7_databricks-20ed0.egg	153491

dbfs:/FileStore/jars/ce0ce418_f2d9_4cc8_89ee_206a3245fff5-hail_tutorial_databricks-e84f3.jar
ce0ce418_f2d9_4cc8_89ee_206a3245fff5-hail_tutorial_databricks-e84f3.jar	27876587

this just doesn't seem right at all...



I BELIEVE YOU COULD POINT DATABRICKS TO ONE OF THOSE TUTORIAL JAR FILES




I BELIEVE HAIL IS NOT STARTING CORRECTLY IN DATABRICKS

when i go to the cluster after it has started, and i look for the 'Libraries', i see:

hail	PyPi	Status: Installed	- (under source)

	i dont think this is right, there is no core hail .jar file !  






LETS TRY INSTALLING IT WITH PYPI:



























.goldenboy

ENABLE_HAIL=true


spark.hadoop.io.compression.codecs org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,org.apache.hadoop.io.compress.GzipCodec
spark.sql.files.openCostInBytes 1099511627776
spark.sql.files.maxPartitionBytes 1099511627776
spark.hadoop.mapreduce.input.fileinputformat.split.minsize 1099511627776
spark.hadoop.parquet.block.size 1099511627776











--- FINAL CONFIGURATION ADVICE THAT SHOULD WORK ---

ENABLE_HAIL=true



https://discuss.hail.is/t/classnotfoundexception-is-hail-asm4s-asmfunction2/231/17

critical information:

spark.executor.extraClassPath - /databricks/jars/<YOUR_JAR_NAME>-hail_all_spark2_0_2-f864f.jar

When a libraray is attached to a cluster executor.extraClassPath is set but there is no way in Databricks to verify if the jar actually exists in the path, so create an init script to manually copy the jars to all executors.

dbutils.fs.put("/databricks/init/install_hail.sh",""" #!/bin/bash cp dbfs:/FileStore/jars/<YOUR_JAR_NAME>-hail_all_spark2_0_2-f864f.jar /mnt/driver-daemon/jars/ cp dbfs:/FileStore/jars/<YOUR_JAR_NAME>-hail_all_spark2_0_2-f864f.jar /mnt/jars/driver-daemon""", True)



This error means you probably need something like the following:

spark.jars=HAIL_JAR_PATH
spark.driver.extraClassPath=HAIL_JAR_PATH
spark.executor.extraClassPath=./hail-all-spark.jar












spark config in cluster setup:
	spark.driver.extraClassPath ./hail-<your-version>.jar
	spark.executor.extraClassPath ./hail-<your-version>.jar

	or like:
	spark.driver.extraClassPath = ./hail.jar
	spark.executor.extraClassPath = ./hail.jar


	
spark.hadoop.io.compression.codecs org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,org.apache.hadoop.io.compress.GzipCodec

spark.sql.files.openCostInBytes 1099511627776

spark.sql.files.maxPartitionBytes 1099511627776

spark.hadoop.mapreduce.input.fileinputformat.split.minsize 1099511627776

spark.hadoop.parquet.block.size 1099511627776

?
spark.sql.files.maxPartitionBytes=100000000000
spark.sql.files.openCostInBytes=100000000000


spark.sql.files.maxPartitionBytes=100000000000
spark.sql.files.openCostInBytes=100000000000















%fs ls /dbfs/FileStore/jars

Next, create an init script to copy that jar into the classpath of the Spark cluster before you start up Spark. NOTE - if you add yet newer versions of hail, you'll need to update this script:


dbutils.fs.put("/databricks/init/install_hail.sh",""" #!/bin/bash
cp /dbfs/FileStore/jars/[YOUR_JAR_NAME--hail_all_spark-71440.jar] /mnt/driver-daemon/jars/

cp /dbfs/FileStore/jars/[YOUR_JAR_NAME-hail_all_spark-71440.jar] /mnt/jars/driver-daemon""", True)















THIS WORKED FOR EVO

spark.hadoop.io.compression.codecs org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,org.apache.hadoop.io.compress.GzipCodec

spark.sql.files.openCostInBytes 1099511627776

spark.sql.files.maxPartitionBytes 1099511627776

spark.hadoop.mapreduce.input.fileinputformat.split.minsize 1099511627776

spark.hadoop.parquet.block.size 1099511627776




if you wanted to set to 50G: 
spark.sql.files.openCostInBytes=53687091200
spark.sql.files.maxPartitionBytes=53687091200'



--conf='spark.driver.extraClassPath=./hail.jar' 
--conf='spark.executor.extraClassPath=./hail.jar'
NB: It is important to note that the JARs are copied to the working directory of the executors, but are not copied to the working directory of the driver. Usually, the spark.driver.extraClassPath will be the same path you passed to --jars whereas spark.executor.extraClassPath must be a relative path.




val blockSize = 1024 * 1024 * 16      // 16MB
sc.hadoopConfiguration.setInt( "dfs.blocksize", blockSize )
sc.hadoopConfiguration.setInt( "parquet.block.size", blockSize )













GOLDEN ADVICE


 -  correct format

	In [3]: a="SEQ187500194.vcf.gz"                                                                                                                                                          
	In [4]: l.import_vcf(a) 

	or hl.import_vcf('SEQ187500194.vcf.gz')


 -  gzip
	
Please see the docs for import_vcf regarding gzipping, though - you should either rename this file .bgz if it is truly block-gzipped, or bgzip it manually before importing to Hail:

https://hail.is/docs/0.2/methods/impex.html#hail.methods.import_vcf


ds = hl.import_vcf('data/example2.vcf.bgz', reference_genome='GRCh37')

https://hail.is/docs/0.2/methods/impex.html#hail.methods.import_vcf


Ensure that the VCF file is correctly prepared for import: VCFs should either be uncompressed (.vcf) or block compressed (.vcf.bgz). If you have a large compressed VCF that ends in .vcf.gz, it is likely that the file is actually block-compressed, and you should rename the file to .vcf.bgz accordingly. If you actually have a standard gzipped file, it is possible to import it to Hail using the force parameter. However, this is not recommended – all parsing will have to take place on one node because gzip decompression is not parallelizable. In this case, import will take significantly longer.





















 -  you will have to put the .jar and .zip on the slave nodes for this all to work 

 -  local jars on the driver and executor class paths
    add explicitly



But, apparently, this does not include the local jars on the driver and executor class paths. You have to explicitly add the JARs to the class paths using these properties:

spark-shell --jars './hail.jar' \
  --conf='spark.sql.files.openCostInBytes=53687091200' \
  --conf='spark.sql.files.maxPartitionBytes=53687091200' \
  --conf='spark.driver.extraClassPath=./hail.jar' \
  --conf='spark.executor.extraClassPath=./hail.jar'


NB: It is important to note that the JARs are copied to the working directory of the executors, but are not copied to the working directory of the driver. Usually, the spark.driver.extraClassPath will be the same path you passed to --jars whereas spark.executor.extraClassPath must be a relative path.







-------------------------------------------------------------------------------------------------------------------------------
RUNNING SPARK ON AWS EMR:

pyspark --jars hail-all-spark.jar \
--py-files hail-python.zip \
--conf spark.driver.extraClassPath=./hail-all-spark.jar \
--conf spark.executor.extraClassPath=./hail-all-spark.jar \
--conf spark.sql.files.openCostInBytes=1099511627776 \
--conf spark.sql.files.maxPartitionBytes=1099511627776 \
--conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator

...
>>> hl.init()




Try hl.init(sc)



hl.init(sc)



[root@ip-172-31-91-41 hail-elasticsearch-pipelines]# ipython
Python 3.6.2 (default, Feb 19 2018, 21:55:54) 
Type 'copyright', 'credits' or 'license' for more information
IPython 7.2.0 -- An enhanced Interactive Python. Type '?' for help.

In [1]: import hail as l                                                                                                                                                                 

In [2]: l.init()          



This repo will save you time as it sets up the EMR cluster, with the latest Hail version and all the proper configurations, and it spot instances (cheaper cluters): https://github.com/hms-dbmi/hail-on-AWS-spot-instances 4. I don’t know how familiar are you with AWS as you have to add some security feats but if you have admin permits in your account you should not have issues.

Due to the file system management in your EMR, it is more convenient to read your files from s3:

s3_path='s3://your-path-here/'
mt=hl.import_vcf(s3_path+'SEQ187500194.vcf.gz')




export SPARK_HOME=/usr/lib/spark

export HAIL_HOME=/opt/hail/hail

export SPARK_CLASSPATH=/opt/hail/hail/build/libs/hail-all-spark.jar

export PYTHONPATH=/opt/hail/hail/build/distributions/hail-python.zip:/usr/lib/spark/python:/usr/lib/spark/python/lib/py4j-0.10.4-src.zip:/opt/hail-elasticsearch-pipelines/hail_scripts.zip

export PYSPARK_SUBMIT_ARGS="–conf spark.driver.extraClassPath=/home/hadoop/hail-all-spark.jar:/usr/share/aws/emr/emrfs/lib/emrfs-hadoop-assembly-2.20.0.jar --conf spark.executor.extraClassPath=/home/hadoop/hail-all-spark.jar:/usr/share/aws/emr/emrfs/lib/emrfs-hadoop-assembly-2.20.0.jar pyspark-shell"

export PYSPARK_PYTHON=python3







-------------------------------------------------------------------------------------------------------------------------------



































 -  they seem to want you to do this:


--conf "spark.driver.extraClassPath=build/libs/hail-all-spark.jar"
--conf "spark.executor.extraClassPath=./hail-all-spark.jar"



spark-shell --jars './hail.jar' --conf='spark.sql.files.openCostInBytes=53687091200' --conf='spark.sql.files.maxPartitionBytes=53687091200'

















AWS EMR
AWS EMR reuires that you distribute the .jar and .zip files for hail on the other slaves. Might be obvious to everybody, but I am different.

Here is the script to test the setup (based on the hail.is v0.2 tutorial)

import hail as hl
import hail.expr.aggregators as agg
hl.init()
hl.utils.get_1kg('data/')
ds = hl.read_matrix_table('data/1kg.mt')
ds.rows().select().show(5)
ds.s.show(5)
ds.entry.take(5)
table = (hl.import_table('data/1kg_annotations.txt', impute=True)
         .key_by('Sample'))
table.describe()
table.show(width=100)
print(ds.col.dtype)
hl.stop()


export SPARK_HOME=/usr/lib/spark
# Yes, I did build it under the $HOME for hadoop, so what?
export HAIL_HOME=/home/hadoop/hail
export PYTHONPATH="$PYTHONPATH:$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.4-src.zip"
# Do not forget to switch to Python v3.6
export PYSPARK_PYTHON=python3

# Note: Had to drop the .jar and .zip on /home/hadoop on the slave nodes!!!
spark-submit \
--jars /home/hadoop/hail-all-spark.jar \
--conf='spark.driver.extraClassPath=./hail-all-spark.jar' \
--conf='spark.executor.extraClassPath=./hail-all-spark.jar' \
--files /home/hadoop/hail-python.zip \
test.py
















STARTING:
	$SPARK_HOME/bin/spark-shell --version







-------------------------------------------------------------------------------------------------------------------------------
GOLDEN:

pyspark --jars hail-all-spark.jar \
  --py-files hail-python.zip \
  --conf spark.driver.extraClassPath=./hail-all-spark.jar \
  --conf spark.executor.extraClassPath=./hail-all-spark.jar \
  --conf spark.sql.files.openCostInBytes=1099511627776 \
  --conf spark.sql.files.maxPartitionBytes=1099511627776 \
  --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator
-------------------------------------------------------------------------------------------------------------------------------




-------------------------------------------------------------------------------------------------------------------------------
advice
		
	go to discuss.hail.is and search on 'your item'

Are you using an 0.1 jar/python zip? init was added in 0.2.


Hail doesn’t do anything special with the SparkContext that is passed into it.
-------------------------------------------------------------------------------------------------------------------------------














-------------------------------------------------------------------------------------------------------------------------------
These are Spark config options that Hail requires – they’re not really documented, which is a problem. When Hail creates a SparkContext, it configures it properly, but when one is passed in (hc = HailContext(sc)) we have to check.

How is your spark context created? Is it created from a pyspark submit command? 
You can configure most Spark startup actions with a --conf option, e.g. --conf spark.sql.files.maxPartitionBytes=1000000000



conf = SparkConf()
conf.set('spark.sql.files.maxPartitionBytes
','60000000000')
conf.set('spark.sql.files.openCostInBytes','60000000000')
conf.set('master','local[*]')
conf.set('spark.submit.deployMode', u'client')
conf.set('spark.app.name', u'PyTest')
sc = SparkContext(conf=conf)



or



In [1]: from pyspark import *

In [2]: from hail import *

In [3]: conf = SparkConf()
   ...: conf.set('spark.sql.files.maxPartitionBytes','60000000000')
   ...: conf.set('spark.sql.files.openCostInBytes','60000000000')
   ...: conf.set('master','local[*]')
   ...: conf.set('spark.submit.deployMode', u'client')
   ...: conf.set('spark.app.name', u'PyTest')
   ...: sc = SparkContext(conf=conf)

In [4]: hc = HailContext(sc)
hail: info: SparkUI: http://192.168.1.2:4040
-------------------------------------------------------------------------------------------------------------------------------













-------------------------------------------------------------------------------------------------------------------------------
from hail import *
hc = HailContext()
-------------------------------------------------------------------------------------------------------------------------------

















-------------------------------------------------------------------------------------------------------------------------------
from pyspark import *
from hail import *
conf = SparkConf()
conf.set('spark.sql.files.maxPartitionBytes','60000000000') 
conf.set('spark.sql.files.openCostInBytes','60000000000') 
conf.set('spark.driver.cores','1') #test with 1 core
sc = SparkContext(conf=conf)
hc = HailContext(sc)
-------------------------------------------------------------------------------------------------------------------------------








-------------------------------------------------------------------------------------------------------------------------------
pyspark2 --jars $HAIL_HOME/build/libs/hail-all-spark.jar 
–py-files $HAIL_HOME/python/hail-python.zip 
–conf spark.executorEnv.JAVA_HOME=/opt/jdk1.8.0.131 
–conf spark.executor.extraClassPath=/log/cloudera/parcels/SPARK2-2.1.0.cloudera1-1.cdh5.7.0.p0.120904/lib/spark2 
–conf spark.driver.extraClassPath=/log/cloudera/parcels/SPARK2-2.1.0.cloudera1-1.cdh5.7.0.p0.120904/lib/spark2 
–conf spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,org.apache.hadoop.io.compress.GzipCodec 
–conf spark.sql.files.openCostInBytes=1099511627776 
–conf spark.sql.files.maxPartitionBytes=1099511627776 
–conf spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1099511627776 
–conf spark.hadoop.parquet.block.size=109951162777

from hail import *
hc = HailContext(sc)
hc.import_vcf(’/user/kmlong/vcf/ALL.chrMT.phase3_callmom-v0_4.20130502.genotypes.vcf’).write(’/user/kmlong/vds/ALL.chrMT.phase3_callmom-v0_4.20130502.genotypes.vds’)
-------------------------------------------------------------------------------------------------------------------------------





-------------------------------------------------------------------------------------------------------------------------------
pyspark2 --jars build/libs/hail-all-spark.jar 
–py-files build/distributions/hail-python.zip 
–conf spark.sql.files.openCostInBytes=1099511627776 
–conf spark.sql.files.maxPartitionBytes=1099511627776 
–conf spark.hadoop.parquet.block.size=1099511627776
-------------------------------------------------------------------------------------------------------------------------------





-------------------------------------------------------------------------------------------------------------------------------
Yeah having a problem as well. Based on @tpoterba previous suggestion I installed Anoconda with Python 3 on all of the nodes and then build a cluster using EMR 5.10.0 (Spark 2.2.0). When I try to use the pyspark command line I cannot even get the hl.init(sc) command to work.


 
[hadoop@ip-172-30-1-4 ~]$ pyspark --jars hail-all-spark.jar  --py-files hail-python.zip
Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 18:21:58)
[GCC 7.2.0] on linux
Type “help”, “copyright”, “credits” or “license” for more information.
Setting default log level to “WARN”.
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
18/05/03 23:18:29 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
18/05/03 23:18:40 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
Welcome to
-------------------------------------------------------------------------------------------------------------------------------





-------------------------------------------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------------------------------------------





-------------------------------------------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------------------------------------------





-------------------------------------------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------------------------------------------





-------------------------------------------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------------------------------------------




