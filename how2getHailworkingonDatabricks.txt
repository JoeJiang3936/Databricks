











 -  Tom, when you are done put the dashes around it to keep it clean 








.goldenboy

ENABLE_HAIL=true


spark.hadoop.io.compression.codecs org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,org.apache.hadoop.io.compress.GzipCodec
spark.sql.files.openCostInBytes 1099511627776
spark.sql.files.maxPartitionBytes 1099511627776
spark.hadoop.mapreduce.input.fileinputformat.split.minsize 1099511627776
spark.hadoop.parquet.block.size 1099511627776











--- FINAL CONFIGURATION ADVICE THAT SHOULD WORK ---

ENABLE_HAIL=true



https://discuss.hail.is/t/classnotfoundexception-is-hail-asm4s-asmfunction2/231/17

critical information:

spark.executor.extraClassPath - /databricks/jars/<YOUR_JAR_NAME>-hail_all_spark2_0_2-f864f.jar

When a libraray is attached to a cluster executor.extraClassPath is set but there is no way in Databricks to verify if the jar actually exists in the path, so create an init script to manually copy the jars to all executors.

dbutils.fs.put("/databricks/init/install_hail.sh",""" #!/bin/bash cp dbfs:/FileStore/jars/<YOUR_JAR_NAME>-hail_all_spark2_0_2-f864f.jar /mnt/driver-daemon/jars/ cp dbfs:/FileStore/jars/<YOUR_JAR_NAME>-hail_all_spark2_0_2-f864f.jar /mnt/jars/driver-daemon""", True)



This error means you probably need something like the following:

spark.jars=HAIL_JAR_PATH
spark.driver.extraClassPath=HAIL_JAR_PATH
spark.executor.extraClassPath=./hail-all-spark.jar












spark config in cluster setup:
	spark.driver.extraClassPath ./hail-<your-version>.jar
	spark.executor.extraClassPath ./hail-<your-version>.jar

	or like:
	spark.driver.extraClassPath = ./hail.jar
	spark.executor.extraClassPath = ./hail.jar


	
spark.hadoop.io.compression.codecs org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,org.apache.hadoop.io.compress.GzipCodec

spark.sql.files.openCostInBytes 1099511627776

spark.sql.files.maxPartitionBytes 1099511627776

spark.hadoop.mapreduce.input.fileinputformat.split.minsize 1099511627776

spark.hadoop.parquet.block.size 1099511627776

?
spark.sql.files.maxPartitionBytes=100000000000
spark.sql.files.openCostInBytes=100000000000


spark.sql.files.maxPartitionBytes=100000000000
spark.sql.files.openCostInBytes=100000000000















%fs ls /dbfs/FileStore/jars

Next, create an init script to copy that jar into the classpath of the Spark cluster before you start up Spark. NOTE - if you add yet newer versions of hail, you'll need to update this script:


dbutils.fs.put("/databricks/init/install_hail.sh",""" #!/bin/bash
cp /dbfs/FileStore/jars/[YOUR_JAR_NAME--hail_all_spark-71440.jar] /mnt/driver-daemon/jars/

cp /dbfs/FileStore/jars/[YOUR_JAR_NAME-hail_all_spark-71440.jar] /mnt/jars/driver-daemon""", True)















THIS WORKED FOR EVO

spark.hadoop.io.compression.codecs org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,org.apache.hadoop.io.compress.GzipCodec

spark.sql.files.openCostInBytes 1099511627776

spark.sql.files.maxPartitionBytes 1099511627776

spark.hadoop.mapreduce.input.fileinputformat.split.minsize 1099511627776

spark.hadoop.parquet.block.size 1099511627776




if you wanted to set to 50G: 
spark.sql.files.openCostInBytes=53687091200
spark.sql.files.maxPartitionBytes=53687091200'



--conf='spark.driver.extraClassPath=./hail.jar' 
--conf='spark.executor.extraClassPath=./hail.jar'
NB: It is important to note that the JARs are copied to the working directory of the executors, but are not copied to the working directory of the driver. Usually, the spark.driver.extraClassPath will be the same path you passed to --jars whereas spark.executor.extraClassPath must be a relative path.




val blockSize = 1024 * 1024 * 16      // 16MB
sc.hadoopConfiguration.setInt( "dfs.blocksize", blockSize )
sc.hadoopConfiguration.setInt( "parquet.block.size", blockSize )













GOLDEN ADVICE


 -  correct format

	In [3]: a="SEQ187500194.vcf.gz"                                                                                                                                                          
	In [4]: l.import_vcf(a) 

	or hl.import_vcf('SEQ187500194.vcf.gz')


 -  gzip
	
Please see the docs for import_vcf regarding gzipping, though - you should either rename this file .bgz if it is truly block-gzipped, or bgzip it manually before importing to Hail:

https://hail.is/docs/0.2/methods/impex.html#hail.methods.import_vcf


ds = hl.import_vcf('data/example2.vcf.bgz', reference_genome='GRCh37')

https://hail.is/docs/0.2/methods/impex.html#hail.methods.import_vcf


Ensure that the VCF file is correctly prepared for import: VCFs should either be uncompressed (.vcf) or block compressed (.vcf.bgz). If you have a large compressed VCF that ends in .vcf.gz, it is likely that the file is actually block-compressed, and you should rename the file to .vcf.bgz accordingly. If you actually have a standard gzipped file, it is possible to import it to Hail using the force parameter. However, this is not recommended – all parsing will have to take place on one node because gzip decompression is not parallelizable. In this case, import will take significantly longer.





















 -  you will have to put the .jar and .zip on the slave nodes for this all to work 

 -  local jars on the driver and executor class paths
    add explicitly



But, apparently, this does not include the local jars on the driver and executor class paths. You have to explicitly add the JARs to the class paths using these properties:

spark-shell --jars './hail.jar' \
  --conf='spark.sql.files.openCostInBytes=53687091200' \
  --conf='spark.sql.files.maxPartitionBytes=53687091200' \
  --conf='spark.driver.extraClassPath=./hail.jar' \
  --conf='spark.executor.extraClassPath=./hail.jar'


NB: It is important to note that the JARs are copied to the working directory of the executors, but are not copied to the working directory of the driver. Usually, the spark.driver.extraClassPath will be the same path you passed to --jars whereas spark.executor.extraClassPath must be a relative path.







-------------------------------------------------------------------------------------------------------------------------------
RUNNING SPARK ON AWS EMR:

pyspark --jars hail-all-spark.jar \
--py-files hail-python.zip \
--conf spark.driver.extraClassPath=./hail-all-spark.jar \
--conf spark.executor.extraClassPath=./hail-all-spark.jar \
--conf spark.sql.files.openCostInBytes=1099511627776 \
--conf spark.sql.files.maxPartitionBytes=1099511627776 \
--conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator

...
>>> hl.init()




Try hl.init(sc)



hl.init(sc)



[root@ip-172-31-91-41 hail-elasticsearch-pipelines]# ipython
Python 3.6.2 (default, Feb 19 2018, 21:55:54) 
Type 'copyright', 'credits' or 'license' for more information
IPython 7.2.0 -- An enhanced Interactive Python. Type '?' for help.

In [1]: import hail as l                                                                                                                                                                 

In [2]: l.init()          



This repo will save you time as it sets up the EMR cluster, with the latest Hail version and all the proper configurations, and it spot instances (cheaper cluters): https://github.com/hms-dbmi/hail-on-AWS-spot-instances 4. I don’t know how familiar are you with AWS as you have to add some security feats but if you have admin permits in your account you should not have issues.

Due to the file system management in your EMR, it is more convenient to read your files from s3:

s3_path='s3://your-path-here/'
mt=hl.import_vcf(s3_path+'SEQ187500194.vcf.gz')




export SPARK_HOME=/usr/lib/spark

export HAIL_HOME=/opt/hail/hail

export SPARK_CLASSPATH=/opt/hail/hail/build/libs/hail-all-spark.jar

export PYTHONPATH=/opt/hail/hail/build/distributions/hail-python.zip:/usr/lib/spark/python:/usr/lib/spark/python/lib/py4j-0.10.4-src.zip:/opt/hail-elasticsearch-pipelines/hail_scripts.zip

export PYSPARK_SUBMIT_ARGS="–conf spark.driver.extraClassPath=/home/hadoop/hail-all-spark.jar:/usr/share/aws/emr/emrfs/lib/emrfs-hadoop-assembly-2.20.0.jar --conf spark.executor.extraClassPath=/home/hadoop/hail-all-spark.jar:/usr/share/aws/emr/emrfs/lib/emrfs-hadoop-assembly-2.20.0.jar pyspark-shell"

export PYSPARK_PYTHON=python3







-------------------------------------------------------------------------------------------------------------------------------



































 -  they seem to want you to do this:


--conf "spark.driver.extraClassPath=build/libs/hail-all-spark.jar"
--conf "spark.executor.extraClassPath=./hail-all-spark.jar"



spark-shell --jars './hail.jar' --conf='spark.sql.files.openCostInBytes=53687091200' --conf='spark.sql.files.maxPartitionBytes=53687091200'

















AWS EMR
AWS EMR reuires that you distribute the .jar and .zip files for hail on the other slaves. Might be obvious to everybody, but I am different.

Here is the script to test the setup (based on the hail.is v0.2 tutorial)

import hail as hl
import hail.expr.aggregators as agg
hl.init()
hl.utils.get_1kg('data/')
ds = hl.read_matrix_table('data/1kg.mt')
ds.rows().select().show(5)
ds.s.show(5)
ds.entry.take(5)
table = (hl.import_table('data/1kg_annotations.txt', impute=True)
         .key_by('Sample'))
table.describe()
table.show(width=100)
print(ds.col.dtype)
hl.stop()


export SPARK_HOME=/usr/lib/spark
# Yes, I did build it under the $HOME for hadoop, so what?
export HAIL_HOME=/home/hadoop/hail
export PYTHONPATH="$PYTHONPATH:$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.4-src.zip"
# Do not forget to switch to Python v3.6
export PYSPARK_PYTHON=python3

# Note: Had to drop the .jar and .zip on /home/hadoop on the slave nodes!!!
spark-submit \
--jars /home/hadoop/hail-all-spark.jar \
--conf='spark.driver.extraClassPath=./hail-all-spark.jar' \
--conf='spark.executor.extraClassPath=./hail-all-spark.jar' \
--files /home/hadoop/hail-python.zip \
test.py
















STARTING:
	$SPARK_HOME/bin/spark-shell --version







-------------------------------------------------------------------------------------------------------------------------------
GOLDEN:

pyspark --jars hail-all-spark.jar \
  --py-files hail-python.zip \
  --conf spark.driver.extraClassPath=./hail-all-spark.jar \
  --conf spark.executor.extraClassPath=./hail-all-spark.jar \
  --conf spark.sql.files.openCostInBytes=1099511627776 \
  --conf spark.sql.files.maxPartitionBytes=1099511627776 \
  --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator
-------------------------------------------------------------------------------------------------------------------------------




-------------------------------------------------------------------------------------------------------------------------------
advice
		
	go to discuss.hail.is and search on 'your item'

Are you using an 0.1 jar/python zip? init was added in 0.2.


Hail doesn’t do anything special with the SparkContext that is passed into it.
-------------------------------------------------------------------------------------------------------------------------------














-------------------------------------------------------------------------------------------------------------------------------
These are Spark config options that Hail requires – they’re not really documented, which is a problem. When Hail creates a SparkContext, it configures it properly, but when one is passed in (hc = HailContext(sc)) we have to check.

How is your spark context created? Is it created from a pyspark submit command? 
You can configure most Spark startup actions with a --conf option, e.g. --conf spark.sql.files.maxPartitionBytes=1000000000



conf = SparkConf()
conf.set('spark.sql.files.maxPartitionBytes
','60000000000')
conf.set('spark.sql.files.openCostInBytes','60000000000')
conf.set('master','local[*]')
conf.set('spark.submit.deployMode', u'client')
conf.set('spark.app.name', u'PyTest')
sc = SparkContext(conf=conf)



or



In [1]: from pyspark import *

In [2]: from hail import *

In [3]: conf = SparkConf()
   ...: conf.set('spark.sql.files.maxPartitionBytes','60000000000')
   ...: conf.set('spark.sql.files.openCostInBytes','60000000000')
   ...: conf.set('master','local[*]')
   ...: conf.set('spark.submit.deployMode', u'client')
   ...: conf.set('spark.app.name', u'PyTest')
   ...: sc = SparkContext(conf=conf)

In [4]: hc = HailContext(sc)
hail: info: SparkUI: http://192.168.1.2:4040
-------------------------------------------------------------------------------------------------------------------------------













-------------------------------------------------------------------------------------------------------------------------------
from hail import *
hc = HailContext()
-------------------------------------------------------------------------------------------------------------------------------

















-------------------------------------------------------------------------------------------------------------------------------
from pyspark import *
from hail import *
conf = SparkConf()
conf.set('spark.sql.files.maxPartitionBytes','60000000000') 
conf.set('spark.sql.files.openCostInBytes','60000000000') 
conf.set('spark.driver.cores','1') #test with 1 core
sc = SparkContext(conf=conf)
hc = HailContext(sc)
-------------------------------------------------------------------------------------------------------------------------------








-------------------------------------------------------------------------------------------------------------------------------
pyspark2 --jars $HAIL_HOME/build/libs/hail-all-spark.jar 
–py-files $HAIL_HOME/python/hail-python.zip 
–conf spark.executorEnv.JAVA_HOME=/opt/jdk1.8.0.131 
–conf spark.executor.extraClassPath=/log/cloudera/parcels/SPARK2-2.1.0.cloudera1-1.cdh5.7.0.p0.120904/lib/spark2 
–conf spark.driver.extraClassPath=/log/cloudera/parcels/SPARK2-2.1.0.cloudera1-1.cdh5.7.0.p0.120904/lib/spark2 
–conf spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,org.apache.hadoop.io.compress.GzipCodec 
–conf spark.sql.files.openCostInBytes=1099511627776 
–conf spark.sql.files.maxPartitionBytes=1099511627776 
–conf spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1099511627776 
–conf spark.hadoop.parquet.block.size=109951162777

from hail import *
hc = HailContext(sc)
hc.import_vcf(’/user/kmlong/vcf/ALL.chrMT.phase3_callmom-v0_4.20130502.genotypes.vcf’).write(’/user/kmlong/vds/ALL.chrMT.phase3_callmom-v0_4.20130502.genotypes.vds’)
-------------------------------------------------------------------------------------------------------------------------------





-------------------------------------------------------------------------------------------------------------------------------
pyspark2 --jars build/libs/hail-all-spark.jar 
–py-files build/distributions/hail-python.zip 
–conf spark.sql.files.openCostInBytes=1099511627776 
–conf spark.sql.files.maxPartitionBytes=1099511627776 
–conf spark.hadoop.parquet.block.size=1099511627776
-------------------------------------------------------------------------------------------------------------------------------





-------------------------------------------------------------------------------------------------------------------------------
Yeah having a problem as well. Based on @tpoterba previous suggestion I installed Anoconda with Python 3 on all of the nodes and then build a cluster using EMR 5.10.0 (Spark 2.2.0). When I try to use the pyspark command line I cannot even get the hl.init(sc) command to work.


 
[hadoop@ip-172-30-1-4 ~]$ pyspark --jars hail-all-spark.jar  --py-files hail-python.zip
Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 18:21:58)
[GCC 7.2.0] on linux
Type “help”, “copyright”, “credits” or “license” for more information.
Setting default log level to “WARN”.
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
18/05/03 23:18:29 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
18/05/03 23:18:40 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
Welcome to
-------------------------------------------------------------------------------------------------------------------------------





-------------------------------------------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------------------------------------------





-------------------------------------------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------------------------------------------





-------------------------------------------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------------------------------------------





-------------------------------------------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------------------------------------------




